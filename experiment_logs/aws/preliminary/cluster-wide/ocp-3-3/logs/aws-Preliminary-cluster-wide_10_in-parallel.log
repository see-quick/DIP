[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[INFO] test                                                               [jar]
[INFO] crd-annotations                                                    [jar]
[INFO] crd-generator                                                      [jar]
[INFO] api                                                                [jar]
[INFO] mockkube                                                           [jar]
[INFO] config-model                                                       [jar]
[INFO] certificate-manager                                                [jar]
[INFO] operator-common                                                    [jar]
[INFO] systemtest                                                         [jar]
[INFO] 
[INFO] -------------------------< io.strimzi:strimzi >-------------------------
[INFO] Building Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ strimzi ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ strimzi >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ strimzi <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ strimzi ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ strimzi ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ strimzi ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ strimzi ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ strimzi ---
[INFO] Skipping pom project
[INFO] 
[INFO] --------------------------< io.strimzi:test >---------------------------
[INFO] Building test 0.29.0-SNAPSHOT                                     [2/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ test ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ test >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ test <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ test ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ test ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/EmbeddedZooKeeper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/WaitException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/ExecResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/Exec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/OpenShift.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/KubeCluster.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Kubernetes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Minikube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Kubectl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Oc.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/KubeCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/KubeClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/NoClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/HelmClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/ExtensionContextParameterResolver.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/TestSeparator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/TestUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/logs/CollectorElement.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/class-use/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ test ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ test ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:crd-annotations >---------------------
[INFO] Building crd-annotations 0.29.0-SNAPSHOT                          [3/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-annotations ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-annotations ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-annotations >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-annotations <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-annotations ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-annotations ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/ApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/KubeVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/VersionRange.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-annotations ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-annotations ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-annotations ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:crd-generator >----------------------
[INFO] Building crd-generator 0.29.0-SNAPSHOT                            [4/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-generator ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-generator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-generator >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-generator <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-generator ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-generator ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/KubeLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Linker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/OpenShiftOriginLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/PropertyType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Schema.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/DescriptionFile.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Example.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/KubeLink.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/OneOf.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Type.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternative.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Description.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Maximum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Minimum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/MinimumItems.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/PresentInVersions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Crd.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Pattern.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Property.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/CrdGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/DocGenerator.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.0:shade (default) @ crd-generator ---
[INFO] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[INFO] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[INFO] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[INFO] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[INFO] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[INFO] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[INFO] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[INFO] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] kubernetes-model-coordination-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 18 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[WARNING]   - 8 more...
[WARNING] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[WARNING]   - okhttp3.logging.LoggingEventListener$Factory
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Level
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor
[WARNING]   - okhttp3.logging.package-info
[WARNING]   - okhttp3.logging.LoggingEventListener
[WARNING]   - okhttp3.logging.LoggingEventListener$1
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[WARNING] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.client.internal.CertUtils
[WARNING]   - io.fabric8.kubernetes.client.CustomResource
[WARNING]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[WARNING]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[WARNING]   - io.fabric8.kubernetes.client.VersionInfo$1
[WARNING]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[WARNING]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[WARNING]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.dsl.Containerable
[WARNING]   - 526 more...
[WARNING] kubernetes-model-events-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 44 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, automaton-1.11-8.jar define 25 overlapping classes: 
[WARNING]   - dk.brics.automaton.AutomatonMatcher
[WARNING]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[WARNING]   - dk.brics.automaton.RegExp$Kind
[WARNING]   - dk.brics.automaton.RunAutomaton
[WARNING]   - dk.brics.automaton.Automaton
[WARNING]   - dk.brics.automaton.RegExp
[WARNING]   - dk.brics.automaton.AutomatonProvider
[WARNING]   - dk.brics.automaton.RegExp$1
[WARNING]   - dk.brics.automaton.MinimizationOperations$StateListNode
[WARNING]   - dk.brics.automaton.State
[WARNING]   - 15 more...
[WARNING] kubernetes-model-metrics-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 30 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[WARNING]   - 20 more...
[WARNING] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[WARNING]   - 224 more...
[WARNING] kubernetes-model-apiextensions-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[WARNING]   - 340 more...
[WARNING] generex-1.0.2.jar, crd-generator-0.29.0-SNAPSHOT.jar define 7 overlapping classes: 
[WARNING]   - com.mifmif.common.regex.GenerexIterator
[WARNING]   - com.mifmif.common.regex.Generex
[WARNING]   - com.mifmif.common.regex.GenerexIterator$Step
[WARNING]   - com.mifmif.common.regex.Node
[WARNING]   - com.mifmif.common.regex.Main
[WARNING]   - com.mifmif.common.regex.util.Iterable
[WARNING]   - com.mifmif.common.regex.util.Iterator
[WARNING] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - 340 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, zjsonpatch-0.3.0.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[WARNING]   - io.fabric8.zjsonpatch.Operation
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[WARNING]   - io.fabric8.zjsonpatch.internal.guava.Strings
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[WARNING]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[WARNING]   - io.fabric8.zjsonpatch.Diff
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[WARNING]   - io.fabric8.zjsonpatch.JsonPatch
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-admissionregistration-5.12.0.jar define 362 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[WARNING]   - 352 more...
[WARNING] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[WARNING]   - 70 more...
[WARNING] kubernetes-model-certificates-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 60 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[WARNING]   - 50 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-policy-5.12.0.jar define 162 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[WARNING]   - 152 more...
[WARNING] jackson-datatype-jsr310-2.13.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 59 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[WARNING]   - 49 more...
[WARNING] kubernetes-model-flowcontrol-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 132 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[WARNING]   - 122 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, crd-annotations-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - io.strimzi.api.annotations.VersionRange
[WARNING]   - io.strimzi.api.annotations.ApiVersion
[WARNING]   - io.strimzi.api.annotations.ApiVersion$Stability
[WARNING]   - io.strimzi.api.annotations.ApiVersion$1
[WARNING]   - io.strimzi.api.annotations.DeprecatedType
[WARNING]   - io.strimzi.api.annotations.DeprecatedProperty
[WARNING]   - io.strimzi.api.annotations.VersionRange$VersionParser
[WARNING]   - io.strimzi.api.annotations.KubeVersion
[WARNING] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[WARNING]   - com.fasterxml.jackson.core.json.JsonReadFeature
[WARNING]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[WARNING]   - com.fasterxml.jackson.core.util.Separators
[WARNING]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[WARNING]   - com.fasterxml.jackson.core.TreeNode
[WARNING]   - com.fasterxml.jackson.core.sym.Name
[WARNING]   - com.fasterxml.jackson.core.util.RequestPayload
[WARNING]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[WARNING]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[WARNING]   - 114 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[WARNING]   - okio.ByteString
[WARNING]   - okio.Source
[WARNING]   - okio.ForwardingSink
[WARNING]   - okio.BufferedSource
[WARNING]   - okio.Util
[WARNING]   - okio.AsyncTimeout$1
[WARNING]   - okio.HashingSource
[WARNING]   - okio.GzipSink
[WARNING]   - okio.Okio$1
[WARNING]   - okio.Pipe$PipeSink
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-databind-2.12.6.jar define 700 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[WARNING]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[WARNING]   - com.fasterxml.jackson.databind.BeanDescription
[WARNING]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[WARNING]   - com.fasterxml.jackson.databind.SerializerProvider
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[WARNING]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[WARNING]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[WARNING]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[WARNING]   - 690 more...
[WARNING] kubernetes-model-discovery-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 88 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - 78 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[WARNING]   - 254 more...
[WARNING] snakeyaml-1.27.jar, crd-generator-0.29.0-SNAPSHOT.jar define 216 overlapping classes: 
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[WARNING]   - org.yaml.snakeyaml.Yaml$3
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[WARNING]   - org.yaml.snakeyaml.util.ArrayUtils
[WARNING]   - org.yaml.snakeyaml.tokens.Token$ID
[WARNING]   - org.yaml.snakeyaml.reader.StreamReader
[WARNING]   - 206 more...
[WARNING] kubernetes-model-node-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 78 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[WARNING]   - 68 more...
[WARNING] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.StatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[WARNING]   - 2384 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[WARNING]   - com.fasterxml.jackson.annotation.JsonInclude
[WARNING]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[WARNING]   - com.fasterxml.jackson.annotation.JsonIgnore
[WARNING]   - com.fasterxml.jackson.annotation.JsonSetter
[WARNING]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[WARNING]   - com.fasterxml.jackson.annotation.JsonSubTypes
[WARNING]   - 61 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, slf4j-api-1.7.36.jar define 34 overlapping classes: 
[WARNING]   - org.slf4j.helpers.SubstituteLogger
[WARNING]   - org.slf4j.helpers.NamedLoggerBase
[WARNING]   - org.slf4j.helpers.NOPMDCAdapter
[WARNING]   - org.slf4j.MarkerFactory
[WARNING]   - org.slf4j.helpers.BasicMarker
[WARNING]   - org.slf4j.spi.LoggerFactoryBinder
[WARNING]   - org.slf4j.MDC$MDCCloseable
[WARNING]   - org.slf4j.spi.LocationAwareLogger
[WARNING]   - org.slf4j.helpers.MessageFormatter
[WARNING]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[WARNING]   - 24 more...
[WARNING] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[WARNING]   - 7 more...
[WARNING] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[WARNING]   - 202 more...
[WARNING] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.model.annotation.Plural
[WARNING]   - io.fabric8.kubernetes.model.annotation.Group
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[WARNING]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[WARNING]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[WARNING]   - io.fabric8.kubernetes.model.annotation.Singular
[WARNING]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.Version
[WARNING]   - 6 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[WARNING]   - 102 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-storageclass-5.12.0.jar define 172 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[WARNING]   - 162 more...
[WARNING] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[WARNING]   - okhttp3.WebSocket
[WARNING]   - okhttp3.Cookie$Builder
[WARNING]   - okhttp3.internal.http.HttpHeaders
[WARNING]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[WARNING]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[WARNING]   - okhttp3.internal.tls.OkHostnameVerifier
[WARNING]   - okhttp3.Cache$Entry
[WARNING]   - okhttp3.internal.http2.Http2Connection$3
[WARNING]   - okhttp3.internal.ws.RealWebSocket$Streams
[WARNING]   - okhttp3.CacheControl$Builder
[WARNING]   - 198 more...
[WARNING] maven-shade-plugin has detected that some class files are
[WARNING] present in two or more JARs. When this happens, only one
[WARNING] single version of the class is copied to the uber jar.
[WARNING] Usually this is not harmful and you can skip these warnings,
[WARNING] otherwise try to manually exclude artifacts based on
[WARNING] mvn dependency:tree -Ddetail=true and the above output.
[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-generator ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-generator ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-generator ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------------< io.strimzi:api >---------------------------
[INFO] Building api 0.29.0-SNAPSHOT                                      [5/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1-eo) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-doc) @ api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 99 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-test-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ api ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ api >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ api <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ api ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ api ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaBridgeList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectorList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMaker2List.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMakerList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaRebalanceList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaTopicList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaUserList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclResourcePatternType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleGroupResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertAndKeySecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateExpirationPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnect.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ContainerEnvVar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityUserOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasConfigurableMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/InlineLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JvmOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/GenericSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ClientTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridge.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Probe.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRule.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateAuthority.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalanceSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopic.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUser.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopicSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserQuotas.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/MetricsConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Password.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Logging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Rack.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Sidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/SystemProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecarLogLevel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Kafka.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/UnknownPropertyPreserving.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ZookeeperClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/BrokerCapacity.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ConnectorPlugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Build.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Plugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Artifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DockerOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/JarArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/MavenArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/OtherArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Output.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/TgzArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ZipArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/NodeAddressType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Condition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaUserStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerAddress.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/HasStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaTopicStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/EphemeralStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/JbodStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/Storage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ContainerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaUserTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodManagementPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/StatefulSetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/InternalServiceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamily.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamilyPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/MetadataTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ResourceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/BuildConfigTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/CruiseControlTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/JaegerTracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/Tracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalConfigurationReference.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSet.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSetSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/Crds.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/StrimziPodSetList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/api/target/api-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ api ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ api ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ api ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ api ---
[INFO] No dependency problems found
[INFO] 
[INFO] ------------------------< io.strimzi:mockkube >-------------------------
[INFO] Building mockkube 0.29.0-SNAPSHOT                                 [6/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ mockkube ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ mockkube ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ mockkube >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ mockkube <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ mockkube ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ mockkube ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/Observer.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/PredicatedWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/CustomResourceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/ServiceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/DeploymentMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockKube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/StatefulSetMockBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ mockkube ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ mockkube ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ mockkube ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:config-model >-----------------------
[INFO] Building config-model 0.29.0-SNAPSHOT                             [7/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ config-model ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ config-model ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ config-model >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ config-model <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ config-model ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ config-model ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Scope.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Type.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ config-model ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ config-model ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ config-model ---
[INFO] No dependency problems found
[INFO] 
[INFO] -------------------< io.strimzi:certificate-manager >-------------------
[INFO] Building certificate-manager 0.29.0-SNAPSHOT                      [8/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ certificate-manager ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ certificate-manager ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ certificate-manager >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ certificate-manager <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ certificate-manager ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ certificate-manager ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/OpenSslCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/SecretCertProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/Subject.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ certificate-manager ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ certificate-manager ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ certificate-manager ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:operator-common >---------------------
[INFO] Building operator-common 0.29.0-SNAPSHOT                          [9/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 9 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ operator-common ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ operator-common ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ operator-common >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ operator-common <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ operator-common ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ operator-common ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/InvalidResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NoSuchResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/Ca.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/ClientsCa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/StatusDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/BackOff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigurationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MaxAttemptsExceededException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/PasswordGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/NamespaceAndName.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/Labels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/OrderedProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ResourceVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ValidationVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PvcOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceSupport.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StatusUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ReconcileResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/CrdOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/TimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/EndpointOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ImageStreamOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/SecretOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ConfigMapOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NodeOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RouteOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StorageClassOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/process/ProcessHelper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsAndLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AbstractOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/DefaultAdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigParameterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MicrometerMetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/OperatorWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Reconciliation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationLogger.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Util.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/KubernetesVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/PlatformFeaturesAvailability.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ operator-common ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ operator-common ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ operator-common ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ operator-common ---
[INFO] No dependency problems found
[INFO] 
[INFO] -----------------------< io.strimzi:systemtest >------------------------
[INFO] Building systemtest 0.29.0-SNAPSHOT                              [10/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ systemtest ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 32 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ systemtest ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ systemtest ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/cloud-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-28T10-26-53_433-jvmRun1.dumpstream
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ systemtest ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ systemtest >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ systemtest <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ systemtest ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ systemtest ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelNamespaceTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/cli/KafkaCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DefaultNetworkPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorInstallType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/CustomResourceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/OlmInstallationStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorRBACType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DeploymentTypes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/k8s/Events.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/keycloak/KeycloakInstance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/ExecutionListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/OrderTestSuites.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/LogCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/TestExecutionWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAllOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAnyOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasNoneOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/Matchers.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaUserResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/HelmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/OlmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/SetupClusterOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceItem.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ThrowableRunner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/JobResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/SecretResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ComponentType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/CertAndKeyFiles.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/BridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/CruiseControlUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/JmxUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/OlmUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/TracingUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/KeycloakUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/FileUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/HttpUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/RollingUpdateUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/StUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/TestKafkaVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/interfaces/IndicativeSentences.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/specific/ScraperTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/BeforeAllOnce.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/metrics/MetricsCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/SuiteThreadController.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/storage/TestStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Environment.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/class-use/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/class-use/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/class-use/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/class-use/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/class-use/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/class-use/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ systemtest ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 14:28:06 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[INFO] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
[INFO] Running io.strimzi.systemtest.operators.user.UserST
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
[INFO] Running io.strimzi.systemtest.operators.topic.TopicST
[INFO] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] DEBUG [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:219] Used environment variables:
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:220] CONFIG: /home/cloud-user/strimzi-kafka-operator/systemtest/config.json
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] SKIP_TEARDOWN: false
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] LB_FINALIZERS: false
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_ORG: strimzi
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_LOG_DIR: /home/cloud-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_TAG: latest
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-28 14:28:07 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.user.UserST - Before Suite] - Setup Suite environment
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [KubeCluster:80] Cluster minikube is not installed!
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] DEBUG [KubeCluster:71] Cluster kubectl is installed
2022-03-28 14:28:07 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - kubectl cluster-info
2022-03-28 14:28:13 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: kubectl cluster-info
2022-03-28 14:28:13 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:28:13 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - kubectl api-resources
2022-03-28 14:28:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: kubectl api-resources
2022-03-28 14:28:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:28:19 [ForkJoinPool-1-worker-13] DEBUG [KubeCluster:77] Cluster kubectl is not running!
2022-03-28 14:28:19 [ForkJoinPool-1-worker-13] DEBUG [KubeCluster:71] Cluster oc is installed
2022-03-28 14:28:19 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc status -n default
2022-03-28 14:28:20 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc status -n default
2022-03-28 14:28:20 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:28:20 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc api-resources
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc api-resources
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] DEBUG [KubeCluster:73] Cluster oc is running
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] INFO  [KubeCluster:87] Using cluster: oc
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@380ad11e
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@108877eb, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@380ad11e, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 14:28:21 [ForkJoinPool-1-worker-13] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o json
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o json
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c32,c24",
            "openshift.io/sa.scc.supplemental-groups": "1001040000/10000",
            "openshift.io/sa.scc.uid-range": "1001040000/10000"
        },
        "creationTimestamp": "2022-03-28T14:28:19Z",
        "labels": {
            "kubernetes.io/metadata.name": "infra-namespace"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:28:19Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:28:19Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "112087",
        "uid": "2d95ce1e-60d1-49de-8b19-835f6a610df9"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 14:28:23 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 14:28:24 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:26 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 14:28:26 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:26 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:28 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 14:28:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 14:28:31 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 14:28:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 14:28:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 14:28:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 14:28:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:28:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 14:28:33 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 14:28:33 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 14:28:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479888ms till timeout)
2022-03-28 14:28:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478774ms till timeout)
2022-03-28 14:28:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477660ms till timeout)
2022-03-28 14:28:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476546ms till timeout)
2022-03-28 14:28:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475432ms till timeout)
2022-03-28 14:28:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474317ms till timeout)
2022-03-28 14:28:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473204ms till timeout)
2022-03-28 14:28:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472090ms till timeout)
2022-03-28 14:28:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470977ms till timeout)
2022-03-28 14:28:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469860ms till timeout)
2022-03-28 14:28:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468739ms till timeout)
2022-03-28 14:28:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467624ms till timeout)
2022-03-28 14:28:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466506ms till timeout)
2022-03-28 14:28:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465390ms till timeout)
2022-03-28 14:28:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464276ms till timeout)
2022-03-28 14:28:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463162ms till timeout)
2022-03-28 14:28:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462048ms till timeout)
2022-03-28 14:28:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460936ms till timeout)
2022-03-28 14:28:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459785ms till timeout)
2022-03-28 14:28:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458672ms till timeout)
2022-03-28 14:28:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457559ms till timeout)
2022-03-28 14:28:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456446ms till timeout)
2022-03-28 14:28:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455334ms till timeout)
2022-03-28 14:28:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454221ms till timeout)
2022-03-28 14:28:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453109ms till timeout)
2022-03-28 14:29:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451995ms till timeout)
2022-03-28 14:29:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450883ms till timeout)
2022-03-28 14:29:03 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 14:29:03 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 14:29:03 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 14:29:03 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:03 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599866ms till timeout)
2022-03-28 14:29:04 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:04 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598754ms till timeout)
2022-03-28 14:29:05 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:05 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597640ms till timeout)
2022-03-28 14:29:06 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:06 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596523ms till timeout)
2022-03-28 14:29:07 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:07 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595410ms till timeout)
2022-03-28 14:29:09 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:09 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594297ms till timeout)
2022-03-28 14:29:10 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:10 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593183ms till timeout)
2022-03-28 14:29:11 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:11 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592066ms till timeout)
2022-03-28 14:29:12 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:12 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590954ms till timeout)
2022-03-28 14:29:13 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:13 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589841ms till timeout)
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-5d8h5 not ready: strimzi-cluster-operator)
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-5d8h5 are ready
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:667] [bridge.HttpBridgeScramShaST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:667] [operators.topic.TopicST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:667] [operators.topic.ThrottlingQuotaST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:69] [operators.topic.TopicST] - Adding parallel suite: TopicST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:69] [operators.topic.ThrottlingQuotaST] - Adding parallel suite: ThrottlingQuotaST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeScramShaST] - Adding parallel suite: HttpBridgeScramShaST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:666] ============================================================================
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:73] [operators.topic.TopicST] - Parallel suites count: 1
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeScramShaST] - Parallel suites count: 3
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:667] [cruisecontrol.CruiseControlConfigurationST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:184] TopicST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:69] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel suite: CruiseControlConfigurationST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:73] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 4
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:184] HttpBridgeScramShaST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:73] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 2
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] DEBUG [TestSuiteNamespaceManager:129] Test suite `TopicST` creates these additional namespaces:[topic-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeScramShaST` creates these additional namespaces:[http-bridge-scram-sha-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:184] ThrottlingQuotaST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 6
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:184] CruiseControlConfigurationST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:129] Test suite `ThrottlingQuotaST` creates these additional namespaces:[throttling-quota-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] DEBUG [TestSuiteNamespaceManager:129] Test suite `CruiseControlConfigurationST` creates these additional namespaces:[cruise-control-configuration-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 7
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-28 14:29:14 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-03-28 14:29:14 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-03-28 14:29:14 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:156] Creating Namespace: topic-st
2022-03-28 14:29:14 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 14:29:14 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: user-st
2022-03-28 14:29:14 [ForkJoinPool-1-worker-23] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-28 14:29:14 [ForkJoinPool-1-worker-19] INFO  [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Namespace topic-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c32,c29",
            "openshift.io/sa.scc.supplemental-groups": "1001050000/10000",
            "openshift.io/sa.scc.uid-range": "1001050000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-scram-sha-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "http-bridge-scram-sha-st",
        "resourceVersion": "112524",
        "uid": "646887a6-6b79-4218-a5a2-6e71e071f52c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-scram-sha-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-5] INFO  [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c17",
            "openshift.io/sa.scc.supplemental-groups": "1001090000/10000",
            "openshift.io/sa.scc.uid-range": "1001090000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "reconciliation-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "112619",
        "uid": "4e24f32f-b865-4f54-9081-3183c3c63a8e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c2",
            "openshift.io/sa.scc.supplemental-groups": "1001060000/10000",
            "openshift.io/sa.scc.uid-range": "1001060000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "topic-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "topic-st",
        "resourceVersion": "112556",
        "uid": "1025cddb-0bd1-4051-bfd8-69bda0103528"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] INFO  [KubeClusterResource:82] Client use Namespace: topic-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=topic-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c27",
            "openshift.io/sa.scc.supplemental-groups": "1001110000/10000",
            "openshift.io/sa.scc.uid-range": "1001110000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "cruise-control-configuration-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "cruise-control-configuration-st",
        "resourceVersion": "112679",
        "uid": "a974a3e9-572c-43d7-a4f2-7a8566ae4685"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] INFO  [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-configuration-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-27] INFO  [TopicST:491] Deploying shared Kafka across all test cases in topic-st namespace
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c7",
            "openshift.io/sa.scc.supplemental-groups": "1001070000/10000",
            "openshift.io/sa.scc.uid-range": "1001070000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "throttling-quota-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "throttling-quota-st",
        "resourceVersion": "112571",
        "uid": "d6541cd1-0017-437a-bd7d-aa0b1a0c8a6e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationFileIsCreated
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=throttling-quota-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 1
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:230] testConfigurationFileIsCreated test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 2
2022-03-28 14:29:15 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaAndKafkaConnectWithConnector test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testCapacityFile
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 3
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 4
2022-03-28 14:29:15 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:230] testCapacityFile test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 5
2022-03-28 14:29:15 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:230] testDeployAndUnDeployCruiseControl test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c12",
            "openshift.io/sa.scc.supplemental-groups": "1001080000/10000",
            "openshift.io/sa.scc.uid-range": "1001080000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-tls-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "http-bridge-tls-st",
        "resourceVersion": "112589",
        "uid": "c1b57bef-c7b2-402e-b132-e0a57e374528"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c22",
            "openshift.io/sa.scc.supplemental-groups": "1001100000/10000",
            "openshift.io/sa.scc.uid-range": "1001100000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:10Z",
        "labels": {
            "kubernetes.io/metadata.name": "user-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:10Z"
            }
        ],
        "name": "user-st",
        "resourceVersion": "112651",
        "uid": "94a7f8c9-66a1-4f62-8c2f-f4469817b14d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: user-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationPerformanceOptions
2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationReflection
2022-03-28 14:29:15 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testConfigurationPerformanceOptions test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-23] INFO  [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 14:29:15 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testConfigurationReflection test now can proceed its execution
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testConfigurationFileIsCreated=my-cluster-0d1aa6d1}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] TRACE [AbstractST:607] USERS_NAME_MAP: {testConfigurationFileIsCreated=my-user-1519556343-1892250062}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testConfigurationFileIsCreated=my-topic-672889728-2023534698}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients}
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testConfigurationFileIsCreated
2022-03-28 14:29:15 [ForkJoinPool-1-worker-21] INFO  [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Namespace namespace-0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace user-st get Namespace namespace-0 -o json
2022-03-28 14:29:16 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 14:29:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-28 14:29:16 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-03-28 14:29:16 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 14:29:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 14:29:16 [ForkJoinPool-1-worker-27] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-28 14:29:16 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 14:29:16 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-28 14:29:16 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 14:29:16 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 14:29:16 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:quota-cluster
2022-03-28 14:29:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Kafka: quota-cluster will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace user-st get Namespace namespace-0 -o json
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c33,c32",
            "openshift.io/sa.scc.supplemental-groups": "1001120000/10000",
            "openshift.io/sa.scc.uid-range": "1001120000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:11Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-0"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:11Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:11Z"
            }
        ],
        "name": "namespace-0",
        "resourceVersion": "112755",
        "uid": "03af5645-7c8e-492a-acc4-18bb2e75990c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0]}
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] INFO  [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testConfigurationFileIsCreated=my-user-1519556343-1892250062, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testConfigurationFileIsCreated=my-topic-672889728-2023534698, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testConfigurationReflection
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-0d1aa6d1 in namespace namespace-0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 14:29:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839794ms till timeout)
2022-03-28 14:29:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839794ms till timeout)
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] INFO  [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-28 14:29:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (839827ms till timeout)
2022-03-28 14:29:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839827ms till timeout)
2022-03-28 14:29:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839827ms till timeout)
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0d1aa6d1
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Namespace namespace-1
2022-03-28 14:29:16 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0d1aa6d1 will have desired state: Ready
2022-03-28 14:29:16 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0d1aa6d1 will have desired state: Ready
2022-03-28 14:29:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1319881ms till timeout)
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c34,c4",
            "openshift.io/sa.scc.supplemental-groups": "1001130000/10000",
            "openshift.io/sa.scc.uid-range": "1001130000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:12Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-1"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:12Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:12Z"
            }
        ],
        "name": "namespace-1",
        "resourceVersion": "112817",
        "uid": "4031614c-833c-4009-8bdd-9aadddc877e5"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0]}
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] INFO  [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testConfigurationFileIsCreated=my-user-1519556343-1892250062, testConfigurationPerformanceOptions=my-user-235473874-831358867, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testConfigurationFileIsCreated=my-topic-672889728-2023534698, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testConfigurationPerformanceOptions
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-9ef63e31 in namespace namespace-1
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-9ef63e31
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-2
2022-03-28 14:29:17 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 14:29:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838628ms till timeout)
2022-03-28 14:29:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838627ms till timeout)
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-9ef63e31 will have desired state: Ready
2022-03-28 14:29:17 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-9ef63e31 will have desired state: Ready
2022-03-28 14:29:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838627ms till timeout)
2022-03-28 14:29:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838624ms till timeout)
2022-03-28 14:29:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (838624ms till timeout)
2022-03-28 14:29:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1319792ms till timeout)
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c34,c9",
            "openshift.io/sa.scc.supplemental-groups": "1001140000/10000",
            "openshift.io/sa.scc.uid-range": "1001140000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:13Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-2"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:13Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:13Z"
            }
        ],
        "name": "namespace-2",
        "resourceVersion": "112862",
        "uid": "95611946-1e49-47de-adf7-a1ab4d0ac912"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0]}
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] TRACE [AbstractST:607] USERS_NAME_MAP: {testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testConfigurationPerformanceOptions=my-user-235473874-831358867, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 14:29:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1318772ms till timeout)
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-f3d16cd7 in namespace namespace-2
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] INFO  [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-f3d16cd7
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Namespace namespace-3
2022-03-28 14:29:18 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-f3d16cd7 will have desired state: Ready
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-f3d16cd7 will have desired state: Ready
2022-03-28 14:29:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1319888ms till timeout)
2022-03-28 14:29:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837519ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837413ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c34,c14",
            "openshift.io/sa.scc.supplemental-groups": "1001150000/10000",
            "openshift.io/sa.scc.uid-range": "1001150000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:14Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-3"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:14Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:14Z"
            }
        ],
        "name": "namespace-3",
        "resourceVersion": "112900",
        "uid": "c5c64fa9-9e11-4028-8664-33ee6efbfe0d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0]}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] INFO  [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testConfigurationPerformanceOptions=my-user-235473874-831358867, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-b0221df6 in namespace namespace-3
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:164] Using Namespace: namespace-3
2022-03-28 14:29:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (837409ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837409ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837410ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-28 14:29:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1318634ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-b0221df6
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace namespace-4
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 14:29:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1317571ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-b0221df6 will have desired state: Ready
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-b0221df6 will have desired state: Ready
2022-03-28 14:29:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1319892ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c34,c19",
            "openshift.io/sa.scc.supplemental-groups": "1001160000/10000",
            "openshift.io/sa.scc.uid-range": "1001160000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:14Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-4"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:14Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:14Z"
            }
        ],
        "name": "namespace-4",
        "resourceVersion": "112943",
        "uid": "8902217d-c7a1-4a6d-80cf-f2f1f7236169"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3]}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testCapacityFile
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-77376ada in namespace namespace-4
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 14:29:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1318777ms till timeout)
2022-03-28 14:29:19 [ForkJoinPool-1-worker-19] INFO  [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-28 14:29:19 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-77376ada
2022-03-28 14:29:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836373ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-77376ada will have desired state: Ready
2022-03-28 14:29:20 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-77376ada will have desired state: Ready
2022-03-28 14:29:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836261ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Namespace namespace-5
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 14:29:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836265ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836264ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (836262ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1317433ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1319792ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1316461ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1318782ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c34,c24",
            "openshift.io/sa.scc.supplemental-groups": "1001170000/10000",
            "openshift.io/sa.scc.uid-range": "1001170000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:15Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-5"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:15Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:15Z"
            }
        ],
        "name": "namespace-5",
        "resourceVersion": "112983",
        "uid": "5800523a-84c9-42e2-9ff1-73c835c0ce9b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-5]}
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] INFO  [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-d0923d27 in namespace namespace-5
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:164] Using Namespace: namespace-5
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-28 14:29:20 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-d0923d27
2022-03-28 14:29:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1317651ms till timeout)
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-6
2022-03-28 14:29:20 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 14:29:21 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-d0923d27 will have desired state: Ready
2022-03-28 14:29:21 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-d0923d27 will have desired state: Ready
2022-03-28 14:29:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835239ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1319782ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835131ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (835128ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835129ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835128ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1316297ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1318656ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c34,c29",
            "openshift.io/sa.scc.supplemental-groups": "1001180000/10000",
            "openshift.io/sa.scc.uid-range": "1001180000/10000"
        },
        "creationTimestamp": "2022-03-28T14:29:16Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-6"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:29:16Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:29:16Z"
            }
        ],
        "name": "namespace-6",
        "resourceVersion": "113021",
        "uid": "19c6be27-7765-4fa5-8d65-8f4a85500eae"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-5]}
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-58df3988 in namespace namespace-6
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 14:29:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1315346ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1317667ms till timeout)
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-58df3988
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-58df3988 will have desired state: Ready
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-58df3988 will have desired state: Ready
2022-03-28 14:29:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (839891ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1316541ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834127ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1318669ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (834015ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833911ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834016ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834014ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1315183ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1317542ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1314231ms till timeout)
2022-03-28 14:29:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1316557ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (838781ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1315343ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833017ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1317559ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832800ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832904ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1314072ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (832797ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1316430ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832797ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1313120ms till timeout)
2022-03-28 14:29:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1315446ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (837672ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1314229ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831906ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1316450ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831690ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831684ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1315318ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1312959ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831684ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (831576ms till timeout)
2022-03-28 14:29:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1312006ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1314332ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (836561ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1313119ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830796ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1315338ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830580ms till timeout)
2022-03-28 14:29:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830574ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830468ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1314100ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1311741ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1313222ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1310790ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (830355ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (835451ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1312008ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829685ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1314222ms till timeout)
2022-03-28 14:29:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829466ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829460ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1312987ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1310628ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829353ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1309676ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1312107ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (829140ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (834342ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1310898ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828574ms till timeout)
2022-03-28 14:29:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1313113ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828355ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828350ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1309519ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1311875ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828138ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1308566ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (828030ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1310891ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (833093ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1309762ms till timeout)
2022-03-28 14:29:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827463ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1311999ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827240ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827238ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1310765ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1308405ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1307455ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827024ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1309780ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (826918ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (831983ms till timeout)
2022-03-28 14:29:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1308651ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826351ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826100ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1310751ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826101ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1309628ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1307267ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1306318ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825888ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1308644ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (825780ms till timeout)
2022-03-28 14:29:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (830873ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1307538ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825238ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824991ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824988ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1309530ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1308516ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1306155ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1305206ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824776ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1307531ms till timeout)
2022-03-28 14:29:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (824667ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (829761ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1306425ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824125ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823880ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823878ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1307404ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1308317ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823666ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1304094ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1304939ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (823558ms till timeout)
2022-03-28 14:29:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1306419ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (828621ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1305289ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822991ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822741ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822742ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1307182ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1306268ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1302958ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1303803ms till timeout)
2022-03-28 14:29:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822528ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (822422ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1305284ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (827485ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1304149ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821850ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821631ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821629ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1306068ms till timeout)
2022-03-28 14:29:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1305155ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1302689ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1301843ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821414ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (821307ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1304169ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (826373ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1303040ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820740ms till timeout)
2022-03-28 14:29:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820522ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820519ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1304958ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1303935ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1300623ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1303054ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1301467ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (820192ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820191ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (825264ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1301929ms till timeout)
2022-03-28 14:29:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819629ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819412ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819410ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1303848ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1302824ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1299514ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1300252ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (818977ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818975ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1301835ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (824155ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1300818ms till timeout)
2022-03-28 14:29:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818518ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818303ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818280ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1302719ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1301697ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1298386ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1300711ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817850ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1299125ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (817849ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (823046ms till timeout)
2022-03-28 14:29:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1299708ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817407ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817193ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817170ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1301608ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1300587ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1297277ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1298015ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816634ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (816633ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1299495ms till timeout)
2022-03-28 14:29:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (821842ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1298507ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816208ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816082ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816061ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1300497ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1299477ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1296167ms till timeout)
2022-03-28 14:29:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1296905ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815376ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (820685ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1298230ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (815367ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1297356ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815053ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814947ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814945ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1299383ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1298363ms till timeout)
2022-03-28 14:29:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1295053ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1295790ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814262ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1297010ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (814148ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (819463ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1296235ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813931ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813788ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813784ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1298214ms till timeout)
2022-03-28 14:29:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1297194ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1293883ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1294622ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813152ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1295899ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1295016ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (812928ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (818244ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812717ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812611ms till timeout)
2022-03-28 14:29:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812608ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1297046ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1296026ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1292713ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1293452ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812042ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1294790ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (811819ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1293907ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (817028ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811500ms till timeout)
2022-03-28 14:29:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811499ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811496ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1295930ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1294910ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1291599ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1292336ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810922ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1293673ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1292786ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (810696ms till timeout)
2022-03-28 14:29:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (815892ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810364ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810361ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810362ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1294802ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1293777ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1290467ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1291206ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809811ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1292563ms till timeout)
2022-03-28 14:29:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1291677ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (809482ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (814690ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809163ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809163ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809153ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1293597ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1292576ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1289265ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1290003ms till timeout)
2022-03-28 14:29:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808622ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1291375ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1290490ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (808295ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (813503ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807975ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807974ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807972ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1292411ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1291391ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1288081ms till timeout)
2022-03-28 14:29:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1288815ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807436ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1290189ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1289309ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (807112ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (812320ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806791ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806791ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806788ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1291228ms till timeout)
2022-03-28 14:29:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1290203ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1286892ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1287630ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806247ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1289001ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1288120ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (805926ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (811135ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805609ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805609ms till timeout)
2022-03-28 14:29:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805607ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1290045ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1289025ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1285715ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1286453ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805070ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1287820ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1286940ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (804745ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (809954ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804427ms till timeout)
2022-03-28 14:29:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804424ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804425ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1288865ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1287844ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1284533ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1285258ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803876ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1286631ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1285751ms till timeout)
2022-03-28 14:29:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (803555ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (808761ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803234ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803233ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803236ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1287676ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1286652ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1283342ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1284080ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802699ms till timeout)
2022-03-28 14:29:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1285453ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1284573ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (802378ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (807587ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802061ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802060ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802059ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1286497ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1285477ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1282166ms till timeout)
2022-03-28 14:29:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1282903ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801520ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1284274ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1283391ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (801196ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (806404ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800877ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800877ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800876ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1285316ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1284296ms till timeout)
2022-03-28 14:29:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1280986ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1281724ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800343ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1283097ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1282217ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (800021ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (805230ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799703ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799702ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799702ms till timeout)
2022-03-28 14:29:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1284142ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1283120ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1279809ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1280546ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799165ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1281918ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1281036ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (798841ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (804050ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798524ms till timeout)
2022-03-28 14:29:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798524ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798519ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1282955ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1281935ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1278624ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1279354ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797966ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1280721ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1279840ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (797643ms till timeout)
2022-03-28 14:29:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (802851ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797325ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797325ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797322ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1281761ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1280742ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1277432ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1278169ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796784ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1279539ms till timeout)
2022-03-28 14:29:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1278659ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (796464ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (801673ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796146ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796146ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796142ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1280582ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1279562ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1276252ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1276989ms till timeout)
2022-03-28 14:30:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795608ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1278363ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1277483ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (795287ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (800496ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794970ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794970ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794966ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1279404ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1278378ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1275064ms till timeout)
2022-03-28 14:30:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1275801ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794420ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1277174ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1276293ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (794096ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (799303ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793777ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793777ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793774ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1278214ms till timeout)
2022-03-28 14:30:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1277195ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1273884ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1274621ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793239ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1275994ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1275114ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (792914ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (798119ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792593ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792592ms till timeout)
2022-03-28 14:30:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792590ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1277029ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1276009ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1272699ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1273425ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792044ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1274798ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1273913ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (791717ms till timeout)
2022-03-28 14:30:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (796926ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791399ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791398ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791397ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1275826ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1274806ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1271493ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1272229ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790847ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1273601ms till timeout)
2022-03-28 14:30:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1272719ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (790517ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (795721ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790194ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790192ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790190ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1274629ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1273610ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1270298ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1271037ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789655ms till timeout)
2022-03-28 14:30:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1272407ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1271524ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (789331ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (794538ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789011ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789011ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789008ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1273448ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1272428ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1269117ms till timeout)
2022-03-28 14:30:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1269855ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788474ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1271228ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1270349ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (788153ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (793361ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787834ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787834ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787829ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1272265ms till timeout)
2022-03-28 14:30:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1271244ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1267934ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1268672ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787290ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1270036ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1269155ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (786959ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (792168ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786641ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786640ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786634ms till timeout)
2022-03-28 14:30:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1271073ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1270053ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1266741ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1267479ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786098ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1268852ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1267972ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (785777ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (790986ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785459ms till timeout)
2022-03-28 14:30:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785459ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785457ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1269897ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1268878ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1265567ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1266306ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784920ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1267675ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1266792ms till timeout)
2022-03-28 14:30:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (784596ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (789804ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784278ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784277ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784273ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1268711ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1267690ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1264378ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1265116ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783730ms till timeout)
2022-03-28 14:30:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1266485ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1265605ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (783410ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (788616ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783090ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783090ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783087ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1267526ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1266503ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1263190ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1263926ms till timeout)
2022-03-28 14:30:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782542ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1265297ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1264415ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (782219ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (787427ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781901ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781900ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781894ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1266324ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1265303ms till timeout)
2022-03-28 14:30:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1261988ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1262725ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781342ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1264097ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1263209ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (781014ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (786221ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780688ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780687ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780684ms till timeout)
2022-03-28 14:30:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1265118ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1264097ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1260786ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1261521ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780139ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1262894ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1262013ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (779817ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (785026ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779499ms till timeout)
2022-03-28 14:30:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779499ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779495ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1263934ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1262914ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1259601ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1260338ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778955ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1261709ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1260829ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (778633ms till timeout)
2022-03-28 14:30:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (783841ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778315ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778314ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778312ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1262749ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1261729ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1258415ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1259151ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777769ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1260523ms till timeout)
2022-03-28 14:30:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1259642ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (777446ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (782649ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777115ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777114ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777106ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1261545ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1260525ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1257212ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1257950ms till timeout)
2022-03-28 14:30:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776566ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1259320ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1258440ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (776244ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (781452ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775925ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775925ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775922ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1260362ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1259343ms till timeout)
2022-03-28 14:30:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1256033ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1256771ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775389ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1258143ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1257261ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (775066ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (780274ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774748ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774747ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774697ms till timeout)
2022-03-28 14:30:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1259148ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1258132ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1254821ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1255558ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774176ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1256931ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1256050ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (773855ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (779063ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773535ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773535ms till timeout)
2022-03-28 14:30:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773533ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1257972ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1256951ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1253645ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1254382ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773001ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1255756ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1254876ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (772676ms till timeout)
2022-03-28 14:30:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (777885ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772357ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772357ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772354ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1256793ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1255772ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1252462ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1253200ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771819ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1254574ms till timeout)
2022-03-28 14:30:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1253694ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (771498ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (776706ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771179ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771178ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771175ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1255613ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1254593ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1251283ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1252020ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770636ms till timeout)
2022-03-28 14:30:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1253391ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1252511ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (770316ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (775524ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769992ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769989ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769988ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1254428ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1253408ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1250098ms till timeout)
2022-03-28 14:30:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1250836ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769454ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1252207ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1251326ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (769130ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (774339ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768812ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768811ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768810ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1253249ms till timeout)
2022-03-28 14:30:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1252227ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1248917ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1249655ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768274ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1251028ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1250147ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (767952ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (773160ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767634ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767633ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767631ms till timeout)
2022-03-28 14:30:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1252071ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1251051ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1247740ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1248477ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767095ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1249845ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1248966ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (766770ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (771979ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766453ms till timeout)
2022-03-28 14:30:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766453ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766449ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1250889ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1249869ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1246559ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1247296ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765915ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1248670ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1247788ms till timeout)
2022-03-28 14:30:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (765592ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (770801ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765274ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765273ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765271ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1249711ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1248688ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1245376ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1246113ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764733ms till timeout)
2022-03-28 14:30:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1247488ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1246607ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (764410ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (769618ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764092ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764092ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764088ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1248528ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1247509ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1244199ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1244937ms till timeout)
2022-03-28 14:30:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763555ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1246310ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1245428ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (763232ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (768441ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762914ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762914ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762912ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1247351ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1246333ms till timeout)
2022-03-28 14:30:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1243021ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1243759ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762378ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1245133ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1244252ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (762055ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (767263ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761736ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761735ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761733ms till timeout)
2022-03-28 14:30:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1246172ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1245153ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1241843ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1242581ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761198ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1243954ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1243073ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (760876ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (766084ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760558ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760557ms till timeout)
2022-03-28 14:30:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760555ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1244995ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1243975ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1240664ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1241401ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760020ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1242775ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1241895ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (759699ms till timeout)
2022-03-28 14:30:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (764907ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759379ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759378ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759376ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1243814ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1242795ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1239483ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1240221ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758840ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1241595ms till timeout)
2022-03-28 14:30:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1240714ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (758519ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (763727ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758202ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758201ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758198ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1242637ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1241619ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1238308ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1239046ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757664ms till timeout)
2022-03-28 14:30:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1240418ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1239539ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (757343ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (762552ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757026ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757026ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757023ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1241462ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1240444ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1237132ms till timeout)
2022-03-28 14:30:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1237870ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756488ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1239242ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1238362ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (756166ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (761375ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755849ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755849ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755841ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1240278ms till timeout)
2022-03-28 14:30:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1239258ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1235947ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1236685ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755304ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1238059ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1237179ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (754982ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (760191ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754665ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754665ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754661ms till timeout)
2022-03-28 14:30:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1239101ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1238076ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1234764ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1235500ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754118ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1236871ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1235990ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (753795ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (759001ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753474ms till timeout)
2022-03-28 14:30:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753474ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753471ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1237911ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1236884ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1233579ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1234317ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752934ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1235685ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1234805ms till timeout)
2022-03-28 14:30:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (752610ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (757818ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752288ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752288ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752285ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1236725ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1235705ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1232393ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1233130ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751748ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1234500ms till timeout)
2022-03-28 14:30:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1233620ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (751425ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (756634ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751107ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751107ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751104ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1235544ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1234525ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1231215ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1231951ms till timeout)
2022-03-28 14:30:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750566ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1233314ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1232432ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (750236ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (755445ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749919ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749919ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749915ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1234352ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1233335ms till timeout)
2022-03-28 14:30:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1230021ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1230758ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749376ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1232130ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1231250ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (749055ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (754263ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748737ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748735ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748731ms till timeout)
2022-03-28 14:30:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1233169ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1232151ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1228840ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1229574ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748193ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1230947ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1230067ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (747872ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (753080ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747550ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747548ms till timeout)
2022-03-28 14:30:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747545ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1231985ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1230966ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1227655ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1228392ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747010ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1229765ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1228885ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (746689ms till timeout)
2022-03-28 14:30:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (751893ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746367ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746367ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746364ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1230801ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1229780ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1226469ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1227207ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745826ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1228581ms till timeout)
2022-03-28 14:30:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1227700ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (745505ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (750714ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745188ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745188ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745184ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1229624ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1228604ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1225294ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1226032ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744651ms till timeout)
2022-03-28 14:30:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1227406ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1226526ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (744329ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (749537ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744010ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744009ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744007ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1228447ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1227428ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1224117ms till timeout)
2022-03-28 14:30:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1224855ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743474ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1226227ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1225347ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (743152ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (748360ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742833ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742833ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742830ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1227269ms till timeout)
2022-03-28 14:30:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1226250ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1222938ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1223675ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742294ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1225048ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1224168ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (741972ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (747181ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741644ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741643ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741635ms till timeout)
2022-03-28 14:30:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1226074ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1225054ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1221744ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1222472ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741091ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1223844ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1222963ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (740765ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (745973ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740446ms till timeout)
2022-03-28 14:30:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740446ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740443ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1224882ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1223864ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1220552ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1221289ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739907ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1222662ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1221782ms till timeout)
2022-03-28 14:30:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (739583ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (744791ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739265ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739264ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739262ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1223701ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1222681ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1219370ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1220107ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738715ms till timeout)
2022-03-28 14:30:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1221470ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1220590ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (738388ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (743596ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738069ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738068ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738061ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1222501ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1221481ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1218171ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1218906ms till timeout)
2022-03-28 14:30:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737526ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1220281ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1219401ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (737205ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (742414ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736888ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736888ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736885ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1221325ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1220305ms till timeout)
2022-03-28 14:30:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1216996ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1217733ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736351ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1219106ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1218226ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (736029ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (741237ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735711ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735710ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735708ms till timeout)
2022-03-28 14:31:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1220146ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1219127ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1215817ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1216554ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735173ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1217927ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1217047ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (734850ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (740059ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734532ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734530ms till timeout)
2022-03-28 14:31:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734528ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1218969ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1217943ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1214632ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1215370ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733987ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1216742ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1215859ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (733663ms till timeout)
2022-03-28 14:31:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (738872ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733346ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733346ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733342ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1217782ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1216763ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1213452ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1214190ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732807ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1215562ms till timeout)
2022-03-28 14:31:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1214681ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (732486ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (737694ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732167ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732166ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732163ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1216572ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1215548ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1212237ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1212974ms till timeout)
2022-03-28 14:31:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731592ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1214347ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1213466ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (731271ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (736479ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730952ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730951ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730948ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1215387ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1214367ms till timeout)
2022-03-28 14:31:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1211057ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1211793ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730411ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1213165ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1212285ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (730090ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (735298ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729768ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729768ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729764ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1214203ms till timeout)
2022-03-28 14:31:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1213184ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1209872ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1210610ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729228ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1211983ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1211103ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (728907ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (734115ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728589ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728589ms till timeout)
2022-03-28 14:31:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728586ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1213026ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1212006ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1208696ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1209433ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728052ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1210807ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1209926ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (727730ms till timeout)
2022-03-28 14:31:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (732939ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727412ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727411ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727409ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1211849ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1210830ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1207520ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1208256ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726875ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1209629ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1208744ms till timeout)
2022-03-28 14:31:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (726549ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (731756ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726230ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726230ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726227ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1210666ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1209647ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1206337ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1207074ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725693ms till timeout)
2022-03-28 14:31:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1208445ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1207565ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (725369ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (730578ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725051ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725050ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725048ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1209487ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1208466ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1205156ms till timeout)
2022-03-28 14:31:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1205893ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724502ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1207255ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1206371ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (724174ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (729382ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723855ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723854ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723852ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1208292ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1207272ms till timeout)
2022-03-28 14:31:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1203961ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1204698ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723317ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1206071ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1205190ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (722992ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (728201ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722674ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722674ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722670ms till timeout)
2022-03-28 14:31:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1207109ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1206089ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1202778ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1203515ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722131ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1204885ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1204005ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (721809ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (727017ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721491ms till timeout)
2022-03-28 14:31:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721490ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721485ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1205924ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1204905ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1201595ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1202333ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720952ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1203706ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1202825ms till timeout)
2022-03-28 14:31:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (720625ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (725833ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720306ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720305ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720303ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1204740ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1203718ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1200408ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1201146ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719764ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1202520ms till timeout)
2022-03-28 14:31:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1201640ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (719444ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (724651ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719125ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719124ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719121ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1203561ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1202542ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1199231ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1199969ms till timeout)
2022-03-28 14:31:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718587ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1201342ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1200461ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (718266ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (723474ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717948ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717946ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717944ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1202385ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1201366ms till timeout)
2022-03-28 14:31:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1198055ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1198792ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717411ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1200166ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1199285ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (717090ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (722299ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716773ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716773ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716770ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1201211ms till timeout)
2022-03-28 14:31:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1200190ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1196879ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1197616ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716236ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1198987ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1198107ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (715911ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (721120ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715593ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715592ms till timeout)
2022-03-28 14:31:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715590ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1200030ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1199011ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1195694ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1196436ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715054ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1197808ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1196928ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (714731ms till timeout)
2022-03-28 14:31:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (719939ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714411ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714410ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714408ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1198847ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1197827ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1194512ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1195249ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713869ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1196622ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1195740ms till timeout)
2022-03-28 14:31:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (713543ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (718752ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713226ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713225ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713226ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1197665ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1196645ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1193330ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1194067ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712687ms till timeout)
2022-03-28 14:31:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1195441ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1194560ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateTopic
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 8
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateTopic test now can proceed its execution
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1691702842-2021683609 in namespace throttling-quota-st
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-28 14:31:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (717572ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712046ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712045ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1691702842-2021683609
2022-03-28 14:31:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712041ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1196481ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1691702842-2021683609 will have desired state: Ready
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1691702842-2021683609 will have desired state: Ready
2022-03-28 14:31:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1195462ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1192150ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1691702842-2021683609 will have desired state: Ready not ready, will try again in 1000 ms (179786ms till timeout)
2022-03-28 14:31:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1192887ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711505ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1194260ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1193379ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (716463ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710936ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710936ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710932ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1195372ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1194352ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1691702842-2021683609 will have desired state: Ready not ready, will try again in 1000 ms (178677ms till timeout)
2022-03-28 14:31:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1191040ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1191778ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710396ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1193151ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1192271ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (715353ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709827ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709826ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709823ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1194262ms till timeout)
2022-03-28 14:31:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1193242ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1189931ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1691702842-2021683609 will have desired state: Ready not ready, will try again in 1000 ms (177567ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1190667ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709286ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1192040ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1191159ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (714136ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708715ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708715ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708715ms till timeout)
2022-03-28 14:31:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1193154ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1192132ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1691702842-2021683609 will have desired state: Ready not ready, will try again in 1000 ms (176458ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1188820ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708141ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1189414ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1190899ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1190016ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707572ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (712992ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707571ms till timeout)
2022-03-28 14:31:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707574ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1192014ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1190994ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-1691702842-2021683609 is in desired state: Ready
2022-03-28 14:31:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1187681ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1188277ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707003ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-1ac6bea1-kafka-clients in namespace throttling-quota-st
2022-03-28 14:31:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1189762ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1188878ms till timeout)
2022-03-28 14:31:29 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-1ac6bea1-kafka-clients
2022-03-28 14:31:29 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-1ac6bea1-kafka-clients will be in active state
2022-03-28 14:31:29 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:31:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706462ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (711777ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706357ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706357ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1190898ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:31:30 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:31:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1189879ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1186569ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299781ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1187168ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1188649ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705787ms till timeout)
2022-03-28 14:31:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1187769ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705352ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705141ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (710561ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705140ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1189686ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1188665ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1185355ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298563ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1186060ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1187540ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704572ms till timeout)
2022-03-28 14:31:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1186659ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704244ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (709452ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703926ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1188577ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703924ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1187556ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1184246ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297352ms till timeout)
2022-03-28 14:31:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1184871ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1186351ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1185470ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703383ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703135ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (708331ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702805ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1187455ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702802ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1186436ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1183121ms till timeout)
2022-03-28 14:31:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296229ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1183746ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1185227ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1184347ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702259ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702007ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (707221ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (701692ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1186340ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (701583ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1185321ms till timeout)
2022-03-28 14:31:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1182009ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295114ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1182632ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1184113ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1183232ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (701145ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700891ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (706104ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1185229ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700578ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1184208ms till timeout)
2022-03-28 14:31:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700470ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1180898ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294004ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1181522ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1183002ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700035ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1182121ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699783ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (704993ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699467ms till timeout)
2022-03-28 14:31:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1184117ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1183098ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699359ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1179786ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292891ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1180410ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1181891ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1181009ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698921ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698670ms till timeout)
2022-03-28 14:31:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (703880ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1183004ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698352ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698246ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1181984ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1178673ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291778ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1179295ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1180775ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1179895ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697808ms till timeout)
2022-03-28 14:31:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697555ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (702770ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697243ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1181893ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1180875ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697136ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1177565ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290662ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1178181ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1179661ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696694ms till timeout)
2022-03-28 14:31:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1178780ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696440ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (701653ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696127ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1180775ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696019ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1179757ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1176446ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289550ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1177070ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1178551ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1177670ms till timeout)
2022-03-28 14:31:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695582ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695331ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (700542ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1179667ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 9
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testCreatingUsersWithSecretPrefix test now can proceed its execution
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 14:31:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1178648ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694909ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-28 14:31:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1175337ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-7
2022-03-28 14:31:41 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 14:31:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288330ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1175847ms till timeout)
2022-03-28 14:31:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1177432ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694463ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1176550ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c35,c0",
            "openshift.io/sa.scc.supplemental-groups": "1001190000/10000",
            "openshift.io/sa.scc.uid-range": "1001190000/10000"
        },
        "creationTimestamp": "2022-03-28T14:31:37Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-7"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:31:37Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:31:37Z"
            }
        ],
        "name": "namespace-7",
        "resourceVersion": "116648",
        "uid": "d1be6218-75ab-4f1e-b272-83c72101f15f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-0], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-5]}
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-67532fa3 in namespace namespace-7
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 14:31:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694223ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (699430ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-67532fa3
2022-03-28 14:31:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1178558ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1177538ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693800ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-67532fa3 will have desired state: Ready
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-67532fa3 will have desired state: Ready
2022-03-28 14:31:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1174227ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (839788ms till timeout)
2022-03-28 14:31:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287216ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1174736ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1176322ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693354ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1175335ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693113ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (698317ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1177441ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (692682ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1176418ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1173108ms till timeout)
2022-03-28 14:31:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (838667ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286103ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1175210ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1173623ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (692240ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1174220ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (692005ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (697209ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1176331ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (691573ms till timeout)
2022-03-28 14:31:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1175204ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1171894ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (837454ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284794ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1174004ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1172416ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (691039ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1173019ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (690823ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (696031ms till timeout)
2022-03-28 14:31:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1175155ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (690394ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1174016ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1170705ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (836257ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283667ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1172784ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1171197ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689923ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1171903ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689706ms till timeout)
2022-03-28 14:31:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (694913ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1174037ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (689280ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1172905ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1169593ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (835006ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1170082ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1171668ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (688807ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282443ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1170788ms till timeout)
2022-03-28 14:31:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (688590ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (693801ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1172924ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (688167ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1171795ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1168484ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (833896ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1168973ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1170453ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (687590ms till timeout)
2022-03-28 14:31:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281229ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1169572ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (687378ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (692587ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1171710ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (686951ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1170583ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1167273ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (832786ms till timeout)
2022-03-28 14:31:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1167857ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1169338ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (686477ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1168454ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280011ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (686258ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (691467ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1170590ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (685831ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1169460ms till timeout)
2022-03-28 14:31:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1166150ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1166744ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (831565ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1168226ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (685359ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1167345ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278790ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (685041ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (690357ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1169481ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (684721ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1168351ms till timeout)
2022-03-28 14:31:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1165040ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1165636ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1167117ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (830351ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (684149ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1166130ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277547ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683812ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (689128ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683602ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1168253ms till timeout)
2022-03-28 14:31:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1167234ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1163923ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1164528ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (829243ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1166007ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (683040ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1165019ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (682679ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (687994ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276315ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1167122ms till timeout)
2022-03-28 14:31:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (682471ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1166103ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1162793ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1163417ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1164898ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (828131ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 14:31:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1219769886-1995934140 in namespace http-bridge-scram-sha-st
2022-03-28 14:31:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1163907ms till timeout)
2022-03-28 14:31:54 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1219769886-1995934140
2022-03-28 14:31:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1166008ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (686777ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (681459ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 14:31:55 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-691444653-1327372672 in namespace http-bridge-tls-st
2022-03-28 14:31:55 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1219769886-1995934140 will have desired state: Ready
2022-03-28 14:31:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1219769886-1995934140 will have desired state: Ready
2022-03-28 14:31:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275104ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1164995ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-1219769886-1995934140 will have desired state: Ready not ready, will try again in 1000 ms (179798ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1161684ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-691444653-1327372672
2022-03-28 14:31:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1162281ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-691444653-1327372672 will have desired state: Ready
2022-03-28 14:31:55 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-691444653-1327372672 will have desired state: Ready
2022-03-28 14:31:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1163764ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (826999ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaUser: my-user-691444653-1327372672 will have desired state: Ready not ready, will try again in 1000 ms (179789ms till timeout)
2022-03-28 14:31:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1162777ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1164900ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1163884ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (680249ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (685565ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273884ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1160573ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-1219769886-1995934140 will have desired state: Ready not ready, will try again in 1000 ms (178686ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1161165ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (825884ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1162648ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaUser: my-user-691444653-1327372672 will have desired state: Ready not ready, will try again in 1000 ms (178675ms till timeout)
2022-03-28 14:31:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1161662ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1163791ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1162775ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (679033ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1159461ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (684347ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-1219769886-1995934140 will have desired state: Ready not ready, will try again in 1000 ms (177573ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272580ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1159950ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (824672ms till timeout)
2022-03-28 14:31:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1161436ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaUser: my-user-691444653-1327372672 will have desired state: Ready not ready, will try again in 1000 ms (177463ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1160450ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1162657ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1161642ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (677902ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaUser: my-user-1219769886-1995934140 is in desired state: Ready
2022-03-28 14:31:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (683213ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1158327ms till timeout)
2022-03-28 14:31:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271426ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1158738ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (823560ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1160325ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaUser: my-user-691444653-1327372672 is in desired state: Ready
2022-03-28 14:31:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1159335ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 14:31:59 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 14:31:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1161469ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 14:31:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1160436ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1157116ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (676686ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (681999ms till timeout)
2022-03-28 14:31:59 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 14:31:59 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 14:31:59 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 14:31:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270237ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (479787ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (822375ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1157551ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1159138ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-23] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 14:32:00 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 14:32:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1158115ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479748ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1160323ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1159305ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1155994ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (680879ms till timeout)
2022-03-28 14:32:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (675562ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269102ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (478679ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (821265ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1156337ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1157923ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1157006ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478537ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1159213ms till timeout)
2022-03-28 14:32:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1158192ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (679767ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1154880ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (674450ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267983ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (477569ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (820155ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1156812ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1155221ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1155896ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (477425ms till timeout)
2022-03-28 14:32:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1158090ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1157072ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (678646ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1153757ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (673327ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266867ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-03-28 14:32:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (819047ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 14:32:03 [ForkJoinPool-1-worker-5] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-28 14:32:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1154012ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1155598ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1154716ms till timeout)
2022-03-28 14:32:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 14:32:03 [ForkJoinPool-1-worker-23] INFO  [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-28 14:32:04 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 14:32:04 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 14:32:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1156949ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1155931ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 14:32:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (672188ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1152617ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479784ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (677502ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 14:32:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265718ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 14:32:04 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 14:32:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479887ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (817841ms till timeout)
2022-03-28 14:32:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1152903ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1154381ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1153501ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1155841ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1154820ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (671079ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (676288ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478569ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1151402ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264508ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478779ms till timeout)
2022-03-28 14:32:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (816731ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1151792ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1153273ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1152393ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1154732ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1153706ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (669965ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (675174ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1150287ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477454ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263388ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477668ms till timeout)
2022-03-28 14:32:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (815620ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1150683ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1152164ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1151283ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1153613ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1152592ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-03-28 14:32:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476343ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (674061ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1149174ms till timeout)
2022-03-28 14:32:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262272ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476552ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (814511ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1149576ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1151055ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1150173ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1152504ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1151478ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475234ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1147959ms till timeout)
2022-03-28 14:32:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (672844ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261066ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475344ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (813303ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1148374ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1149854ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1148973ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1151325ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1150304ms till timeout)
2022-03-28 14:32:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474125ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1146850ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (671629ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259850ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474132ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (812088ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1147160ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1148640ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1147759ms till timeout)
2022-03-28 14:32:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1150111ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1149089ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472944ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1145669ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (670447ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258664ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472945ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (810899ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1145969ms till timeout)
2022-03-28 14:32:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1147447ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1146566ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1148918ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1147899ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471756ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1144471ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (669249ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257407ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471723ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (809785ms till timeout)
2022-03-28 14:32:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1144857ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1146337ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1145456ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1147806ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1146787ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470642ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1143362ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (668139ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256285ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (808635ms till timeout)
2022-03-28 14:32:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470572ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1143707ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1145185ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1144304ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1146656ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1145635ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469492ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1142217ms till timeout)
2022-03-28 14:32:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (666996ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255165ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (807509ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469446ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1142581ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1144060ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1143179ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1145531ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1144511ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468367ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1141093ms till timeout)
2022-03-28 14:32:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (665871ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254047ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (806393ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468330ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1141465ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1142941ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1142052ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1144404ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1143385ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467242ms till timeout)
2022-03-28 14:32:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1139962ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (664738ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252930ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (805266ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467203ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1140342ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1141821ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1140939ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1143292ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1142271ms till timeout)
2022-03-28 14:32:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466126ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1138710ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (663596ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251814ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466092ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (804046ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1139224ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1140704ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1139822ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1142174ms till timeout)
2022-03-28 14:32:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1141155ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465012ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1137600ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (662379ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250594ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464879ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (802835ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1138012ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1139493ms till timeout)
2022-03-28 14:32:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1138612ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1140965ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1139943ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463798ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (661262ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1136376ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249470ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463748ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (801706ms till timeout)
2022-03-28 14:32:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1136883ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1138363ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1137481ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1139834ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1138814ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462672ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1135252ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (660138ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248354ms till timeout)
2022-03-28 14:32:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462635ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (800590ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1135767ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1137247ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1136365ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1138718ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1137699ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461517ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (659027ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1134141ms till timeout)
2022-03-28 14:32:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247238ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461520ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (799477ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1134654ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1136135ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1137570ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1135110ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1136558ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460271ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (657886ms till timeout)
2022-03-28 14:32:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1133000ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246064ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460382ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1133516ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (798338ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1134996ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1136459ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1133998ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1135435ms till timeout)
2022-03-28 14:32:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459147ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1131879ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (656765ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244946ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459229ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (797186ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1132364ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1133843ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1135349ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1132783ms till timeout)
2022-03-28 14:32:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1134223ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458039ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (655651ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1130765ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243831ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458113ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1131248ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (796070ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1132728ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1134236ms till timeout)
2022-03-28 14:32:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1131671ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1133110ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 10
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-303680974-1702892499 in namespace http-bridge-scram-sha-st
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-28 14:32:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1129652ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (654538ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-303680974-1702892499
2022-03-28 14:32:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242655ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-303680974-1702892499 will have desired state: Ready
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-303680974-1702892499 will have desired state: Ready
2022-03-28 14:32:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (456935ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (794893ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1130071ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1131550ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-303680974-1702892499 will have desired state: Ready not ready, will try again in 1000 ms (179786ms till timeout)
2022-03-28 14:32:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1133126ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1130561ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1132001ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (653427ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1128541ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241534ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (455820ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1128955ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (793777ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-303680974-1702892499 is in desired state: Ready
2022-03-28 14:32:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1130434ms till timeout)
2022-03-28 14:32:28 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job consumer-1617086096 in namespace http-bridge-scram-sha-st
2022-03-28 14:32:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1132013ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1129440ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1617086096
2022-03-28 14:32:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1130889ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: consumer-1617086096 will be in active state
2022-03-28 14:32:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:32:29 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 14:32:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1127364ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (652250ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job producer-789251939 in namespace http-bridge-scram-sha-st
2022-03-28 14:32:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240361ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (454645ms till timeout)
2022-03-28 14:32:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-789251939
2022-03-28 14:32:30 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1127781ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (792603ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: producer-789251939 will be in active state
2022-03-28 14:32:30 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:32:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1129261ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1130840ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:61] Waiting till producer producer-789251939 and consumer consumer-1617086096 finish
2022-03-28 14:32:30 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 14:32:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1128274ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1129714ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219789ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1126255ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (651033ms till timeout)
2022-03-28 14:32:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239217ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (453523ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (791478ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1126656ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1128136ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1129714ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1127149ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1128588ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218662ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1125145ms till timeout)
2022-03-28 14:32:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (649922ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238094ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (452376ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (790332ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1125510ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1126990ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1128558ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1125993ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1127433ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217506ms till timeout)
2022-03-28 14:32:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1124015ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-58df3988 is in desired state: Ready
2022-03-28 14:32:33 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-03-28 14:32:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236884ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (451268ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-03-28 14:32:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (789224ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-58df3988 will have desired state: ReconciliationPaused
2022-03-28 14:32:33 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-58df3988 will have desired state: ReconciliationPaused
2022-03-28 14:32:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1124295ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1125776ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1127354ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (839786ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1124789ms till timeout)
2022-03-28 14:32:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1126229ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216300ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1122811ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235769ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (450050ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (788112ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1123185ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1124662ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1126239ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-58df3988 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (838672ms till timeout)
2022-03-28 14:32:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1123675ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1125114ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215188ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1121698ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234655ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:32:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (786994ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 11
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:205] [testSendSimpleMessageTls] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testSendSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1122069ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1123549ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-58df3988 is in desired state: ReconciliationPaused
2022-03-28 14:32:35 [ForkJoinPool-1-worker-3] INFO  [PodUtils:209] Wait until Pod my-cluster-58df3988-kafka will have stable 3 replicas
2022-03-28 14:32:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1125122ms till timeout)
2022-03-28 14:32:35 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-58df3988-kafka will have 3 replicas
2022-03-28 14:32:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1122561ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1123979ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 14:32:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (179770ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214052ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1120558ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233536ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (785880ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1120952ms till timeout)
2022-03-28 14:32:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1122431ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1124009ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1121434ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1122871ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 14:32:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (178540ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212823ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1119335ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232407ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (784758ms till timeout)
2022-03-28 14:32:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1119829ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1121309ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1122888ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1120322ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1121760ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 14:32:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (177428ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211711ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1118221ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231289ms till timeout)
2022-03-28 14:32:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (783636ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1118707ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1120186ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1121765ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1119199ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1120638ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 14:32:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (176314ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210597ms till timeout)
2022-03-28 14:32:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1117105ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230148ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (782511ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1117583ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1119063ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1120641ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1118075ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testSendSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1119514ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 14:32:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (175193ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (209467ms till timeout)
2022-03-28 14:32:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1115977ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229024ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (781374ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1116446ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1117924ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1119500ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1116933ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1118373ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 14:32:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (174055ms till timeout)
2022-03-28 14:32:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (208337ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1114847ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227907ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (780250ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1115320ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1116801ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1118379ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1115813ms till timeout)
2022-03-28 14:32:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1117248ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 14:32:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (172933ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (207214ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1113725ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226786ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (779137ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1114207ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1115687ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1117265ms till timeout)
2022-03-28 14:32:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1114700ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1116138ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 14:32:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (171821ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (206103ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1112612ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225658ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (778018ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1113090ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1114570ms till timeout)
2022-03-28 14:32:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1116147ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1113582ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1115021ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 14:32:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (170704ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (204987ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1111490ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testSendSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224523ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (776869ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1111940ms till timeout)
2022-03-28 14:32:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1113421ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1114998ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1112430ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1113869ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 14:32:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (169553ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (203835ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1110345ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223402ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (775749ms till timeout)
2022-03-28 14:32:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1110819ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1112300ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1113878ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1111312ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1112750ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 14:32:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (168433ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (202715ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-0d1aa6d1 will have desired state: Ready not ready, will try again in 1000 ms (1109224ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222285ms till timeout)
2022-03-28 14:32:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (774629ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1109699ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1111179ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1112758ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1110193ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1111632ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 14:32:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (167313ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (201598ms till timeout)
2022-03-28 14:32:48 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] Kafka: my-cluster-0d1aa6d1 is in desired state: Ready
2022-03-28 14:32:48 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-0d1aa6d1-cruise-control-7f69765cfb-cfzs4 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 14:32:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221105ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (773451ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1108522ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1110002ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1111581ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1109015ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1110455ms till timeout)
2022-03-28 14:32:49 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 14:32:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (166137ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-789251939 deletion
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-789251939 to be deleted
2022-03-28 14:32:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219919ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (772263ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job producer-789251939 was deleted
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-0d1aa6d1-cruise-control-7f69765cfb-cfzs4 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] INFO  [Exec:417] Return code: 0
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of Kafka my-cluster-0d1aa6d1 in namespace namespace-0
2022-03-28 14:32:50 [ForkJoinPool-1-worker-21] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-0d1aa6d1
2022-03-28 14:32:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1107333ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1108814ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1617086096 deletion
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1617086096 to be deleted
2022-03-28 14:32:50 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testSendSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1110394ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job consumer-1617086096 was deleted
2022-03-28 14:32:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1107827ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job consumer-1617086096 in namespace http-bridge-scram-sha-st
2022-03-28 14:32:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1109267ms till timeout)
2022-03-28 14:32:50 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1617086096
2022-03-28 14:32:51 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 14:32:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (164949ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job producer-789251939 in namespace http-bridge-scram-sha-st
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-789251939
2022-03-28 14:32:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218743ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (771090ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-303680974-1702892499 in namespace http-bridge-scram-sha-st
2022-03-28 14:32:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-9ef63e31 will have desired state: Ready not ready, will try again in 1000 ms (1106164ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0d1aa6d1
2022-03-28 14:32:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1107642ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-303680974-1702892499
2022-03-28 14:32:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:32:51 [ForkJoinPool-1-worker-21] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testConfigurationFileIsCreated
2022-03-28 14:32:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1109221ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTlsScramSha - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls] to and randomly select one to start execution
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 10
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:32:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1106650ms till timeout)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 11
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:205] [testSendSimpleMessageTlsScramSha] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:51 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1108084ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-28 14:32:52 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:52 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 14:32:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (163766ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217612ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:52 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479465ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (769964ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] Kafka: my-cluster-9ef63e31 is in desired state: Ready
2022-03-28 14:32:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1106513ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Kafka: my-cluster-d0923d27 will have desired state: Ready not ready, will try again in 1000 ms (1108091ms till timeout)
2022-03-28 14:32:52 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-1 exec my-cluster-9ef63e31-cruise-control-958898d49-zmxgn -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 14:32:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1105526ms till timeout)
2022-03-28 14:32:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1106963ms till timeout)
2022-03-28 14:32:53 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 14:32:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (162652ms till timeout)
2022-03-28 14:32:53 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216496ms till timeout)
2022-03-28 14:32:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (768839ms till timeout)
2022-03-28 14:32:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1105405ms till timeout)
2022-03-28 14:32:54 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:444] Kafka: my-cluster-d0923d27 is in desired state: Ready
2022-03-28 14:32:54 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:54 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (477989ms till timeout)
2022-03-28 14:32:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1104417ms till timeout)
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace namespace-1 exec my-cluster-9ef63e31-cruise-control-958898d49-zmxgn -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of Kafka my-cluster-9ef63e31 in namespace namespace-1
2022-03-28 14:32:54 [ForkJoinPool-1-worker-25] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-1, for cruise control Kafka cluster my-cluster-9ef63e31
2022-03-28 14:32:54 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-d0923d27-cruise-control-f78577cf6-mph8g -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 14:32:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1105747ms till timeout)
2022-03-28 14:32:54 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 14:32:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (161432ms till timeout)
2022-03-28 14:32:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (767659ms till timeout)
2022-03-28 14:32:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215192ms till timeout)
2022-03-28 14:32:55 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1104209ms till timeout)
2022-03-28 14:32:55 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-9ef63e31
2022-03-28 14:32:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-f3d16cd7 will have desired state: Ready not ready, will try again in 1000 ms (1103223ms till timeout)
2022-03-28 14:32:55 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:32:55 [ForkJoinPool-1-worker-25] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testConfigurationReflection
2022-03-28 14:32:55 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:55 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476514ms till timeout)
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [Exec:417] Command: oc --namespace namespace-5 exec my-cluster-d0923d27-cruise-control-f78577cf6-mph8g -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:348] Delete all resources for testCapacityFile
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of Kafka my-cluster-d0923d27 in namespace namespace-5
2022-03-28 14:32:55 [ForkJoinPool-1-worker-19] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-5, for cruise control Kafka cluster my-cluster-d0923d27
2022-03-28 14:32:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1104533ms till timeout)
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationReflection=my-cluster-9ef63e31}
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationReflection=my-user-113568301-1979160930}
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationReflection=my-topic-1952538393-1689955181}
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients}
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-508722385-1884455142 in namespace http-bridge-tls-st
2022-03-28 14:32:55 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-28 14:32:55 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:32:55 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 14:32:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (160207ms till timeout)
2022-03-28 14:32:55 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-508722385-1884455142
2022-03-28 14:32:56 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-508722385-1884455142 will have desired state: Ready
2022-03-28 14:32:56 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-508722385-1884455142 will have desired state: Ready
2022-03-28 14:32:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (766465ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214004ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:32:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479535ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaTopic: my-topic-508722385-1884455142 will have desired state: Ready not ready, will try again in 1000 ms (179789ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1103020ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:56 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-f3d16cd7 is in desired state: Ready
2022-03-28 14:32:56 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d0923d27
2022-03-28 14:32:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1103365ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d0923d27 not ready, will try again in 10000 ms (839792ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 14:32:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-kafka will have 3 replicas not ready, will try again in 1000 ms (159048ms till timeout)
2022-03-28 14:32:56 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:57 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475029ms till timeout)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-03-28 14:32:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (765281ms till timeout)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212816ms till timeout)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-f3d16cd7-cruise-control rolling update
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:32:57 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: my-topic-508722385-1884455142 is in desired state: Ready
2022-03-28 14:32:57 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job producer-160741592 in namespace http-bridge-tls-st
2022-03-28 14:32:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1101820ms till timeout)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-160741592
2022-03-28 14:32:57 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:32:57 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478042ms till timeout)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:32:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (599596ms till timeout)
2022-03-28 14:32:57 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: producer-160741592 will be in active state
2022-03-28 14:32:57 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:32:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1102163ms till timeout)
2022-03-28 14:32:58 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:58 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-160741592 to finished
2022-03-28 14:32:58 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] INFO  [PodUtils:228] Pod my-cluster-58df3988-kafka has 3 replicas
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-03-28 14:32:58 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:32:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219674ms till timeout)
2022-03-28 14:32:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (764081ms till timeout)
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-58df3988-kafka to be ready
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 14:32:58 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:58 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473531ms till timeout)
2022-03-28 14:32:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211620ms till timeout)
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:32:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2399890ms till timeout)
2022-03-28 14:32:58 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:32:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1100626ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1101052ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:32:59 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476583ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:32:59 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218456ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (762866ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210315ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:32:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2398680ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1099422ms till timeout)
2022-03-28 14:32:59 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:32:59 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:32:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472077ms till timeout)
2022-03-28 14:33:00 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1099941ms till timeout)
2022-03-28 14:33:00 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:00 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475065ms till timeout)
2022-03-28 14:33:00 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (761753ms till timeout)
2022-03-28 14:33:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217129ms till timeout)
2022-03-28 14:33:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2397563ms till timeout)
2022-03-28 14:33:01 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209093ms till timeout)
2022-03-28 14:33:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1098304ms till timeout)
2022-03-28 14:33:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1098832ms till timeout)
2022-03-28 14:33:01 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:01 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (760642ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1097189ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2396342ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:02 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473520ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207871ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215801ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1097724ms till timeout)
2022-03-28 14:33:02 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:02 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:02 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (594377ms till timeout)
2022-03-28 14:33:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (759517ms till timeout)
2022-03-28 14:33:03 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1096080ms till timeout)
2022-03-28 14:33:03 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2395128ms till timeout)
2022-03-28 14:33:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206656ms till timeout)
2022-03-28 14:33:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214581ms till timeout)
2022-03-28 14:33:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1096533ms till timeout)
2022-03-28 14:33:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (758408ms till timeout)
2022-03-28 14:33:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1094971ms till timeout)
2022-03-28 14:33:04 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2394016ms till timeout)
2022-03-28 14:33:04 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205528ms till timeout)
2022-03-28 14:33:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1095418ms till timeout)
2022-03-28 14:33:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213258ms till timeout)
2022-03-28 14:33:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (757299ms till timeout)
2022-03-28 14:33:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1093863ms till timeout)
2022-03-28 14:33:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2392905ms till timeout)
2022-03-28 14:33:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204409ms till timeout)
2022-03-28 14:33:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1094293ms till timeout)
2022-03-28 14:33:05 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212027ms till timeout)
2022-03-28 14:33:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (756189ms till timeout)
2022-03-28 14:33:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465540ms till timeout)
2022-03-28 14:33:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1092702ms till timeout)
2022-03-28 14:33:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2391757ms till timeout)
2022-03-28 14:33:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203274ms till timeout)
2022-03-28 14:33:06 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1093154ms till timeout)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testCapacityFile
2022-03-28 14:33:07 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210672ms till timeout)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (755077ms till timeout)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1091593ms till timeout)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479450ms till timeout)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2390640ms till timeout)
2022-03-28 14:33:07 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202157ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463968ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1092036ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:08 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (589138ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209457ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (753866ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:08 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466865ms till timeout)
2022-03-28 14:33:08 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1090484ms till timeout)
2022-03-28 14:33:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2389531ms till timeout)
2022-03-28 14:33:09 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201032ms till timeout)
2022-03-28 14:33:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1090918ms till timeout)
2022-03-28 14:33:09 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:09 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (477905ms till timeout)
2022-03-28 14:33:09 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (752748ms till timeout)
2022-03-28 14:33:09 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208124ms till timeout)
2022-03-28 14:33:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1089297ms till timeout)
2022-03-28 14:33:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2388346ms till timeout)
2022-03-28 14:33:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199864ms till timeout)
2022-03-28 14:33:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1089746ms till timeout)
2022-03-28 14:33:10 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (751638ms till timeout)
2022-03-28 14:33:11 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206910ms till timeout)
2022-03-28 14:33:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1088084ms till timeout)
2022-03-28 14:33:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2387134ms till timeout)
2022-03-28 14:33:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198622ms till timeout)
2022-03-28 14:33:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1088532ms till timeout)
2022-03-28 14:33:11 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (750529ms till timeout)
2022-03-28 14:33:12 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1086975ms till timeout)
2022-03-28 14:33:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205587ms till timeout)
2022-03-28 14:33:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2385915ms till timeout)
2022-03-28 14:33:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197434ms till timeout)
2022-03-28 14:33:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1087316ms till timeout)
2022-03-28 14:33:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (749421ms till timeout)
2022-03-28 14:33:13 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:13 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:13 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (583918ms till timeout)
2022-03-28 14:33:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1085821ms till timeout)
2022-03-28 14:33:13 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job producer-160741592 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:33:08Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:33:08Z, lastTransitionTime=2022-03-28T14:33:08Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:32:53Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2384764ms till timeout)
2022-03-28 14:33:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196187ms till timeout)
2022-03-28 14:33:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (1086165ms till timeout)
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-160741592 deletion
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-160741592 to be deleted
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job producer-160741592 was deleted
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job consumer-604063354 in namespace http-bridge-tls-st
2022-03-28 14:33:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (748304ms till timeout)
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-604063354
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: consumer-604063354 will be in active state
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-604063354 to finished
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:33:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1084626ms till timeout)
2022-03-28 14:33:14 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2383558ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219563ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] Kafka: my-cluster-77376ada is in desired state: Ready
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-2100355713-204306190 in namespace namespace-7
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 14:33:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194961ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2100355713-204306190
2022-03-28 14:33:15 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:15 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456736ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:15 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:15 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (460359ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (747194ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2100355713-204306190 will have desired state: Ready
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2100355713-204306190 will have desired state: Ready
2022-03-28 14:33:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic: my-topic-2100355713-204306190 will have desired state: Ready not ready, will try again in 1000 ms (179892ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1083515ms till timeout)
2022-03-28 14:33:15 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:15 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (471349ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2382446ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218340ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:16 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193752ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (746085ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaTopic: my-topic-2100355713-204306190 is in desired state: Ready
2022-03-28 14:33:16 [ForkJoinPool-1-worker-17] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 14:33:16 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:16 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455177ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:16 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458801ms till timeout)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2100355713-204306190 will have desired state: ReconciliationPaused
2022-03-28 14:33:16 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2100355713-204306190 will have desired state: ReconciliationPaused
2022-03-28 14:33:16 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:16 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1082311ms till timeout)
2022-03-28 14:33:17 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaTopic: my-topic-2100355713-204306190 is in desired state: ReconciliationPaused
2022-03-28 14:33:17 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2381252ms till timeout)
2022-03-28 14:33:17 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217117ms till timeout)
2022-03-28 14:33:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192528ms till timeout)
2022-03-28 14:33:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (744872ms till timeout)
2022-03-28 14:33:17 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:17 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:17 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (469406ms till timeout)
2022-03-28 14:33:17 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1081203ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2380140ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:18 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:18 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (457272ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:18 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453609ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:18 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:18 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (578594ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191290ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (743732ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215760ms till timeout)
2022-03-28 14:33:18 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1080092ms till timeout)
2022-03-28 14:33:19 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:19 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:19 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:19 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (467844ms till timeout)
2022-03-28 14:33:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2379025ms till timeout)
2022-03-28 14:33:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190075ms till timeout)
2022-03-28 14:33:20 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (742516ms till timeout)
2022-03-28 14:33:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214439ms till timeout)
2022-03-28 14:33:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1078984ms till timeout)
2022-03-28 14:33:20 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2377912ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:21 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (466289ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (741302ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188855ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1077859ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213014ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2376801ms till timeout)
2022-03-28 14:33:21 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendSimpleMessageTlsScramSha is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:22 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (740191ms till timeout)
2022-03-28 14:33:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187529ms till timeout)
2022-03-28 14:33:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1076739ms till timeout)
2022-03-28 14:33:22 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2375683ms till timeout)
2022-03-28 14:33:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211685ms till timeout)
2022-03-28 14:33:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (739080ms till timeout)
2022-03-28 14:33:23 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186303ms till timeout)
2022-03-28 14:33:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1075513ms till timeout)
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job consumer-604063354 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:09Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2374465ms till timeout)
2022-03-28 14:33:24 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:24 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (573193ms till timeout)
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-604063354 deletion
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-604063354 to be deleted
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job consumer-604063354 was deleted
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job producer-160741592 in namespace http-bridge-tls-st
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-160741592
2022-03-28 14:33:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (737899ms till timeout)
2022-03-28 14:33:24 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job consumer-604063354 in namespace http-bridge-tls-st
2022-03-28 14:33:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1074304ms till timeout)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-604063354
2022-03-28 14:33:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185083ms till timeout)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2373355ms till timeout)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-508722385-1884455142 in namespace http-bridge-tls-st
2022-03-28 14:33:25 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:25 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (450407ms till timeout)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:25 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446743ms till timeout)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-508722385-1884455142
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 10
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 11
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:205] [testReceiveSimpleMessageTls] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (736791ms till timeout)
2022-03-28 14:33:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1073195ms till timeout)
2022-03-28 14:33:26 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:26 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183770ms till timeout)
2022-03-28 14:33:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2372133ms till timeout)
2022-03-28 14:33:26 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:26 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (445170ms till timeout)
2022-03-28 14:33:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (735681ms till timeout)
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testSendSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationReflection=my-cluster-9ef63e31, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb}
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationReflection=my-user-113568301-1979160930, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952}
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationReflection=my-topic-1952538393-1689955181, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374}
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients}
2022-03-28 14:33:26 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1395759873-1814979566 in namespace http-bridge-scram-sha-st
2022-03-28 14:33:27 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1395759873-1814979566
2022-03-28 14:33:27 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1395759873-1814979566 will have desired state: Ready
2022-03-28 14:33:27 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1395759873-1814979566 will have desired state: Ready
2022-03-28 14:33:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1072072ms till timeout)
2022-03-28 14:33:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-1395759873-1814979566 will have desired state: Ready not ready, will try again in 1000 ms (179781ms till timeout)
2022-03-28 14:33:27 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2371002ms till timeout)
2022-03-28 14:33:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182531ms till timeout)
2022-03-28 14:33:27 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:27 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:28 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:33:28 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 14:33:28 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (734572ms till timeout)
2022-03-28 14:33:28 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:28 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459119ms till timeout)
2022-03-28 14:33:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1070962ms till timeout)
2022-03-28 14:33:28 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:28 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (443622ms till timeout)
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-1395759873-1814979566 is in desired state: Ready
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job producer-7992292 in namespace http-bridge-scram-sha-st
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-7992292
2022-03-28 14:33:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2369789ms till timeout)
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: producer-7992292 will be in active state
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:33:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181210ms till timeout)
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-7992292 to finished
2022-03-28 14:33:28 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:33:29 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-7992292 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:29 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-67532fa3 will have desired state: Ready not ready, will try again in 1000 ms (733433ms till timeout)
2022-03-28 14:33:29 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219677ms till timeout)
2022-03-28 14:33:29 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1069843ms till timeout)
2022-03-28 14:33:29 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:29 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (567828ms till timeout)
2022-03-28 14:33:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2368677ms till timeout)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:30 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457205ms till timeout)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179987ms till timeout)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-67532fa3 is in desired state: Ready
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-850384724-1448934442 in namespace namespace-7
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 14:33:30 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-7992292 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:30 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-850384724-1448934442
2022-03-28 14:33:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218354ms till timeout)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1068662ms till timeout)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-850384724-1448934442 will have desired state: Ready
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-850384724-1448934442 will have desired state: Ready
2022-03-28 14:33:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaTopic: my-topic-850384724-1448934442 will have desired state: Ready not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 14:33:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2367567ms till timeout)
2022-03-28 14:33:31 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178769ms till timeout)
2022-03-28 14:33:31 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-7992292 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:31 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:31 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (455599ms till timeout)
2022-03-28 14:33:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1067554ms till timeout)
2022-03-28 14:33:31 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:31 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (443880ms till timeout)
2022-03-28 14:33:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217031ms till timeout)
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: my-topic-850384724-1448934442 is in desired state: Ready
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-7
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 14:33:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2366384ms till timeout)
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 14:33:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: encrypted-leopold will have desired state: Ready not ready, will try again in 1000 ms (179893ms till timeout)
2022-03-28 14:33:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177541ms till timeout)
2022-03-28 14:33:32 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:32 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1066444ms till timeout)
2022-03-28 14:33:33 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-7992292 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215814ms till timeout)
2022-03-28 14:33:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2365272ms till timeout)
2022-03-28 14:33:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-03-28 14:33:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-7
2022-03-28 14:33:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 14:33:33 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 14:33:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 14:33:33 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 14:33:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176233ms till timeout)
2022-03-28 14:33:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-03-28 14:33:34 [ForkJoinPool-1-worker-13] INFO  [UserST:346] Deploying KafkaClients pod for TLS listener
2022-03-28 14:33:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Kafka: my-cluster-b0221df6 will have desired state: Ready not ready, will try again in 1000 ms (1065228ms till timeout)
2022-03-28 14:33:34 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-7992292 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:24Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2364156ms till timeout)
2022-03-28 14:33:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-67532fa3-tls-kafka-clients in namespace namespace-7
2022-03-28 14:33:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 14:33:34 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214371ms till timeout)
2022-03-28 14:33:34 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-67532fa3-tls-kafka-clients
2022-03-28 14:33:34 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:34 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (562553ms till timeout)
2022-03-28 14:33:35 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-67532fa3-tls-kafka-clients will be ready
2022-03-28 14:33:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-67532fa3-tls-kafka-clients will be ready
2022-03-28 14:33:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175019ms till timeout)
2022-03-28 14:33:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-67532fa3-tls-kafka-clients will be ready not ready, will try again in 1000 ms (479784ms till timeout)
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-0 -o yaml
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 1
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-5]}
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:267] testConfigurationFileIsCreated - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationFileIsCreated
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 10
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 11
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:205] [testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:35 [ForkJoinPool-1-worker-21] TRACE [SuiteThreadController:210] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:35 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] Kafka: my-cluster-b0221df6 is in desired state: Ready
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb}
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952}
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374}
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients}
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1414726449-1847133478 in namespace http-bridge-tls-st
2022-03-28 14:33:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2362962ms till timeout)
2022-03-28 14:33:35 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1414726449-1847133478
2022-03-28 14:33:35 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job producer-7992292 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:33:31Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:33:31Z, lastTransitionTime=2022-03-28T14:33:31Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:33:24Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1414726449-1847133478 will have desired state: Ready
2022-03-28 14:33:35 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1414726449-1847133478 will have desired state: Ready
2022-03-28 14:33:35 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b0221df6-kafka rolling update
2022-03-28 14:33:35 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 14:33:35 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for component with name my-cluster-b0221df6-kafka rolling update
2022-03-28 14:33:35 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] KafkaTopic: my-topic-1414726449-1847133478 will have desired state: Ready not ready, will try again in 1000 ms (179801ms till timeout)
2022-03-28 14:33:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:33:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1799793ms till timeout)
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-7992292 deletion
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-7992292 to be deleted
2022-03-28 14:33:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173842ms till timeout)
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job producer-7992292 was deleted
2022-03-28 14:33:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-67532fa3-tls-kafka-clients will be ready not ready, will try again in 1000 ms (478604ms till timeout)
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job consumer-2105828907 in namespace http-bridge-scram-sha-st
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-2105828907
2022-03-28 14:33:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2361781ms till timeout)
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: consumer-2105828907 will be in active state
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-2105828907 to finished
2022-03-28 14:33:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:33:37 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaTopic: my-topic-1414726449-1847133478 is in desired state: Ready
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job consumer-1178456583 in namespace http-bridge-tls-st
2022-03-28 14:33:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219680ms till timeout)
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1178456583
2022-03-28 14:33:37 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:37 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:33:37 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-03-28 14:33:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (170586ms till timeout)
2022-03-28 14:33:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172642ms till timeout)
2022-03-28 14:33:37 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-67532fa3-tls-kafka-clients is ready
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: consumer-1178456583 will be in active state
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:33:37 [ForkJoinPool-1-worker-13] INFO  [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 14:33:37 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job producer-2082706357 in namespace http-bridge-tls-st
2022-03-28 14:33:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2360591ms till timeout)
2022-03-28 14:33:38 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-2082706357
2022-03-28 14:33:38 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-67532fa3-plain-kafka-clients in namespace namespace-7
2022-03-28 14:33:38 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 14:33:38 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: producer-2082706357 will be in active state
2022-03-28 14:33:38 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:33:38 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-67532fa3-plain-kafka-clients
2022-03-28 14:33:38 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:38 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (448931ms till timeout)
2022-03-28 14:33:38 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:38 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:38 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (437299ms till timeout)
2022-03-28 14:33:38 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:61] Waiting till producer producer-2082706357 and consumer consumer-1178456583 finish
2022-03-28 14:33:38 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 14:33:38 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219785ms till timeout)
2022-03-28 14:33:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218286ms till timeout)
2022-03-28 14:33:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171472ms till timeout)
2022-03-28 14:33:38 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-67532fa3-plain-kafka-clients will be ready
2022-03-28 14:33:38 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-67532fa3-plain-kafka-clients will be ready
2022-03-28 14:33:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-67532fa3-plain-kafka-clients will be ready not ready, will try again in 1000 ms (479893ms till timeout)
2022-03-28 14:33:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2359480ms till timeout)
2022-03-28 14:33:39 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:39 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218672ms till timeout)
2022-03-28 14:33:39 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:39 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170151ms till timeout)
2022-03-28 14:33:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-67532fa3-plain-kafka-clients will be ready not ready, will try again in 1000 ms (478777ms till timeout)
2022-03-28 14:33:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216954ms till timeout)
2022-03-28 14:33:40 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959, my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:40 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Deployment my-cluster-f3d16cd7-cruise-control rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (557241ms till timeout)
2022-03-28 14:33:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2358301ms till timeout)
2022-03-28 14:33:40 [ForkJoinPool-1-worker-21] TRACE [SuiteThreadController:210] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217563ms till timeout)
2022-03-28 14:33:41 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168934ms till timeout)
2022-03-28 14:33:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-67532fa3-plain-kafka-clients will be ready not ready, will try again in 1000 ms (477559ms till timeout)
2022-03-28 14:33:41 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:41 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:41 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:41 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:33:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1794567ms till timeout)
2022-03-28 14:33:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215629ms till timeout)
2022-03-28 14:33:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2357086ms till timeout)
2022-03-28 14:33:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216451ms till timeout)
2022-03-28 14:33:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167717ms till timeout)
2022-03-28 14:33:42 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-67532fa3-plain-kafka-clients is ready
2022-03-28 14:33:42 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:42 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2355976ms till timeout)
2022-03-28 14:33:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214306ms till timeout)
2022-03-28 14:33:42 [ForkJoinPool-1-worker-13] INFO  [UserST:357] Checking if user secrets with secret prefixes exists
2022-03-28 14:33:42 [ForkJoinPool-1-worker-13] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 14:33:42 [ForkJoinPool-1-worker-13] INFO  [UserST:373] Checking if TLS user is able to send messages
2022-03-28 14:33:43 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@252ba4e0, which are set.
2022-03-28 14:33:43 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2386e517, messages=[], arguments=[--bootstrap-server, my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093, USER=top_secret_encrypted_leopold, --topic, my-topic-303680974-1702892499, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft', podNamespace='namespace-7', bootstrapServer='my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-303680974-1702892499', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@252ba4e0}
2022-03-28 14:33:43 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:124] Producing 100 messages to my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093:my-topic-303680974-1702892499 from pod my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft
2022-03-28 14:33:43 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093 USER=top_secret_encrypted_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:33:43 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093 USER=top_secret_encrypted_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:33:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215263ms till timeout)
2022-03-28 14:33:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166597ms till timeout)
2022-03-28 14:33:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2354849ms till timeout)
2022-03-28 14:33:43 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213070ms till timeout)
2022-03-28 14:33:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214154ms till timeout)
2022-03-28 14:33:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165382ms till timeout)
2022-03-28 14:33:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2353735ms till timeout)
2022-03-28 14:33:45 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211853ms till timeout)
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f3d16cd7-cruise-control-64cfc67b4-l6pzv=85e2798e-b6ff-4ec3-8a2a-8dc877402959}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-21] TRACE [SuiteThreadController:210] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 1
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb=806dc381-cc2c-4570-8678-40d2ba8c57b6}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-f3d16cd7-cruise-control will be ready
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-f3d16cd7-cruise-control will be ready
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-5]}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testConfigurationReflection - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods] to and randomly select one to start execution
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationReflection
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 10
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-5 -o yaml
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 1
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:267] testCapacityFile - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods] to and randomly select one to start execution
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testCapacityFile
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 9
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testTopicModificationOfReplicationFactor
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 10
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testTopicModificationOfReplicationFactor test now can proceed its execution
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients}
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 11
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:205] [testSendingMessagesToNonExistingTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:45 [ForkJoinPool-1-worker-19] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-725749039-421471693 in namespace topic-st
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:168] Deployment: my-cluster-f3d16cd7-cruise-control is ready
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-725749039-421471693
2022-03-28 14:33:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212790ms till timeout)
2022-03-28 14:33:45 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-725749039-421471693 will have desired state: Ready
2022-03-28 14:33:45 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-725749039-421471693 will have desired state: Ready
2022-03-28 14:33:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164250ms till timeout)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaTopic: my-topic-725749039-421471693 will have desired state: Ready not ready, will try again in 1000 ms (179786ms till timeout)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:46 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2352532ms till timeout)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599782ms till timeout)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210636ms till timeout)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:46 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:46 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:46 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:33:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1789455ms till timeout)
2022-03-28 14:33:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211561ms till timeout)
2022-03-28 14:33:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163126ms till timeout)
2022-03-28 14:33:47 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: my-topic-725749039-421471693 is in desired state: Ready
2022-03-28 14:33:47 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2351391ms till timeout)
2022-03-28 14:33:47 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:47 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598642ms till timeout)
2022-03-28 14:33:47 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-725749039-421471693 will have desired state: NotReady
2022-03-28 14:33:47 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-725749039-421471693 will have desired state: NotReady
2022-03-28 14:33:47 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job consumer-2105828907 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:33:41Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:33:41Z, lastTransitionTime=2022-03-28T14:33:41Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:33:32Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:47 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: my-topic-725749039-421471693 is in desired state: NotReady
2022-03-28 14:33:47 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-2105828907 deletion
2022-03-28 14:33:47 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-2105828907 to be deleted
2022-03-28 14:33:48 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-725749039-421471693
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job consumer-2105828907 was deleted
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job producer-7992292 in namespace http-bridge-scram-sha-st
2022-03-28 14:33:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161952ms till timeout)
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-7992292
2022-03-28 14:33:48 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:48 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:48 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597465ms till timeout)
2022-03-28 14:33:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2350212ms till timeout)
2022-03-28 14:33:48 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-2082706357 deletion
2022-03-28 14:33:48 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-2082706357 to be deleted
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job consumer-2105828907 in namespace http-bridge-scram-sha-st
2022-03-28 14:33:48 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job producer-2082706357 was deleted
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:127] Producer finished correctly: true
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:131] Producer produced 100 messages
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-2105828907
2022-03-28 14:33:48 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1178456583 deletion
2022-03-28 14:33:48 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1178456583 to be deleted
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5c118655, which are set.
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@ec34ae4, messages=[], arguments=[--bootstrap-server, my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093, --group-id, my-consumer-group-2035696301, --group-instance-id, instance427216319, USER=top_secret_encrypted_leopold, --topic, my-topic-303680974-1702892499, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft', podNamespace='namespace-7', bootstrapServer='my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-303680974-1702892499', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-2035696301', consumerInstanceId='instance427216319', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5c118655}
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:192] Consuming 100 messages from my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093:my-topic-303680974-1702892499 from pod my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-2035696301 --group-instance-id instance427216319 USER=top_secret_encrypted_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:33:48 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-67532fa3-tls-kafka-clients-59998cc5df-96cft -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-2035696301 --group-instance-id instance427216319 USER=top_secret_encrypted_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:33:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1395759873-1814979566 in namespace http-bridge-scram-sha-st
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job consumer-1178456583 was deleted
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job consumer-1178456583 in namespace http-bridge-tls-st
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1395759873-1814979566
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1178456583
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testSendSimpleMessageTlsScramSha - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 10
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160773ms till timeout)
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:690] [bridge.HttpBridgeScramShaST - After All] - Clean up after test suite
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-725749039-421471693
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-725749039-421471693 deletion
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-725749039-421471693
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job producer-2082706357 in namespace http-bridge-tls-st
2022-03-28 14:33:49 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:49 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:49 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596291ms till timeout)
2022-03-28 14:33:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2349040ms till timeout)
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-725749039-421471693 in namespace topic-st
2022-03-28 14:33:49 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-2082706357
2022-03-28 14:33:49 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-725749039-421471693
2022-03-28 14:33:49 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1414726449-1847133478 in namespace http-bridge-tls-st
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testTopicModificationOfReplicationFactor - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testTopicModificationOfReplicationFactor
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 9
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 10
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testMoreReplicasThanAvailableBrokers test now can proceed its execution
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-450528877-1105063654 in namespace topic-st
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1414726449-1847133478
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [TopicST:461] Checking in KafkaTopic CR that topic my-topic-450528877-1105063654 exists
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] INFO  [TopicST:456] Checking topic my-topic-450528877-1105063654 in Kafka
2022-03-28 14:33:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (479368ms till timeout)
2022-03-28 14:33:50 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:230] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods test now can proceed its execution
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic, testMoreReplicasThanAvailableBrokers] to and randomly select one to start execution
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 9
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 14:33:50 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:230] testSendingMessagesToNonExistingTopic test now can proceed its execution
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] INFO  [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 14:33:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159604ms till timeout)
2022-03-28 14:33:50 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Namespace namespace-8
2022-03-28 14:33:50 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:50 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 14:33:50 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595121ms till timeout)
2022-03-28 14:33:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2347870ms till timeout)
2022-03-28 14:33:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479570ms till timeout)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c35,c5",
            "openshift.io/sa.scc.supplemental-groups": "1001200000/10000",
            "openshift.io/sa.scc.uid-range": "1001200000/10000"
        },
        "creationTimestamp": "2022-03-28T14:33:46Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-8"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:33:46Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:33:46Z"
            }
        ],
        "name": "namespace-8",
        "resourceVersion": "120050",
        "uid": "95744119-a9ab-4ba9-9394-740a5d171f60"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] INFO  [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-da099474 in namespace namespace-8
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 14:33:51 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-da099474
2022-03-28 14:33:51 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:51 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:33:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1784342ms till timeout)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-da099474 will have desired state: Ready
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-da099474 will have desired state: Ready
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] INFO  [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 14:33:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158388ms till timeout)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1319788ms till timeout)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:51 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2346655ms till timeout)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593906ms till timeout)
2022-03-28 14:33:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (479788ms till timeout)
2022-03-28 14:33:52 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:52 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:33:52 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-03-28 14:33:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (155526ms till timeout)
2022-03-28 14:33:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157173ms till timeout)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1318669ms till timeout)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2345537ms till timeout)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592787ms till timeout)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (478674ms till timeout)
2022-03-28 14:33:53 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155950ms till timeout)
2022-03-28 14:33:54 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:54 [ForkJoinPool-1-worker-19] INFO  [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-03-28 14:33:54 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:54 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591565ms till timeout)
2022-03-28 14:33:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1317348ms till timeout)
2022-03-28 14:33:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2344216ms till timeout)
2022-03-28 14:33:54 [ForkJoinPool-1-worker-19] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 14:33:54 [ForkJoinPool-1-worker-19] INFO  [TopicST:320] Checking if my-topic-1290220113-1164879046 is on topic list
2022-03-28 14:33:54 [ForkJoinPool-1-worker-19] INFO  [TopicST:456] Checking topic my-topic-1290220113-1164879046 in Kafka
2022-03-28 14:33:54 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:33:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154735ms till timeout)
2022-03-28 14:33:55 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:55 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:55 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590251ms till timeout)
2022-03-28 14:33:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2342999ms till timeout)
2022-03-28 14:33:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1316130ms till timeout)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:56 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f3d16cd7, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f3d16cd7-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589028ms till timeout)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153407ms till timeout)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1314902ms till timeout)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:56 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:33:56 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:33:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1779040ms till timeout)
2022-03-28 14:33:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2341671ms till timeout)
2022-03-28 14:33:57 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: cruise-control)
2022-03-28 14:33:57 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb not ready: tls-sidecar)
2022-03-28 14:33:57 [ForkJoinPool-1-worker-7] DEBUG [PodUtils:106] Pods my-cluster-f3d16cd7-cruise-control-6fdcff455b-2g7zb are ready
2022-03-28 14:33:57 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:141] Deployment my-cluster-f3d16cd7-cruise-control rolling update finished
2022-03-28 14:33:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1313686ms till timeout)
2022-03-28 14:33:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2340451ms till timeout)
2022-03-28 14:33:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152081ms till timeout)
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 14:33:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299884ms till timeout)
2022-03-28 14:33:58 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:33:58 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:33:58 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-03-28 14:33:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (149224ms till timeout)
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] INFO  [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-03-28 14:33:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1312575ms till timeout)
2022-03-28 14:33:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:33:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2339336ms till timeout)
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@48324ce8, which are set.
2022-03-28 14:33:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150863ms till timeout)
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@39dccc82, messages=[], arguments=[--bootstrap-server, my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092, USER=top_secret_scramed_leopold, --topic, my-topic-303680974-1702892499, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j', podNamespace='namespace-7', bootstrapServer='my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092', topicName='my-topic-303680974-1702892499', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@48324ce8}
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092:my-topic-303680974-1702892499 from pod my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092 USER=top_secret_scramed_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:33:59 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092 USER=top_secret_scramed_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:33:59 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:33:59 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:33:59 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:33:59 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:33:59 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 14:33:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298770ms till timeout)
2022-03-28 14:33:59 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1311464ms till timeout)
2022-03-28 14:34:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2338119ms till timeout)
2022-03-28 14:34:00 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149646ms till timeout)
2022-03-28 14:34:00 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:00 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:00 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:00 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 14:34:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297565ms till timeout)
2022-03-28 14:34:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (468862ms till timeout)
2022-03-28 14:34:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1310353ms till timeout)
2022-03-28 14:34:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (469134ms till timeout)
2022-03-28 14:34:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2337007ms till timeout)
2022-03-28 14:34:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148423ms till timeout)
2022-03-28 14:34:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:01 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:01 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 14:34:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296448ms till timeout)
2022-03-28 14:34:01 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:01 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:01 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:01 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1773928ms till timeout)
2022-03-28 14:34:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1309239ms till timeout)
2022-03-28 14:34:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2335890ms till timeout)
2022-03-28 14:34:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147196ms till timeout)
2022-03-28 14:34:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:02 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:02 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 14:34:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295225ms till timeout)
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [TopicST:323] Topic with name my-topic-1290220113-1164879046 is not created yet
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [TopicST:325] Trying to send messages to non-existing topic my-topic-1290220113-1164879046
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4eed934d, which are set.
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1057f56c, messages=[], arguments=[--bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1290220113-1164879046, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-7c47b4459f-5zvwd', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1290220113-1164879046', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4eed934d}
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-1290220113-1164879046 from pod topic-cluster-name-kafka-clients-7c47b4459f-5zvwd
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-5zvwd -n topic-st -- /opt/kafka/producer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1290220113-1164879046 --max-messages 100
2022-03-28 14:34:03 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-5zvwd -n topic-st -- /opt/kafka/producer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1290220113-1164879046 --max-messages 100
2022-03-28 14:34:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1308097ms till timeout)
2022-03-28 14:34:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2334780ms till timeout)
2022-03-28 14:34:03 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145976ms till timeout)
2022-03-28 14:34:04 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:04 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:04 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:04 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 14:34:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294002ms till timeout)
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-450528877-1105063654 will have desired state: NotReady
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-450528877-1105063654 will have desired state: NotReady
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: my-topic-450528877-1105063654 is in desired state: NotReady
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] INFO  [TopicST:90] Delete topic my-topic-450528877-1105063654
2022-03-28 14:34:04 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-450528877-1105063654
2022-03-28 14:34:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1306937ms till timeout)
2022-03-28 14:34:04 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2333665ms till timeout)
2022-03-28 14:34:05 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144760ms till timeout)
2022-03-28 14:34:05 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:05 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:05 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:05 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 14:34:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292786ms till timeout)
2022-03-28 14:34:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1305827ms till timeout)
2022-03-28 14:34:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2332550ms till timeout)
2022-03-28 14:34:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143544ms till timeout)
2022-03-28 14:34:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:06 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:06 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 14:34:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291572ms till timeout)
2022-03-28 14:34:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1304715ms till timeout)
2022-03-28 14:34:06 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:07 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:07 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:07 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1768816ms till timeout)
2022-03-28 14:34:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2331342ms till timeout)
2022-03-28 14:34:07 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142327ms till timeout)
2022-03-28 14:34:07 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:07 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:07 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:07 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 14:34:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290353ms till timeout)
2022-03-28 14:34:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1303606ms till timeout)
2022-03-28 14:34:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2330229ms till timeout)
2022-03-28 14:34:08 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141112ms till timeout)
2022-03-28 14:34:09 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:09 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:09 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:09 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 14:34:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289041ms till timeout)
2022-03-28 14:34:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1302497ms till timeout)
2022-03-28 14:34:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2329116ms till timeout)
2022-03-28 14:34:10 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139896ms till timeout)
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-450528877-1105063654
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-450528877-1105063654 deletion
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-450528877-1105063654
2022-03-28 14:34:10 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:10 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:10 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:10 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 14:34:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287828ms till timeout)
2022-03-28 14:34:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1301291ms till timeout)
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-03-28 14:34:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2327950ms till timeout)
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 14:34:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 14:34:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-03-28 14:34:11 [ForkJoinPool-1-worker-25] INFO  [TopicST:456] Checking topic topic-example-new in Kafka
2022-03-28 14:34:11 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (458373ms till timeout)
2022-03-28 14:34:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138778ms till timeout)
2022-03-28 14:34:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1300174ms till timeout)
2022-03-28 14:34:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:11 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:11 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 14:34:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286706ms till timeout)
2022-03-28 14:34:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2326826ms till timeout)
2022-03-28 14:34:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (458633ms till timeout)
2022-03-28 14:34:12 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1763705ms till timeout)
2022-03-28 14:34:12 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137642ms till timeout)
2022-03-28 14:34:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1299060ms till timeout)
2022-03-28 14:34:12 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:12 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 14:34:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285590ms till timeout)
2022-03-28 14:34:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2325716ms till timeout)
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6a4aa0d4, which are set.
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@174f055a, messages=[], arguments=[--bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --group-id, my-consumer-group-1264161241, --group-instance-id, instance82967280, --topic, my-topic-1290220113-1164879046, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-7c47b4459f-5zvwd', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1290220113-1164879046', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1264161241', consumerInstanceId='instance82967280', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6a4aa0d4}
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-1290220113-1164879046 from pod topic-cluster-name-kafka-clients-7c47b4459f-5zvwd
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-5zvwd -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-1264161241 --group-instance-id instance82967280 --topic my-topic-1290220113-1164879046 --max-messages 100
2022-03-28 14:34:12 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-5zvwd -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-1264161241 --group-instance-id instance82967280 --topic my-topic-1290220113-1164879046 --max-messages 100
2022-03-28 14:34:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136425ms till timeout)
2022-03-28 14:34:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1297921ms till timeout)
2022-03-28 14:34:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:13 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:13 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 14:34:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284452ms till timeout)
2022-03-28 14:34:13 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:13 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:13 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-03-28 14:34:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (134151ms till timeout)
2022-03-28 14:34:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2324606ms till timeout)
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@69afd6c2, which are set.
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7d2f3579, messages=[], arguments=[--bootstrap-server, my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092, --group-id, my-consumer-group-2035696301, --group-instance-id, instance1626303648, USER=top_secret_scramed_leopold, --topic, my-topic-303680974-1702892499, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j', podNamespace='namespace-7', bootstrapServer='my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092', topicName='my-topic-303680974-1702892499', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-2035696301', consumerInstanceId='instance1626303648', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@69afd6c2}
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092#my-topic-303680974-1702892499 from pod my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092 --group-id my-consumer-group-2035696301 --group-instance-id instance1626303648 USER=top_secret_scramed_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:34:14 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-67532fa3-plain-kafka-clients-fc8c76f8f-tch9j -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-67532fa3-kafka-bootstrap.namespace-7.svc:9092 --group-id my-consumer-group-2035696301 --group-instance-id instance1626303648 USER=top_secret_scramed_leopold --topic my-topic-303680974-1702892499 --max-messages 100
2022-03-28 14:34:14 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:14 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135209ms till timeout)
2022-03-28 14:34:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1296706ms till timeout)
2022-03-28 14:34:15 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:15 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:15 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:15 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 14:34:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283135ms till timeout)
2022-03-28 14:34:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2323470ms till timeout)
2022-03-28 14:34:16 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133891ms till timeout)
2022-03-28 14:34:16 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:16 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:16 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:16 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 14:34:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281922ms till timeout)
2022-03-28 14:34:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1295389ms till timeout)
2022-03-28 14:34:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2322256ms till timeout)
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-example-new exists
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 14:34:17 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-450528877-1105063654 in namespace topic-st
2022-03-28 14:34:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1758536ms till timeout)
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-450528877-1105063654
2022-03-28 14:34:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2321063ms till timeout)
2022-03-28 14:34:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1294094ms till timeout)
2022-03-28 14:34:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:17 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 14:34:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280625ms till timeout)
2022-03-28 14:34:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132590ms till timeout)
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testMoreReplicasThanAvailableBrokers - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers] to and randomly select one to start execution
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicViaKafka
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 9
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testCreateTopicViaKafka test now can proceed its execution
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] DEBUG [TopicST:113] Creating topic my-topic-282023942-123831967 with 3 replicas and 3 partitions
2022-03-28 14:34:17 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-282023942-123831967 --replication-factor 3 --partitions 3
2022-03-28 14:34:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2319840ms till timeout)
2022-03-28 14:34:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1292967ms till timeout)
2022-03-28 14:34:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:18 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:18 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 14:34:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279497ms till timeout)
2022-03-28 14:34:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131361ms till timeout)
2022-03-28 14:34:18 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:18 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:18 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-03-28 14:34:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (129165ms till timeout)
2022-03-28 14:34:19 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:19 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2318625ms till timeout)
2022-03-28 14:34:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1291752ms till timeout)
2022-03-28 14:34:20 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:20 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:20 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:20 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 14:34:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278185ms till timeout)
2022-03-28 14:34:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130049ms till timeout)
2022-03-28 14:34:21 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2317513ms till timeout)
2022-03-28 14:34:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1290539ms till timeout)
2022-03-28 14:34:21 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:21 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:21 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:21 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 14:34:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277066ms till timeout)
2022-03-28 14:34:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128925ms till timeout)
2022-03-28 14:34:21 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 14:34:21 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 14:34:21 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1219769886-1995934140 in namespace http-bridge-scram-sha-st
2022-03-28 14:34:21 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1219769886-1995934140
2022-03-28 14:34:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1219769886-1995934140 not ready, will try again in 10000 ms (179773ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 14:34:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2316351ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1289377ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:22 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:22 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 14:34:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275908ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 14:34:22 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-282023942-123831967 --replication-factor 3 --partitions 3
2022-03-28 14:34:22 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:22 [ForkJoinPool-1-worker-25] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-282023942-123831967 creation 
2022-03-28 14:34:22 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-282023942-123831967
2022-03-28 14:34:22 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127740ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaUser my-user-691444653-1327372672 in namespace http-bridge-tls-st
2022-03-28 14:34:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaTopic creation my-topic-282023942-123831967 not ready, will try again in 1000 ms (179773ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:22 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:22 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1753367ms till timeout)
2022-03-28 14:34:22 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-691444653-1327372672
2022-03-28 14:34:22 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 14:34:23 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 14:34:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839888ms till timeout)
2022-03-28 14:34:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2315203ms till timeout)
2022-03-28 14:34:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1288230ms till timeout)
2022-03-28 14:34:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:23 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:23 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 14:34:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274761ms till timeout)
2022-03-28 14:34:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126611ms till timeout)
2022-03-28 14:34:23 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 14:34:23 [ForkJoinPool-1-worker-19] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 14:34:23 [ForkJoinPool-1-worker-19] INFO  [TopicST:341] Checking if my-topic-1290220113-1164879046 is on topic list
2022-03-28 14:34:23 [ForkJoinPool-1-worker-19] INFO  [TopicST:456] Checking topic my-topic-1290220113-1164879046 in Kafka
2022-03-28 14:34:23 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:23 [ForkJoinPool-1-worker-25] INFO  [TopicST:482] Checking in KafkaTopic CR that topic my-topic-282023942-123831967 was created with expected settings
2022-03-28 14:34:23 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:24 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2314063ms till timeout)
2022-03-28 14:34:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1287094ms till timeout)
2022-03-28 14:34:24 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:24 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:24 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:24 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 14:34:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273625ms till timeout)
2022-03-28 14:34:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125481ms till timeout)
2022-03-28 14:34:25 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2312951ms till timeout)
2022-03-28 14:34:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1285977ms till timeout)
2022-03-28 14:34:25 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:25 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:25 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:25 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 14:34:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272507ms till timeout)
2022-03-28 14:34:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124358ms till timeout)
2022-03-28 14:34:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2311839ms till timeout)
2022-03-28 14:34:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1284865ms till timeout)
2022-03-28 14:34:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:26 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:26 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 14:34:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271395ms till timeout)
2022-03-28 14:34:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123240ms till timeout)
2022-03-28 14:34:27 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:27 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:27 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-03-28 14:34:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (120699ms till timeout)
2022-03-28 14:34:27 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:27 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:27 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:27 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1748254ms till timeout)
2022-03-28 14:34:27 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:27 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2310728ms till timeout)
2022-03-28 14:34:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1283750ms till timeout)
2022-03-28 14:34:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:28 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:28 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 14:34:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270174ms till timeout)
2022-03-28 14:34:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121930ms till timeout)
2022-03-28 14:34:28 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2309618ms till timeout)
2022-03-28 14:34:29 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1282636ms till timeout)
2022-03-28 14:34:29 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:29 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:29 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:29 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 14:34:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269060ms till timeout)
2022-03-28 14:34:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120711ms till timeout)
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [UserST:398] Deleting KafkaUser:scramed-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [UserST:402] Checking if secrets are deleted
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-03-28 14:34:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-encrypted-leopold deleted
2022-03-28 14:34:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2308503ms till timeout)
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-scramed-leopold deleted
2022-03-28 14:34:30 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1281424ms till timeout)
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-03-28 14:34:30 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:30 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:30 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:30 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 14:34:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267846ms till timeout)
2022-03-28 14:34:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119525ms till timeout)
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-7
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 14:34:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-67532fa3-plain-kafka-clients in namespace namespace-7
2022-03-28 14:34:31 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-plain-kafka-clients
2022-03-28 14:34:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2307389ms till timeout)
2022-03-28 14:34:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1280308ms till timeout)
2022-03-28 14:34:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:31 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:31 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 14:34:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266733ms till timeout)
2022-03-28 14:34:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-plain-kafka-clients not ready, will try again in 10000 ms (479468ms till timeout)
2022-03-28 14:34:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118380ms till timeout)
2022-03-28 14:34:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 14:34:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 14:34:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2306263ms till timeout)
2022-03-28 14:34:32 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:34:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1279164ms till timeout)
2022-03-28 14:34:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:32 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:32 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 14:34:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265575ms till timeout)
2022-03-28 14:34:32 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st removal
2022-03-28 14:34:32 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:32 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:32 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:32 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:32 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1743140ms till timeout)
2022-03-28 14:34:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117256ms till timeout)
2022-03-28 14:34:33 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:33 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (479523ms till timeout)
2022-03-28 14:34:33 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:33 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:33 [ForkJoinPool-1-worker-25] INFO  [TopicST:121] Editing topic via Kafka, settings to partitions 5
2022-03-28 14:34:33 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-282023942-123831967 --partitions 5
2022-03-28 14:34:33 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1290220113-1164879046 creation 
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1290220113-1164879046
2022-03-28 14:34:33 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-28 14:34:33 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2305092ms till timeout)
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [TopicST:353] Topic successfully created
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 14:34:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1278006ms till timeout)
2022-03-28 14:34:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:33 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:33 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 14:34:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264431ms till timeout)
2022-03-28 14:34:33 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 14:34:33 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:33 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479530ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116056ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (479555ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:34 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:34 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-03-28 14:34:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (113637ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2303982ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:34 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (477995ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1276897ms till timeout)
2022-03-28 14:34:34 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:34 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:34 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:34 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:34 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 14:34:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263318ms till timeout)
2022-03-28 14:34:35 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:35 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478048ms till timeout)
2022-03-28 14:34:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114842ms till timeout)
2022-03-28 14:34:35 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:35 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2302870ms till timeout)
2022-03-28 14:34:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1275786ms till timeout)
2022-03-28 14:34:35 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:35 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:35 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:35 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:35 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 14:34:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262206ms till timeout)
2022-03-28 14:34:36 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:36 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (476511ms till timeout)
2022-03-28 14:34:36 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113629ms till timeout)
2022-03-28 14:34:36 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:36 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476571ms till timeout)
2022-03-28 14:34:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2301758ms till timeout)
2022-03-28 14:34:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1274676ms till timeout)
2022-03-28 14:34:36 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:37 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:37 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 14:34:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261094ms till timeout)
2022-03-28 14:34:37 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-282023942-123831967 --partitions 5
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] DEBUG [TopicST:124] Topic my-topic-282023942-123831967 updated from 3 to 5 partitions
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-282023942-123831967
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-282023942-123831967
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Describing topic my-topic-282023942-123831967 using pod CLI
2022-03-28 14:34:37 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-282023942-123831967
2022-03-28 14:34:37 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:37 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (474947ms till timeout)
2022-03-28 14:34:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112407ms till timeout)
2022-03-28 14:34:37 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:37 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:37 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:37 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:37 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1738028ms till timeout)
2022-03-28 14:34:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2300555ms till timeout)
2022-03-28 14:34:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1273566ms till timeout)
2022-03-28 14:34:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:38 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:38 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 14:34:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259971ms till timeout)
2022-03-28 14:34:38 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:38 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (475038ms till timeout)
2022-03-28 14:34:38 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111185ms till timeout)
2022-03-28 14:34:39 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:39 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:39 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-03-28 14:34:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (108931ms till timeout)
2022-03-28 14:34:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2299441ms till timeout)
2022-03-28 14:34:39 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:39 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (473466ms till timeout)
2022-03-28 14:34:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1272451ms till timeout)
2022-03-28 14:34:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:39 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:39 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:39 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 14:34:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258860ms till timeout)
2022-03-28 14:34:39 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:39 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473582ms till timeout)
2022-03-28 14:34:40 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:40 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109970ms till timeout)
2022-03-28 14:34:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2298235ms till timeout)
2022-03-28 14:34:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1271338ms till timeout)
2022-03-28 14:34:40 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:40 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:40 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:40 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:40 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 14:34:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257745ms till timeout)
2022-03-28 14:34:40 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:40 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (471998ms till timeout)
2022-03-28 14:34:40 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:41 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:41 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (472071ms till timeout)
2022-03-28 14:34:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108752ms till timeout)
2022-03-28 14:34:41 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1270149ms till timeout)
2022-03-28 14:34:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2297018ms till timeout)
2022-03-28 14:34:41 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:41 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:41 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:41 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 14:34:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256579ms till timeout)
2022-03-28 14:34:41 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-plain-kafka-clients not ready, will try again in 10000 ms (469101ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (470524ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-282023942-123831967
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [TopicST:470] Checking topic my-topic-282023942-123831967 in Kafka topic-cluster-name
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [TopicST:471] Topic my-topic-282023942-123831967 info: [Topic:my-topic-282023942-123831967, TopicId:_TYdd4LzRcGZODx-cZc1MA, PartitionCount:5, ReplicationFactor:3, Configs:min.insync.replicas=2,message.format.version=3.0-IV1, Topic:my-topic-282023942-123831967, Partition:0, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-282023942-123831967, Partition:1, Leader:2, Replicas:2,1,0, Isr:2,1,0, Topic:my-topic-282023942-123831967, Partition:2, Leader:1, Replicas:1,0,2, Isr:1,0,2, Topic:my-topic-282023942-123831967, Partition:3, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-282023942-123831967, Partition:4, Leader:1, Replicas:1,0,2, Isr:1,0,2]
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testCreateTopicViaKafka - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicViaKafka
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 9
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testCreateTopicAfterUnsupportedOperation test now can proceed its execution
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 14:34:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107533ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1269027ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 14:34:42 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2295887ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:42 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 14:34:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255456ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:42 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (470583ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: Ready not ready, will try again in 1000 ms (179791ms till timeout)
2022-03-28 14:34:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:34:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1732918ms till timeout)
2022-03-28 14:34:43 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (469018ms till timeout)
2022-03-28 14:34:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:43 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106315ms till timeout)
2022-03-28 14:34:43 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1267795ms till timeout)
2022-03-28 14:34:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2294664ms till timeout)
2022-03-28 14:34:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:43 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:43 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 14:34:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254244ms till timeout)
2022-03-28 14:34:43 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-03-28 14:34:44 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:44 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:44 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-03-28 14:34:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (103848ms till timeout)
2022-03-28 14:34:44 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:44 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (469097ms till timeout)
2022-03-28 14:34:44 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 14:34:44 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 14:34:44 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-03-28 14:34:44 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (468920ms till timeout)
2022-03-28 14:34:44 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:44 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-03-28 14:34:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1266606ms till timeout)
2022-03-28 14:34:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2293472ms till timeout)
2022-03-28 14:34:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105098ms till timeout)
2022-03-28 14:34:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (467548ms till timeout)
2022-03-28 14:34:45 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:45 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:45 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:45 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:45 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 14:34:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253031ms till timeout)
2022-03-28 14:34:45 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:another-topic
2022-03-28 14:34:45 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:45 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 14:34:45 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 14:34:45 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-03-28 14:34:45 [ForkJoinPool-1-worker-25] INFO  [TopicST:456] Checking topic topic-with-replication-to-change in Kafka
2022-03-28 14:34:45 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:45 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:45 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (467554ms till timeout)
2022-03-28 14:34:46 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:46 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1265496ms till timeout)
2022-03-28 14:34:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2292256ms till timeout)
2022-03-28 14:34:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103785ms till timeout)
2022-03-28 14:34:46 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:46 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:46 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:46 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 14:34:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251813ms till timeout)
2022-03-28 14:34:46 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:46 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (466060ms till timeout)
2022-03-28 14:34:46 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:47 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:47 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (466073ms till timeout)
2022-03-28 14:34:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1264385ms till timeout)
2022-03-28 14:34:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2291138ms till timeout)
2022-03-28 14:34:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:47 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:47 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 14:34:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250701ms till timeout)
2022-03-28 14:34:47 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102566ms till timeout)
2022-03-28 14:34:47 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (464569ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:34:48 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:34:48 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-0 hasn't rolled
2022-03-28 14:34:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1727804ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1263276ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2290021ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:48 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:48 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 14:34:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249578ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101434ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:48 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (464577ms till timeout)
2022-03-28 14:34:48 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:48 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:48 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-03-28 14:34:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (99058ms till timeout)
2022-03-28 14:34:49 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1262167ms till timeout)
2022-03-28 14:34:49 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:49 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (463010ms till timeout)
2022-03-28 14:34:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2288911ms till timeout)
2022-03-28 14:34:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:49 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:49 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 14:34:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248466ms till timeout)
2022-03-28 14:34:49 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:49 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:49 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:49 [ForkJoinPool-1-worker-25] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-03-28 14:34:49 [ForkJoinPool-1-worker-25] INFO  [TopicST:456] Checking topic another-topic in Kafka
2022-03-28 14:34:49 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:49 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100221ms till timeout)
2022-03-28 14:34:50 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:50 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (463070ms till timeout)
2022-03-28 14:34:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1261057ms till timeout)
2022-03-28 14:34:50 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:50 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2287800ms till timeout)
2022-03-28 14:34:50 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:50 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:50 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:50 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 14:34:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247356ms till timeout)
2022-03-28 14:34:51 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:51 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (461518ms till timeout)
2022-03-28 14:34:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99006ms till timeout)
2022-03-28 14:34:51 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1259948ms till timeout)
2022-03-28 14:34:51 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:51 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (461601ms till timeout)
2022-03-28 14:34:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2286690ms till timeout)
2022-03-28 14:34:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:51 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:51 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 14:34:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (246241ms till timeout)
2022-03-28 14:34:52 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97878ms till timeout)
2022-03-28 14:34:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-plain-kafka-clients not ready, will try again in 10000 ms (458600ms till timeout)
2022-03-28 14:34:52 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:52 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (460042ms till timeout)
2022-03-28 14:34:52 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1258839ms till timeout)
2022-03-28 14:34:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2285579ms till timeout)
2022-03-28 14:34:52 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:53 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:53 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 14:34:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245127ms till timeout)
2022-03-28 14:34:53 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:53 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:53 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (460106ms till timeout)
2022-03-28 14:34:53 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:34:53 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:34:53 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-0 hasn't rolled
2022-03-28 14:34:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1722621ms till timeout)
2022-03-28 14:34:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96759ms till timeout)
2022-03-28 14:34:53 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1257729ms till timeout)
2022-03-28 14:34:54 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:54 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:54 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-03-28 14:34:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (93957ms till timeout)
2022-03-28 14:34:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2284469ms till timeout)
2022-03-28 14:34:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (458507ms till timeout)
2022-03-28 14:34:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:54 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:54 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 14:34:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244014ms till timeout)
2022-03-28 14:34:54 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95539ms till timeout)
2022-03-28 14:34:54 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:54 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (458579ms till timeout)
2022-03-28 14:34:55 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:34:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1256599ms till timeout)
2022-03-28 14:34:55 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:55 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2283253ms till timeout)
2022-03-28 14:34:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (458381ms till timeout)
2022-03-28 14:34:55 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:55 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:55 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:55 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 14:34:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (242808ms till timeout)
2022-03-28 14:34:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (457015ms till timeout)
2022-03-28 14:34:55 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 14:34:55 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:34:55 [ForkJoinPool-1-worker-25] INFO  [TopicST:461] Checking in KafkaTopic CR that topic another-topic exists
2022-03-28 14:34:55 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 14:34:55 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94321ms till timeout)
2022-03-28 14:34:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1255489ms till timeout)
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion topic-with-replication-to-change
2022-03-28 14:34:56 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:56 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (457077ms till timeout)
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2282120ms till timeout)
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce, my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4}
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] DEBUG [RollingUpdateUtils:50] At least my-cluster-f3d16cd7-kafka-2 hasn't rolled
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [RollingUpdateUtils:143] {my-cluster-f3d16cd7-kafka-2=dca21ead-ab18-4bc6-91f5-b6bbd80348e4, my-cluster-f3d16cd7-kafka-0=cd30202e-1e54-4930-9e81-87a3cbf56af3, my-cluster-f3d16cd7-kafka-1=3c64df41-9b52-4043-944f-0b4f62a995ce} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-03-28 14:34:56 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-f3d16cd7 in namespace namespace-2
2022-03-28 14:34:56 [ForkJoinPool-1-worker-7] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-f3d16cd7
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-03-28 14:34:56 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion another-topic
2022-03-28 14:34:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93193ms till timeout)
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-03-28 14:34:57 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:57 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (455527ms till timeout)
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:another-topic
2022-03-28 14:34:57 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 14:34:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1254280ms till timeout)
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 14:34:57 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2280936ms till timeout)
2022-03-28 14:34:57 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:57 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (455537ms till timeout)
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testCreateTopicAfterUnsupportedOperation - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation] to and randomly select one to start execution
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUserWithQuotas
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 9
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testTlsExternalUserWithQuotas test now can proceed its execution
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:34:57 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-759590758-326116942 in namespace user-st
2022-03-28 14:34:58 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-759590758-326116942
2022-03-28 14:34:58 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92036ms till timeout)
2022-03-28 14:34:58 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-759590758-326116942 will have desired state: Ready
2022-03-28 14:34:58 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-759590758-326116942 will have desired state: Ready
2022-03-28 14:34:58 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:34:58 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f3d16cd7
2022-03-28 14:34:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaUser: my-user-759590758-326116942 will have desired state: Ready not ready, will try again in 1000 ms (179778ms till timeout)
2022-03-28 14:34:58 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:34:58 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:34:58 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-0 hasn't rolled
2022-03-28 14:34:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1717444ms till timeout)
2022-03-28 14:34:58 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:58 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (454062ms till timeout)
2022-03-28 14:34:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1253100ms till timeout)
2022-03-28 14:34:58 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:34:58 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testConfigurationPerformanceOptions
2022-03-28 14:34:58 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-28 14:34:58 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:34:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2279738ms till timeout)
2022-03-28 14:34:58 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:34:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479536ms till timeout)
2022-03-28 14:34:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90921ms till timeout)
2022-03-28 14:34:59 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:34:59 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:34:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (454036ms till timeout)
2022-03-28 14:34:59 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaUser: my-user-759590758-326116942 is in desired state: Ready
2022-03-28 14:34:59 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:34:59 [ForkJoinPool-1-worker-25] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-759590758-326116942
2022-03-28 14:34:59 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-759590758-326116942
2022-03-28 14:34:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1251893ms till timeout)
2022-03-28 14:34:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:34:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2278628ms till timeout)
2022-03-28 14:35:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:35:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (452604ms till timeout)
2022-03-28 14:35:00 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:00 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:35:00 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:00 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:00 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-03-28 14:35:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (87545ms till timeout)
2022-03-28 14:35:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89701ms till timeout)
2022-03-28 14:35:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478005ms till timeout)
2022-03-28 14:35:00 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:35:00 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (452493ms till timeout)
2022-03-28 14:35:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1250780ms till timeout)
2022-03-28 14:35:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2277517ms till timeout)
2022-03-28 14:35:01 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:35:01 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:35:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (451145ms till timeout)
2022-03-28 14:35:01 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88439ms till timeout)
2022-03-28 14:35:01 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:35:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1249669ms till timeout)
2022-03-28 14:35:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2276404ms till timeout)
2022-03-28 14:35:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476525ms till timeout)
2022-03-28 14:35:02 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:35:02 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (451000ms till timeout)
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:35:02 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-67532fa3-tls-kafka-clients in namespace namespace-7
2022-03-28 14:35:02 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-tls-kafka-clients
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-scram-sha-st" not found
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:254] HttpBridgeScramShaST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeScramShaST] - Removing parallel suite: HttpBridgeScramShaST
2022-03-28 14:35:02 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
2022-03-28 14:35:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87169ms till timeout)
[INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 415.907 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 10
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateAlterPartitions test now can proceed its execution
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1020460016-1914644203 in namespace throttling-quota-st
2022-03-28 14:35:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1248560ms till timeout)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-tls-kafka-clients not ready, will try again in 10000 ms (479458ms till timeout)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1020460016-1914644203
2022-03-28 14:35:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2275217ms till timeout)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1020460016-1914644203 will have desired state: Ready
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1020460016-1914644203 will have desired state: Ready
2022-03-28 14:35:03 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-1020460016-1914644203 will have desired state: Ready not ready, will try again in 1000 ms (179894ms till timeout)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-0 hasn't rolled
2022-03-28 14:35:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1712262ms till timeout)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475047ms till timeout)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace http-bridge-tls-st -o yaml
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Return code: 1
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-2], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 416.723 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testKafkaAdminTopicOperations
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 11
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:205] [testKafkaAdminTopicOperations] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:03 [ForkJoinPool-1-worker-23] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85948ms till timeout)
2022-03-28 14:35:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1247448ms till timeout)
2022-03-28 14:35:04 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2274106ms till timeout)
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-759590758-326116942
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-759590758-326116942
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-759590758-326116942
2022-03-28 14:35:04 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:04 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaUser: my-user-1020460016-1914644203 is in desired state: Ready
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] INFO  [KafkaUserUtils:75] KafkaUser my-user-759590758-326116942 deleted
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=my-user-759590758-326116942 attributes will be cleaned
2022-03-28 14:35:04 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-759590758-326116942
2022-03-28 14:35:04 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-93bfec4c-kafka-clients in namespace throttling-quota-st
2022-03-28 14:35:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-93bfec4c-kafka-clients
2022-03-28 14:35:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473521ms till timeout)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-93bfec4c-kafka-clients will be in active state
2022-03-28 14:35:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:35:05 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:05 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:05 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-03-28 14:35:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (82728ms till timeout)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1246260ms till timeout)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84757ms till timeout)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-5] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:35:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:267] testSendingMessagesToNonExistingTopic - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testKafkaAdminTopicOperations] to and randomly select one to start execution
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 10
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 11
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:205] [testThrottlingQuotasDeleteTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-19] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2272914ms till timeout)
2022-03-28 14:35:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299784ms till timeout)
2022-03-28 14:35:06 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:06 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1245149ms till timeout)
2022-03-28 14:35:06 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:06 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472013ms till timeout)
2022-03-28 14:35:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83436ms till timeout)
2022-03-28 14:35:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2271800ms till timeout)
2022-03-28 14:35:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298672ms till timeout)
2022-03-28 14:35:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1244039ms till timeout)
2022-03-28 14:35:07 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82222ms till timeout)
2022-03-28 14:35:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2270486ms till timeout)
2022-03-28 14:35:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297459ms till timeout)
2022-03-28 14:35:08 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:08 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470542ms till timeout)
2022-03-28 14:35:08 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1242927ms till timeout)
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:230] testKafkaAdminTopicOperations test now can proceed its execution
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1088363724-759527614 in namespace throttling-quota-st
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-759590758-326116942
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-03-28 14:35:08 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaUser my-user-759590758-326116942 in namespace user-st
2022-03-28 14:35:08 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:08 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-0 hasn't rolled
2022-03-28 14:35:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1706952ms till timeout)
2022-03-28 14:35:08 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1088363724-759527614
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-759590758-326116942
2022-03-28 14:35:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81007ms till timeout)
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1088363724-759527614 will have desired state: Ready
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1088363724-759527614 will have desired state: Ready
2022-03-28 14:35:09 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2269372ms till timeout)
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testTlsExternalUserWithQuotas - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testThrottlingQuotasDeleteTopic] to and randomly select one to start execution
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUserWithQuotas
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 10
2022-03-28 14:35:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296244ms till timeout)
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserTemplate
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 11
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:205] [testUserTemplate] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:09 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:444] KafkaUser: my-user-1088363724-759527614 is in desired state: Ready
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:35:09 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:09 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469052ms till timeout)
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-431951f6-kafka-clients will be in active state
2022-03-28 14:35:09 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:35:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1241751ms till timeout)
2022-03-28 14:35:10 [ForkJoinPool-1-worker-23] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:35:10 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:35:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119890ms till timeout)
2022-03-28 14:35:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79894ms till timeout)
2022-03-28 14:35:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2268158ms till timeout)
2022-03-28 14:35:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295024ms till timeout)
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:230] testThrottlingQuotasDeleteTopic test now can proceed its execution
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1322201233-666179901 in namespace throttling-quota-st
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1322201233-666179901
2022-03-28 14:35:10 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1322201233-666179901 will have desired state: Ready
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1322201233-666179901 will have desired state: Ready
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:444] KafkaUser: my-user-1322201233-666179901 is in desired state: Ready
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-03-28 14:35:10 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:35:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1240588ms till timeout)
2022-03-28 14:35:11 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:11 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467592ms till timeout)
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:35:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118733ms till timeout)
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:35:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78765ms till timeout)
2022-03-28 14:35:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2267028ms till timeout)
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:35:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293898ms till timeout)
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219783ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1239479ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117619ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77647ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2265913ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (466076ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292779ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:12 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:12 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-03-28 14:35:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (75258ms till timeout)
2022-03-28 14:35:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218558ms till timeout)
2022-03-28 14:35:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1238362ms till timeout)
2022-03-28 14:35:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116502ms till timeout)
2022-03-28 14:35:13 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76427ms till timeout)
2022-03-28 14:35:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:13 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2264796ms till timeout)
2022-03-28 14:35:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-tls-kafka-clients not ready, will try again in 10000 ms (468827ms till timeout)
2022-03-28 14:35:13 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:35:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291557ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:14 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:14 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-0 hasn't rolled
2022-03-28 14:35:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1701839ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:14 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (464586ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:14 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217334ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1237253ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115391ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75313ms till timeout)
2022-03-28 14:35:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2263573ms till timeout)
2022-03-28 14:35:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290444ms till timeout)
2022-03-28 14:35:15 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216117ms till timeout)
2022-03-28 14:35:15 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:15 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463121ms till timeout)
2022-03-28 14:35:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1236066ms till timeout)
2022-03-28 14:35:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114276ms till timeout)
2022-03-28 14:35:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74198ms till timeout)
2022-03-28 14:35:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2262460ms till timeout)
2022-03-28 14:35:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289330ms till timeout)
2022-03-28 14:35:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1234958ms till timeout)
2022-03-28 14:35:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214796ms till timeout)
2022-03-28 14:35:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113099ms till timeout)
2022-03-28 14:35:17 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:17 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (461631ms till timeout)
2022-03-28 14:35:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73079ms till timeout)
2022-03-28 14:35:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2261339ms till timeout)
2022-03-28 14:35:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288208ms till timeout)
2022-03-28 14:35:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1233846ms till timeout)
2022-03-28 14:35:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111985ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213468ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71910ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2260175ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:18 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:18 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-03-28 14:35:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (69628ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287046ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (460151ms till timeout)
2022-03-28 14:35:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1232735ms till timeout)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=70c32876-c5fc-479e-b8d4-e2d348ac6365, my-cluster-b0221df6-kafka-1=fcb63153-2bb5-49d9-a320-9bd84487e9fe, my-cluster-b0221df6-kafka-2=28c16053-3c1b-4be3-b2e1-8de62cf4c91f}
2022-03-28 14:35:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110868ms till timeout)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-b0221df6-kafka has been successfully rolled
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 14:35:19 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212248ms till timeout)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2259055ms till timeout)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-b0221df6-kafka to be ready
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 14:35:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70583ms till timeout)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285931ms till timeout)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799798ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:20 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (458670ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1231627ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109758ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211031ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2257847ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798686ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284716ms till timeout)
2022-03-28 14:35:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69268ms till timeout)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1230516ms till timeout)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108647ms till timeout)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457192ms till timeout)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2256734ms till timeout)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209707ms till timeout)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797575ms till timeout)
2022-03-28 14:35:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283498ms till timeout)
2022-03-28 14:35:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68145ms till timeout)
2022-03-28 14:35:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1229404ms till timeout)
2022-03-28 14:35:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107533ms till timeout)
2022-03-28 14:35:22 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2255623ms till timeout)
2022-03-28 14:35:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (455714ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:23 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796463ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282387ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208377ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66921ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1228294ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106421ms till timeout)
2022-03-28 14:35:23 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2254471ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795312ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281233ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-67532fa3-tls-kafka-clients not ready, will try again in 10000 ms (458285ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65679ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1227176ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:24 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454212ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207018ms till timeout)
2022-03-28 14:35:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105309ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2253361ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794198ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280120ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64565ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1225962ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205799ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104097ms till timeout)
2022-03-28 14:35:25 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:25 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (452735ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2252250ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:26 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:26 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-03-28 14:35:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (61692ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793088ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279008ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63451ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1224847ms till timeout)
2022-03-28 14:35:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:26 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204576ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102876ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:27 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2251133ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (451219ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791970ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277894ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62330ms till timeout)
2022-03-28 14:35:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1223724ms till timeout)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203361ms till timeout)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101662ms till timeout)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2250022ms till timeout)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790857ms till timeout)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276770ms till timeout)
2022-03-28 14:35:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61210ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1222611ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:29 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (449629ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202140ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100440ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2248844ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789683ms till timeout)
2022-03-28 14:35:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275604ms till timeout)
2022-03-28 14:35:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60096ms till timeout)
2022-03-28 14:35:30 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:30 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1221492ms till timeout)
2022-03-28 14:35:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (448103ms till timeout)
2022-03-28 14:35:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200922ms till timeout)
2022-03-28 14:35:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99223ms till timeout)
2022-03-28 14:35:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2247627ms till timeout)
2022-03-28 14:35:31 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788467ms till timeout)
2022-03-28 14:35:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274389ms till timeout)
2022-03-28 14:35:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58930ms till timeout)
2022-03-28 14:35:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1220328ms till timeout)
2022-03-28 14:35:31 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199705ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98007ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:32 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (446614ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2246412ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787251ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273173ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57714ms till timeout)
2022-03-28 14:35:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1219110ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:33 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:33 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:33 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-03-28 14:35:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (54971ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96895ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198378ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2245193ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786034ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271957ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:33 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (445092ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56498ms till timeout)
2022-03-28 14:35:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1217894ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95782ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2244071ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197038ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-850384724-1448934442 in namespace namespace-7
2022-03-28 14:35:34 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784906ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270829ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55372ms till timeout)
2022-03-28 14:35:34 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-850384724-1448934442
2022-03-28 14:35:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Kafka: my-cluster-da099474 will have desired state: Ready not ready, will try again in 1000 ms (1216772ms till timeout)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-7
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-2 -o yaml
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testConfigurationPerformanceOptions - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testUserTemplate] to and randomly select one to start execution
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationPerformanceOptions
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 10
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 11
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testUpdateUser] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-67532fa3 in namespace namespace-7
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-67532fa3
2022-03-28 14:35:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94583ms till timeout)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 14:35:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2242858ms till timeout)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:35 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783680ms till timeout)
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-28 14:35:35 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269610ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:444] Kafka: my-cluster-da099474 is in desired state: Ready
2022-03-28 14:35:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195599ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54132ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479507ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93426ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-21] INFO  [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-03-28 14:35:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2241724ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-0)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782564ms till timeout)
2022-03-28 14:35:36 [ForkJoinPool-1-worker-21] INFO  [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-03-28 14:35:36 [ForkJoinPool-1-worker-21] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-da099474-cruise-control rolling update
2022-03-28 14:35:36 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8
2022-03-28 14:35:36 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268479ms till timeout)
2022-03-28 14:35:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52915ms till timeout)
2022-03-28 14:35:37 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:37 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:37 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (599567ms till timeout)
2022-03-28 14:35:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194252ms till timeout)
2022-03-28 14:35:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92315ms till timeout)
2022-03-28 14:35:37 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:37 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478001ms till timeout)
2022-03-28 14:35:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2240613ms till timeout)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781451ms till timeout)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267367ms till timeout)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51800ms till timeout)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:38 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:38 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-03-28 14:35:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (49593ms till timeout)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193033ms till timeout)
2022-03-28 14:35:38 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91204ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2239497ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780336ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266256ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476538ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testUserTemplate test now can proceed its execution
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testConfigurationReflection=my-cluster-9ef63e31, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testCapacityFile=my-cluster-d0923d27, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testUserTemplate=my-cluster-e309ead0, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testConfigurationReflection=my-user-113568301-1979160930, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testCapacityFile=my-user-1855618805-723404278, testConfigurationPerformanceOptions=my-user-235473874-831358867, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testUserTemplate=my-user-1093693051-959868521, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testConfigurationReflection=my-topic-1952538393-1689955181, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testCapacityFile=my-topic-785425830-966293436, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testUserTemplate=my-topic-1669116120-245826018, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1093693051-959868521 in namespace user-st
2022-03-28 14:35:39 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1093693051-959868521
2022-03-28 14:35:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50647ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1093693051-959868521 will have desired state: Ready
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1093693051-959868521 will have desired state: Ready
2022-03-28 14:35:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaUser: my-user-1093693051-959868521 will have desired state: Ready not ready, will try again in 1000 ms (179789ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191667ms till timeout)
2022-03-28 14:35:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90076ms till timeout)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2238380ms till timeout)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779221ms till timeout)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265137ms till timeout)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49532ms till timeout)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:40 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475054ms till timeout)
2022-03-28 14:35:40 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaUser: my-user-1093693051-959868521 is in desired state: Ready
2022-03-28 14:35:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testUserTemplate
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1093693051-959868521 in namespace user-st
2022-03-28 14:35:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88857ms till timeout)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190342ms till timeout)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2237263ms till timeout)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1093693051-959868521
2022-03-28 14:35:41 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778098ms till timeout)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264026ms till timeout)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testUserTemplate - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testUpdateUser] to and randomly select one to start execution
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserTemplate
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 10
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUser
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 11
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:205] [testTlsExternalUser] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48419ms till timeout)
2022-03-28 14:35:41 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473545ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87745ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:42 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2236136ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776952ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189000ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262896ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:35:42 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (594195ms till timeout)
2022-03-28 14:35:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47300ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86635ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:43 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:43 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-03-28 14:35:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (44603ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2235025ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775842ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261759ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:43 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472037ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187758ms till timeout)
2022-03-28 14:35:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46183ms till timeout)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85524ms till timeout)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2233912ms till timeout)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774729ms till timeout)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260645ms till timeout)
2022-03-28 14:35:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186531ms till timeout)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testConfigurationReflection=my-cluster-9ef63e31, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testCapacityFile=my-cluster-d0923d27, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testUpdateUser=my-cluster-3e2cd433, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testUserTemplate=my-cluster-e309ead0, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testConfigurationReflection=my-user-113568301-1979160930, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testCapacityFile=my-user-1855618805-723404278, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testUpdateUser=my-user-1450522583-540195646, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testConfigurationPerformanceOptions=my-user-235473874-831358867, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testUserTemplate=my-user-1093693051-959868521, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testConfigurationReflection=my-topic-1952538393-1689955181, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testCapacityFile=my-topic-785425830-966293436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testUpdateUser=my-topic-1915950564-404568310, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testUserTemplate=my-topic-1669116120-245826018, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testUpdateUser=my-cluster-3e2cd433-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1450522583-540195646 in namespace user-st
2022-03-28 14:35:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (470560ms till timeout)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-1ac6bea1-kafka-clients-xzf5m log
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1450522583-540195646
2022-03-28 14:35:45 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-1ac6bea1-kafka-clients deletion
2022-03-28 14:35:45 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-1ac6bea1-kafka-clients to be deleted
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1450522583-540195646 will have desired state: Ready
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1450522583-540195646 will have desired state: Ready
2022-03-28 14:35:45 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-1ac6bea1-kafka-clients was deleted
2022-03-28 14:35:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84369ms till timeout)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2232770ms till timeout)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: my-user-1450522583-540195646 will have desired state: Ready not ready, will try again in 1000 ms (179775ms till timeout)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:45 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773596ms till timeout)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259510ms till timeout)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-0
2022-03-28 14:35:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:46 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185305ms till timeout)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-25] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-0
2022-03-28 14:35:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 14:35:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (469064ms till timeout)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83256ms till timeout)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2231659ms till timeout)
2022-03-28 14:35:46 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-1450522583-540195646 is in desired state: Ready
2022-03-28 14:35:47 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772392ms till timeout)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258314ms till timeout)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 14:35:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 14:35:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183905ms till timeout)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:47 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82100ms till timeout)
2022-03-28 14:35:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 14:35:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-102
2022-03-28 14:35:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2230506ms till timeout)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-4 exec my-cluster-77376ada-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2100355713-204306190 --describe --bootstrap-server my-cluster-77376ada-kafka-bootstrap:9092
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-03-28 14:35:48 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:35:48 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (588786ms till timeout)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-1450522583-540195646
2022-03-28 14:35:48 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771242ms till timeout)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (467508ms till timeout)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] increase observation generation from 1 for user my-user-1450522583-540195646 not ready, will try again in 1000 ms (179866ms till timeout)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257164ms till timeout)
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-2100355713-204306190
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-2100355713-204306190
2022-03-28 14:35:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-102
2022-03-28 14:35:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-105
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update KafkaRebalance my-cluster-77376ada in namespace namespace-8
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-28 14:35:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:48 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-77376ada
2022-03-28 14:35:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182584ms till timeout)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80883ms till timeout)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2229294ms till timeout)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:49 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-77376ada will have desired state: PendingProposal
2022-03-28 14:35:49 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-77376ada will have desired state: PendingProposal
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-77376ada is in desired state: PendingProposal
2022-03-28 14:35:49 [ForkJoinPool-1-worker-17] INFO  [ReconciliationST:164] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-03-28 14:35:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255949ms till timeout)
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-b0221df6 will have desired state: Ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-b0221df6 will have desired state: Ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-7] INFO  [SecretUtils:46] Waiting for Secret my-user-1450522583-540195646
2022-03-28 14:35:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Expected secret my-user-1450522583-540195646 exists
2022-03-28 14:35:49 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady
2022-03-28 14:35:49 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady
2022-03-28 14:35:49 [ForkJoinPool-1-worker-7] INFO  [SecretUtils:50] Secret my-user-1450522583-540195646 created
2022-03-28 14:35:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1450522583-540195646 will have desired state: Ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] Kafka: my-cluster-b0221df6 is in desired state: Ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1450522583-540195646 will have desired state: Ready
2022-03-28 14:35:49 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-b0221df6 is ready
2022-03-28 14:35:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (599787ms till timeout)
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-1450522583-540195646 is in desired state: Ready
2022-03-28 14:35:50 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-03-28 14:35:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:50 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:120] Verifying that my-cluster-b0221df6-cruise-control- pod is not present
2022-03-28 14:35:50 [ForkJoinPool-1-worker-31] INFO  [PodUtils:209] Wait until Pod my-cluster-b0221df6-cruise-control- will have stable 0 replicas
2022-03-28 14:35:50 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas
2022-03-28 14:35:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79703ms till timeout)
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-28 14:35:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181189ms till timeout)
2022-03-28 14:35:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2228111ms till timeout)
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 14:35:50 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 14:35:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (179786ms till timeout)
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1450522583-540195646
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-1450522583-540195646
2022-03-28 14:35:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254764ms till timeout)
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:75] KafkaUser my-user-1450522583-540195646 deleted
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1450522583-540195646 in namespace user-st
2022-03-28 14:35:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1450522583-540195646
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testTlsExternalUser] to and randomly select one to start execution
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 10
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserWithNameMoreThan64Chars
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 11
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testUserWithNameMoreThan64Chars] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (598605ms till timeout)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78592ms till timeout)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2226997ms till timeout)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:230] testTlsExternalUser test now can proceed its execution
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testConfigurationReflection=my-cluster-9ef63e31, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testCapacityFile=my-cluster-d0923d27, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testTlsExternalUser=my-cluster-d51fc880, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testUpdateUser=my-cluster-3e2cd433, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testUserTemplate=my-cluster-e309ead0, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testConfigurationReflection=my-user-113568301-1979160930, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testCapacityFile=my-user-1855618805-723404278, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testTlsExternalUser=my-user-2138474265-243832882, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testUpdateUser=my-user-1450522583-540195646, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testConfigurationPerformanceOptions=my-user-235473874-831358867, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testUserTemplate=my-user-1093693051-959868521, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testConfigurationReflection=my-topic-1952538393-1689955181, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testCapacityFile=my-topic-785425830-966293436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testTlsExternalUser=my-topic-568688711-1515642318, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testUpdateUser=my-topic-1915950564-404568310, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testUserTemplate=my-topic-1669116120-245826018, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testTlsExternalUser=my-cluster-d51fc880-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testUpdateUser=my-cluster-3e2cd433-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testTlsExternalUser
2022-03-28 14:35:51 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 14:35:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (178672ms till timeout)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] INFO  [KubeClusterResource:156] Creating Namespace: namespace-9
2022-03-28 14:35:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179859ms till timeout)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253653ms till timeout)
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Namespace namespace-9
2022-03-28 14:35:51 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 14:35:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (597495ms till timeout)
2022-03-28 14:35:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77475ms till timeout)
2022-03-28 14:35:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2225880ms till timeout)
2022-03-28 14:35:52 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 14:35:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (177555ms till timeout)
2022-03-28 14:35:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252532ms till timeout)
2022-03-28 14:35:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178527ms till timeout)
2022-03-28 14:35:53 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (596383ms till timeout)
2022-03-28 14:35:53 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:35:53 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (583411ms till timeout)
2022-03-28 14:35:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76361ms till timeout)
2022-03-28 14:35:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2224764ms till timeout)
2022-03-28 14:35:53 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 14:35:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (176441ms till timeout)
2022-03-28 14:35:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251422ms till timeout)
2022-03-28 14:35:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-105
2022-03-28 14:35:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-107
2022-03-28 14:35:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177303ms till timeout)
2022-03-28 14:35:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (595274ms till timeout)
2022-03-28 14:35:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75250ms till timeout)
2022-03-28 14:35:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-107
2022-03-28 14:35:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 14:35:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2223653ms till timeout)
2022-03-28 14:35:54 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 14:35:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (175330ms till timeout)
2022-03-28 14:35:55 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:55 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (460618ms till timeout)
2022-03-28 14:35:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250312ms till timeout)
2022-03-28 14:35:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176084ms till timeout)
2022-03-28 14:35:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (594149ms till timeout)
2022-03-28 14:35:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74137ms till timeout)
2022-03-28 14:35:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2222539ms till timeout)
2022-03-28 14:35:56 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:35:56 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 14:35:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (174213ms till timeout)
2022-03-28 14:35:56 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:35:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249201ms till timeout)
2022-03-28 14:35:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (593040ms till timeout)
2022-03-28 14:35:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174762ms till timeout)
2022-03-28 14:35:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73025ms till timeout)
2022-03-28 14:35:57 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2221425ms till timeout)
2022-03-28 14:35:57 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 14:35:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (173099ms till timeout)
2022-03-28 14:35:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248088ms till timeout)
2022-03-28 14:35:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (591932ms till timeout)
2022-03-28 14:35:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173545ms till timeout)
2022-03-28 14:35:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71844ms till timeout)
2022-03-28 14:35:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2220241ms till timeout)
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c35,c10",
            "openshift.io/sa.scc.supplemental-groups": "1001210000/10000",
            "openshift.io/sa.scc.uid-range": "1001210000/10000"
        },
        "creationTimestamp": "2022-03-28T14:35:47Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-9"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T14:35:47Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T14:35:47Z"
            }
        ],
        "name": "namespace-9",
        "resourceVersion": "123248",
        "uid": "55f72263-0dcf-43e9-bc1d-e314aab834e0"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-7], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] INFO  [KubeClusterResource:82] Client use Namespace: namespace-9
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-9, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-d51fc880 in namespace namespace-9
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:35:58 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 14:35:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (171916ms till timeout)
2022-03-28 14:35:58 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-d51fc880
2022-03-28 14:35:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246899ms till timeout)
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-d51fc880 will have desired state: Ready
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-d51fc880 will have desired state: Ready
2022-03-28 14:35:58 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:35:58 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:35:58 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (578094ms till timeout)
2022-03-28 14:35:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (839785ms till timeout)
2022-03-28 14:35:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (590741ms till timeout)
2022-03-28 14:35:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:35:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172329ms till timeout)
2022-03-28 14:35:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70629ms till timeout)
2022-03-28 14:35:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:35:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2219034ms till timeout)
2022-03-28 14:35:59 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 14:35:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (170706ms till timeout)
2022-03-28 14:35:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245785ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (838676ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (589630ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 14:36:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-113
2022-03-28 14:36:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171110ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69409ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2217816ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 14:36:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (169491ms till timeout)
2022-03-28 14:36:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244575ms till timeout)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (837566ms till timeout)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (588522ms till timeout)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169891ms till timeout)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:36:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (454092ms till timeout)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68192ms till timeout)
2022-03-28 14:36:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2216598ms till timeout)
2022-03-28 14:36:02 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 14:36:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (168269ms till timeout)
2022-03-28 14:36:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243353ms till timeout)
2022-03-28 14:36:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (836451ms till timeout)
2022-03-28 14:36:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (587409ms till timeout)
2022-03-28 14:36:02 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:36:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67081ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168563ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2215484ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 14:36:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (167154ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:36:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (452559ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242241ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (835341ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (586298ms till timeout)
2022-03-28 14:36:03 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:04 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:36:04 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:04 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (572877ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65864ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:36:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:04 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2214268ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 14:36:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (165945ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167129ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241029ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (834129ms till timeout)
2022-03-28 14:36:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (585085ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64753ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2213157ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 14:36:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (164832ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239811ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (583977ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165804ms till timeout)
2022-03-28 14:36:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (832912ms till timeout)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-113
2022-03-28 14:36:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-117
2022-03-28 14:36:06 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63640ms till timeout)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2212044ms till timeout)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 14:36:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (163719ms till timeout)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238699ms till timeout)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-117
2022-03-28 14:36:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-118
2022-03-28 14:36:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (582865ms till timeout)
2022-03-28 14:36:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (831801ms till timeout)
2022-03-28 14:36:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164578ms till timeout)
2022-03-28 14:36:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62524ms till timeout)
2022-03-28 14:36:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2210928ms till timeout)
2022-03-28 14:36:07 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 14:36:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (162605ms till timeout)
2022-03-28 14:36:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237580ms till timeout)
2022-03-28 14:36:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (581745ms till timeout)
2022-03-28 14:36:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (830681ms till timeout)
2022-03-28 14:36:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163355ms till timeout)
2022-03-28 14:36:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61411ms till timeout)
2022-03-28 14:36:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2209817ms till timeout)
2022-03-28 14:36:08 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 14:36:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (161494ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236467ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (580631ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (829567ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:09 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:36:09 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (567449ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162031ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60301ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testCreatingUsersWithSecretPrefix - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testUserWithNameMoreThan64Chars] to and randomly select one to start execution
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 10
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testScramUserWithQuotas
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 11
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testScramUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2208707ms till timeout)
2022-03-28 14:36:09 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 14:36:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (160381ms till timeout)
2022-03-28 14:36:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235356ms till timeout)
2022-03-28 14:36:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (579520ms till timeout)
2022-03-28 14:36:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (828455ms till timeout)
2022-03-28 14:36:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160813ms till timeout)
2022-03-28 14:36:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59114ms till timeout)
2022-03-28 14:36:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2207520ms till timeout)
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testUserWithNameMoreThan64Chars test now can proceed its execution
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testConfigurationReflection=my-cluster-9ef63e31, testUserWithNameMoreThan64Chars=my-cluster-8957492e, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testCapacityFile=my-cluster-d0923d27, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testTlsExternalUser=my-cluster-d51fc880, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testUpdateUser=my-cluster-3e2cd433, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testUserTemplate=my-cluster-e309ead0, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testConfigurationReflection=my-user-113568301-1979160930, testUserWithNameMoreThan64Chars=my-user-501712433-1492391134, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testCapacityFile=my-user-1855618805-723404278, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testTlsExternalUser=my-user-2138474265-243832882, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testUpdateUser=my-user-1450522583-540195646, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testConfigurationPerformanceOptions=my-user-235473874-831358867, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testUserTemplate=my-user-1093693051-959868521, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testConfigurationReflection=my-topic-1952538393-1689955181, testUserWithNameMoreThan64Chars=my-topic-1535830174-1459104465, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testCapacityFile=my-topic-785425830-966293436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testTlsExternalUser=my-topic-568688711-1515642318, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testUpdateUser=my-topic-1915950564-404568310, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testUserTemplate=my-topic-1669116120-245826018, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-8957492e-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testTlsExternalUser=my-cluster-d51fc880-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testUpdateUser=my-cluster-3e2cd433-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 14:36:11 [ForkJoinPool-1-worker-31] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 14:36:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176]  Podmy-cluster-b0221df6-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (159196ms till timeout)
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 14:36:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234177ms till timeout)
2022-03-28 14:36:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (578342ms till timeout)
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 14:36:11 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (827277ms till timeout)
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 14:36:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 14:36:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 14:36:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57912ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159398ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2206320ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 14:36:12 [ForkJoinPool-1-worker-31] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 14:36:12 [ForkJoinPool-1-worker-31] INFO  [PodUtils:228] Pod my-cluster-b0221df6-cruise-control- has 0 replicas
2022-03-28 14:36:12 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-03-28 14:36:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-118
2022-03-28 14:36:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 14:36:12 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties
2022-03-28 14:36:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (120000ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232958ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (577131ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (826066ms till timeout)
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-03-28 14:36:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 14:36:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 14:36:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-120
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 14:36:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56784ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2205190ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk
2022-03-28 14:36:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158159ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (119000ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 14:36:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231845ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (576008ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (824944ms till timeout)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testUserWithNameMoreThan64Chars - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testScramUserWithQuotas] to and randomly select one to start execution
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserWithNameMoreThan64Chars
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 10
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsUserWithQuotas
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 11
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testTlsUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:13 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55673ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2204080ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (117999ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testScramUserWithQuotas test now can proceed its execution
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testConfigurationReflection=my-cluster-9ef63e31, testUserWithNameMoreThan64Chars=my-cluster-8957492e, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testCapacityFile=my-cluster-d0923d27, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testTlsExternalUser=my-cluster-d51fc880, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testScramUserWithQuotas=my-cluster-f9f41b57, testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testUpdateUser=my-cluster-3e2cd433, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testUserTemplate=my-cluster-e309ead0, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testConfigurationReflection=my-user-113568301-1979160930, testUserWithNameMoreThan64Chars=my-user-501712433-1492391134, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testCapacityFile=my-user-1855618805-723404278, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testTlsExternalUser=my-user-2138474265-243832882, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testScramUserWithQuotas=my-user-689134389-2001947957, testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testUpdateUser=my-user-1450522583-540195646, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testConfigurationPerformanceOptions=my-user-235473874-831358867, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testUserTemplate=my-user-1093693051-959868521, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testConfigurationReflection=my-topic-1952538393-1689955181, testUserWithNameMoreThan64Chars=my-topic-1535830174-1459104465, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testCapacityFile=my-topic-785425830-966293436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testTlsExternalUser=my-topic-568688711-1515642318, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testScramUserWithQuotas=my-topic-1667902716-1547137080, testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testUpdateUser=my-topic-1915950564-404568310, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testUserTemplate=my-topic-1669116120-245826018, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-8957492e-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testTlsExternalUser=my-cluster-d51fc880-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testScramUserWithQuotas=my-cluster-f9f41b57-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testUpdateUser=my-cluster-3e2cd433-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-03-28 14:36:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156837ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01, my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230630ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:14 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Deployment my-cluster-da099474-cruise-control rolling update in namespace:namespace-8 not ready, will try again in 5000 ms (562040ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (574795ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (823729ms till timeout)
2022-03-28 14:36:14 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 14:36:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 14:36:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 14:36:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-03-28 14:36:15 [ForkJoinPool-1-worker-13] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 14:36:15 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 14:36:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54561ms till timeout)
2022-03-28 14:36:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (116999ms till timeout)
2022-03-28 14:36:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2202968ms till timeout)
2022-03-28 14:36:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229511ms till timeout)
2022-03-28 14:36:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (573676ms till timeout)
2022-03-28 14:36:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155503ms till timeout)
2022-03-28 14:36:16 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (822610ms till timeout)
2022-03-28 14:36:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (115998ms till timeout)
2022-03-28 14:36:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53449ms till timeout)
2022-03-28 14:36:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2201853ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228399ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (572558ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (821494ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154272ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (114997ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52235ms till timeout)
2022-03-28 14:36:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2200643ms till timeout)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227287ms till timeout)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (820382ms till timeout)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (571339ms till timeout)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (113997ms till timeout)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152953ms till timeout)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-120
2022-03-28 14:36:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-121
2022-03-28 14:36:18 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (11/10)
2022-03-28 14:36:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51125ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2199527ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-121
2022-03-28 14:36:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-123
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 14:36:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226175ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 14:36:19 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (819169ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (112997ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for all KafkaUser scramed-arnost attributes will be cleaned
2022-03-28 14:36:19 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 14:36:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (570127ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-123
2022-03-28 14:36:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-125
2022-03-28 14:36:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151737ms till timeout)
2022-03-28 14:36:19 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-da099474-cruise-control-59b5959466-xpv7g=ce1d8a62-253c-43e1-b57c-bc1ac6b0bc01}
2022-03-28 14:36:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49911ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-da099474-cruise-control-d8cf74749-2skr6=e1d16877-ab7a-4169-b222-497bc4690843}
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-da099474-cruise-control will be ready
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-da099474-cruise-control will be ready
2022-03-28 14:36:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2198214ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] INFO  [DeploymentUtils:168] Deployment: my-cluster-da099474-cruise-control is ready
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready
2022-03-28 14:36:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-125
2022-03-28 14:36:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-126
2022-03-28 14:36:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (111995ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224954ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (818054ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599769ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (569004ms till timeout)
2022-03-28 14:36:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150520ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-126
2022-03-28 14:36:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-127
2022-03-28 14:36:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48801ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2197104ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (110994ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223837ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (816937ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598653ms till timeout)
2022-03-28 14:36:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-127
2022-03-28 14:36:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 14:36:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (567894ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149301ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 14:36:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-129
2022-03-28 14:36:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47602ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2195983ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (109994ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222725ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (815822ms till timeout)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:22 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597539ms till timeout)
2022-03-28 14:36:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (566779ms till timeout)
2022-03-28 14:36:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-129
2022-03-28 14:36:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 14:36:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-03-28 14:36:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148084ms till timeout)
2022-03-28 14:36:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (108993ms till timeout)
2022-03-28 14:36:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 14:36:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-138
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 14:36:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46384ms till timeout)
2022-03-28 14:36:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2194795ms till timeout)
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testScramUserWithQuotas - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions, testTlsUserWithQuotas] to and randomly select one to start execution
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testScramUserWithQuotas
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 10
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-03-28 14:36:23 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testTlsUserWithQuotas test now can proceed its execution
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testConfigurationReflection=my-cluster-9ef63e31, testUserWithNameMoreThan64Chars=my-cluster-8957492e, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testCapacityFile=my-cluster-d0923d27, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testTlsExternalUser=my-cluster-d51fc880, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testScramUserWithQuotas=my-cluster-f9f41b57, testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testUpdateUser=my-cluster-3e2cd433, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testTlsUserWithQuotas=my-cluster-551ec9f6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testUserTemplate=my-cluster-e309ead0, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testConfigurationReflection=my-user-113568301-1979160930, testUserWithNameMoreThan64Chars=my-user-501712433-1492391134, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testCapacityFile=my-user-1855618805-723404278, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testTlsExternalUser=my-user-2138474265-243832882, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testScramUserWithQuotas=my-user-689134389-2001947957, testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testUpdateUser=my-user-1450522583-540195646, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testTlsUserWithQuotas=my-user-823839014-420067000, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testConfigurationPerformanceOptions=my-user-235473874-831358867, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testUserTemplate=my-user-1093693051-959868521, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testConfigurationReflection=my-topic-1952538393-1689955181, testUserWithNameMoreThan64Chars=my-topic-1535830174-1459104465, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testCapacityFile=my-topic-785425830-966293436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testTlsExternalUser=my-topic-568688711-1515642318, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testScramUserWithQuotas=my-topic-1667902716-1547137080, testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testUpdateUser=my-topic-1915950564-404568310, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testTlsUserWithQuotas=my-topic-1134599290-536078433, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testUserTemplate=my-topic-1669116120-245826018, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-8957492e-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testTlsExternalUser=my-cluster-d51fc880-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testScramUserWithQuotas=my-cluster-f9f41b57-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testUpdateUser=my-cluster-3e2cd433-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testTlsUserWithQuotas=my-cluster-551ec9f6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:36:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-03-28 14:36:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221553ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (814651ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596368ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 14:36:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (565611ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-138
2022-03-28 14:36:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 14:36:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 14:36:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 14:36:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: encrypted-arnost will have desired state: Ready not ready, will try again in 1000 ms (179894ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (107992ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45273ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 14:36:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 14:36:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146755ms till timeout)
2022-03-28 14:36:24 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2193677ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220441ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (813541ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595257ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (564497ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 14:36:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 14:36:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-03-28 14:36:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (106991ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-7] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 14:36:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 14:36:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44163ms till timeout)
2022-03-28 14:36:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 14:36:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-151
2022-03-28 14:36:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2192462ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145432ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219224ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (812325ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (563389ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594040ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (105991ms till timeout)
2022-03-28 14:36:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-151
2022-03-28 14:36:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-152
2022-03-28 14:36:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43045ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2191351ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-152
2022-03-28 14:36:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-155
2022-03-28 14:36:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144213ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (562278ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218007ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (811202ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (104990ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592810ms till timeout)
2022-03-28 14:36:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-155
2022-03-28 14:36:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-157
2022-03-28 14:36:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41932ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2190237ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-157
2022-03-28 14:36:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 14:36:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (103989ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (561163ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216886ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142885ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (809990ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591604ms till timeout)
2022-03-28 14:36:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 14:36:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-161
2022-03-28 14:36:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40821ms till timeout)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 14:36:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2189029ms till timeout)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-161
2022-03-28 14:36:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-166
2022-03-28 14:36:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (102989ms till timeout)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] INFO  [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=encrypted-arnost attributes will be cleaned
2022-03-28 14:36:29 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 14:36:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (560054ms till timeout)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215674ms till timeout)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (808775ms till timeout)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:29 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:29 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590492ms till timeout)
2022-03-28 14:36:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141559ms till timeout)
2022-03-28 14:36:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-166
2022-03-28 14:36:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-168
2022-03-28 14:36:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39708ms till timeout)
2022-03-28 14:36:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (101988ms till timeout)
2022-03-28 14:36:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2187917ms till timeout)
2022-03-28 14:36:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-168
2022-03-28 14:36:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 14:36:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (558946ms till timeout)
2022-03-28 14:36:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214561ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (807662ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:31 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-da099474, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-da099474-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589379ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 14:36:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-173
2022-03-28 14:36:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140342ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38579ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (100988ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2186802ms till timeout)
2022-03-28 14:36:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-173
2022-03-28 14:36:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 14:36:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (557837ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213450ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: cruise-control)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-da099474-cruise-control-d8cf74749-2skr6 not ready: tls-sidecar)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [PodUtils:106] Pods my-cluster-da099474-cruise-control-d8cf74749-2skr6 are ready
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] INFO  [DeploymentUtils:141] Deployment my-cluster-da099474-cruise-control rolling update finished
2022-03-28 14:36:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (806547ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 14:36:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 14:36:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] INFO  [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (99984ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139006ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37415ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 14:36:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299784ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 14:36:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2185692ms till timeout)
2022-03-28 14:36:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 14:36:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-184
2022-03-28 14:36:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (556729ms till timeout)
2022-03-28 14:36:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212339ms till timeout)
2022-03-28 14:36:33 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (805438ms till timeout)
2022-03-28 14:36:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-184
2022-03-28 14:36:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 14:36:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (98983ms till timeout)
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-03-28 14:36:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:33 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:33 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 14:36:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36196ms till timeout)
2022-03-28 14:36:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137685ms till timeout)
2022-03-28 14:36:33 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:33 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:33 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:33 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 14:36:33 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298568ms till timeout)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:36:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testTlsUserWithQuotas - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:36:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsUserWithQuotas
2022-03-28 14:36:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 9
2022-03-28 14:36:34 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-03-28 14:36:34 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:36:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2184500ms till timeout)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 14:36:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-190
2022-03-28 14:36:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (555623ms till timeout)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211226ms till timeout)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (804324ms till timeout)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (97983ms till timeout)
2022-03-28 14:36:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-190
2022-03-28 14:36:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-197
2022-03-28 14:36:34 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:34 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (35085ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:35 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:35 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:35 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:35 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 14:36:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297451ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2183382ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-197
2022-03-28 14:36:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 14:36:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136351ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (554416ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210037ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (96982ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (803137ms till timeout)
2022-03-28 14:36:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 14:36:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 14:36:36 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33972ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:36 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:36 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:36 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 14:36:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296340ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2182270ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 14:36:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-20
2022-03-28 14:36:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (553307ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (95982ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135029ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208822ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (801921ms till timeout)
2022-03-28 14:36:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-20
2022-03-28 14:36:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 14:36:37 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32860ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:37 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:37 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:37 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 14:36:37 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295228ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2181158ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 14:36:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 14:36:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (94981ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (552199ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133800ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (800786ms till timeout)
2022-03-28 14:36:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207577ms till timeout)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 14:36:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 14:36:38 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31747ms till timeout)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:38 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:38 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:38 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 14:36:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294115ms till timeout)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2180045ms till timeout)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (93980ms till timeout)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 14:36:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 14:36:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (551091ms till timeout)
2022-03-28 14:36:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132585ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (799583ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206374ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 14:36:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 14:36:39 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30632ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:39 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:39 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:39 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 14:36:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292998ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (92980ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2178929ms till timeout)
2022-03-28 14:36:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 14:36:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 14:36:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (549983ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 14:36:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 14:36:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (798475ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131366ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205159ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:40 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29521ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (91979ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:40 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:40 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:40 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 14:36:40 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291889ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2177818ms till timeout)
2022-03-28 14:36:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 14:36:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 14:36:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (548875ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 14:36:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 14:36:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (797367ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203945ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (90979ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129936ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28344ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:41 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:41 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:41 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 14:36:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290715ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2176644ms till timeout)
2022-03-28 14:36:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 14:36:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 14:36:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (547767ms till timeout)
2022-03-28 14:36:42 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (796255ms till timeout)
2022-03-28 14:36:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 14:36:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 14:36:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (89979ms till timeout)
2022-03-28 14:36:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202833ms till timeout)
2022-03-28 14:36:42 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:42 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:42 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:42 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:42 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 14:36:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289603ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-23] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-431951f6-kafka-clients-hr6df log
2022-03-28 14:36:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128611ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2175533ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 14:36:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 14:36:43 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-431951f6-kafka-clients deletion
2022-03-28 14:36:43 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-431951f6-kafka-clients to be deleted
2022-03-28 14:36:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (546575ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-431951f6-kafka-clients to be deleted not ready, will try again in 5000 ms (179790ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (88978ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (795148ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 14:36:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 14:36:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201723ms till timeout)
2022-03-28 14:36:43 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:44 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:44 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:44 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:44 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 14:36:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288493ms till timeout)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2174421ms till timeout)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 14:36:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 14:36:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127391ms till timeout)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (545455ms till timeout)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (87977ms till timeout)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (794039ms till timeout)
2022-03-28 14:36:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 14:36:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 14:36:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200614ms till timeout)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:45 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:45 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:45 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:45 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 14:36:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287370ms till timeout)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2173299ms till timeout)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 14:36:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 14:36:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:35:06Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (544335ms till timeout)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (86977ms till timeout)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-19] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:36:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:36:45 [ForkJoinPool-1-worker-19] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-d617cda6-kafka-clients-fz2kw log
2022-03-28 14:36:45 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (792831ms till timeout)
2022-03-28 14:36:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 14:36:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:36:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199409ms till timeout)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job create-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:36:46 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:46 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:46 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:46 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 14:36:46 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286180ms till timeout)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2172108ms till timeout)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:36:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 14:36:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:36:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (85976ms till timeout)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (543144ms till timeout)
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219783ms till timeout)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 14:36:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 14:36:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (791647ms till timeout)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198296ms till timeout)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:47 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:47 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:47 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:47 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 14:36:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285067ms till timeout)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2170996ms till timeout)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (84976ms till timeout)
2022-03-28 14:36:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 14:36:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 14:36:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (542036ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (790540ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 14:36:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 14:36:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218453ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197111ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:48 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job create-admin-my-cluster-431951f6-kafka-clients was deleted
2022-03-28 14:36:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (83975ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:48 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:48 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:48 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 14:36:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283881ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:36:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2169809ms till timeout)
2022-03-28 14:36:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 14:36:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 14:36:48 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:36:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (540848ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-431951f6-kafka-clients will be in active state
2022-03-28 14:36:49 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:36:49 [ForkJoinPool-1-worker-23] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:36:49 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:36:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (789356ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 14:36:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-220
2022-03-28 14:36:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119786ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (82975ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195929ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217056ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:49 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:49 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:49 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 14:36:49 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282700ms till timeout)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2168616ms till timeout)
2022-03-28 14:36:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-220
2022-03-28 14:36:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-226
2022-03-28 14:36:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (539739ms till timeout)
2022-03-28 14:36:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (788246ms till timeout)
2022-03-28 14:36:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-226
2022-03-28 14:36:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 14:36:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (81974ms till timeout)
2022-03-28 14:36:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118675ms till timeout)
2022-03-28 14:36:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194814ms till timeout)
2022-03-28 14:36:50 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:50 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:50 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:50 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:50 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 14:36:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281558ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215673ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2167458ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (538604ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 14:36:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-230
2022-03-28 14:36:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (80973ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (787136ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117564ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-230
2022-03-28 14:36:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-235
2022-03-28 14:36:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193696ms till timeout)
2022-03-28 14:36:51 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:52 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:52 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:52 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:52 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 14:36:52 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280441ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (537496ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2166245ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-235
2022-03-28 14:36:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-24
2022-03-28 14:36:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214351ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (79973ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (786027ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116453ms till timeout)
2022-03-28 14:36:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-24
2022-03-28 14:36:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 14:36:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192584ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:53 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:53 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:53 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:53 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 14:36:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279237ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (536387ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 14:36:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-241
2022-03-28 14:36:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2165029ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (78972ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213030ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (784897ms till timeout)
2022-03-28 14:36:53 [ForkJoinPool-1-worker-23] INFO  [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-431951f6-kafka-clients-dpdw6 log
2022-03-28 14:36:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191469ms till timeout)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-241
2022-03-28 14:36:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment list-admin-my-cluster-431951f6-kafka-clients deletion
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet list-admin-my-cluster-431951f6-kafka-clients to be deleted
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job list-admin-my-cluster-431951f6-kafka-clients was deleted
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:36:54 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:36:54 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:54 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:54 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:54 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 14:36:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278026ms till timeout)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (77972ms till timeout)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-431951f6-kafka-clients will be in active state
2022-03-28 14:36:54 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:36:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (535206ms till timeout)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 14:36:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 14:36:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2163849ms till timeout)
2022-03-28 14:36:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:55 [ForkJoinPool-1-worker-23] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:36:55 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:36:55 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (783712ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211635ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 14:36:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-244
2022-03-28 14:36:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190291ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119783ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (76971ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:55 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:55 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:55 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 14:36:55 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (276914ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (534095ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2162737ms till timeout)
2022-03-28 14:36:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-244
2022-03-28 14:36:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-245
2022-03-28 14:36:56 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (782599ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118667ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189172ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-245
2022-03-28 14:36:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-246
2022-03-28 14:36:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210300ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (75970ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:56 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:56 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:56 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:56 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 14:36:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275804ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (532986ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2161628ms till timeout)
2022-03-28 14:36:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-246
2022-03-28 14:36:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-247
2022-03-28 14:36:57 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (781491ms till timeout)
2022-03-28 14:36:57 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117553ms till timeout)
2022-03-28 14:36:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-247
2022-03-28 14:36:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-248
2022-03-28 14:36:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187949ms till timeout)
2022-03-28 14:36:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (74970ms till timeout)
2022-03-28 14:36:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209081ms till timeout)
2022-03-28 14:36:57 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:57 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:57 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:57 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:57 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 14:36:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274693ms till timeout)
2022-03-28 14:36:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (531873ms till timeout)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2160514ms till timeout)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-248
2022-03-28 14:36:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 14:36:58 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (780380ms till timeout)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116441ms till timeout)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (73969ms till timeout)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186838ms till timeout)
2022-03-28 14:36:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:36:58 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 14:36:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 14:36:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207856ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:59 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:36:59 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:36:59 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 14:36:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273498ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (530677ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2159317ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (779269ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (72969ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115331ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185726ms till timeout)
2022-03-28 14:36:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 14:36:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:36:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-258
2022-03-28 14:37:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:00 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206641ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:00 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:00 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:00 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 14:37:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272283ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (529464ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2158107ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-258
2022-03-28 14:37:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 14:37:00 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (778161ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (71968ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114215ms till timeout)
2022-03-28 14:37:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184595ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:01 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205423ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (528354ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:01 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:01 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:01 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 14:37:01 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271065ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2156995ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (70967ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (777053ms till timeout)
2022-03-28 14:37:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113105ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183483ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (527237ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:02 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:02 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:02 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 14:37:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269951ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (69967ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204094ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2155879ms till timeout)
2022-03-28 14:37:02 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (775944ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111993ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182370ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (68966ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (526127ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:03 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:03 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:03 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:03 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 14:37:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268732ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2154663ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202769ms till timeout)
2022-03-28 14:37:03 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (774739ms till timeout)
2022-03-28 14:37:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110882ms till timeout)
2022-03-28 14:37:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181252ms till timeout)
2022-03-28 14:37:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (67966ms till timeout)
2022-03-28 14:37:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (525020ms till timeout)
2022-03-28 14:37:04 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:05 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:05 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:05 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:05 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 14:37:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267482ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (773601ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2153415ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201521ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109671ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180068ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (66965ms till timeout)
2022-03-28 14:37:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (523911ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:06 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:06 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:06 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:06 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 14:37:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266372ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 14:37:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-268
2022-03-28 14:37:06 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (772493ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2152201ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200306ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108458ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (65965ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178856ms till timeout)
2022-03-28 14:37:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (522800ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:07 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:07 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:07 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:07 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 14:37:07 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265262ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (771378ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2151080ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (64964ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107333ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198970ms till timeout)
2022-03-28 14:37:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177735ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (521690ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:08 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:08 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:08 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:08 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 14:37:08 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264151ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Kafka: my-cluster-d51fc880 will have desired state: Ready not ready, will try again in 1000 ms (770267ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58df3988-kafka-3)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2149970ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (63963ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106222ms till timeout)
2022-03-28 14:37:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176621ms till timeout)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197749ms till timeout)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (520583ms till timeout)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:09 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:09 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:09 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:09 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 14:37:09 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263040ms till timeout)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] Kafka: my-cluster-d51fc880 is in desired state: Ready
2022-03-28 14:37:09 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-2138474265-243832882 in namespace namespace-9
2022-03-28 14:37:09 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 14:37:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (62963ms till timeout)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2148852ms till timeout)
2022-03-28 14:37:09 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-2138474265-243832882
2022-03-28 14:37:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105108ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-2138474265-243832882 will have desired state: Ready
2022-03-28 14:37:10 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-2138474265-243832882 will have desired state: Ready
2022-03-28 14:37:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175498ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:10 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] KafkaUser: my-user-2138474265-243832882 will have desired state: Ready not ready, will try again in 1000 ms (179789ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (519460ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196422ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:10 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:10 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:10 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:10 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 14:37:10 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261923ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (61962ms till timeout)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2147742ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103997ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174386ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaUser: my-user-2138474265-243832882 is in desired state: Ready
2022-03-28 14:37:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (518348ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-2138474265-243832882 will have desired state: Ready
2022-03-28 14:37:11 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-2138474265-243832882 will have desired state: Ready
2022-03-28 14:37:11 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (60962ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195100ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:11 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:11 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:11 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 14:37:11 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260744ms till timeout)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-268
2022-03-28 14:37:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-27
2022-03-28 14:37:11 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:444] KafkaUser: my-user-2138474265-243832882 is in desired state: Ready
2022-03-28 14:37:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:11 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2146564ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:37:12 [ForkJoinPool-1-worker-25] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 14:37:12 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:37:12 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-03-28 14:37:12 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaUser my-user-2138474265-243832882 in namespace namespace-9
2022-03-28 14:37:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-d51fc880 in namespace namespace-9
2022-03-28 14:37:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102820ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173217ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d51fc880
2022-03-28 14:37:12 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2138474265-243832882
2022-03-28 14:37:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d51fc880 not ready, will try again in 10000 ms (839880ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-27
2022-03-28 14:37:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 14:37:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (59961ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (517139ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:12 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:12 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:12 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:12 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 14:37:12 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259630ms till timeout)
2022-03-28 14:37:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193774ms till timeout)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2145452ms till timeout)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101710ms till timeout)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172107ms till timeout)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (58960ms till timeout)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (516029ms till timeout)
2022-03-28 14:37:13 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:13 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:13 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:13 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:13 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 14:37:13 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258517ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2144336ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192443ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100592ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170990ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (57960ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (514921ms till timeout)
2022-03-28 14:37:14 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:15 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:15 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:15 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:15 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 14:37:15 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257405ms till timeout)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2143222ms till timeout)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99479ms till timeout)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (56958ms till timeout)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191114ms till timeout)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169879ms till timeout)
2022-03-28 14:37:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (513811ms till timeout)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:16 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:16 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:16 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:16 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 14:37:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256293ms till timeout)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2142108ms till timeout)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (55958ms till timeout)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98367ms till timeout)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168765ms till timeout)
2022-03-28 14:37:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189893ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (512696ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:17 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:17 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:17 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:17 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 14:37:17 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255180ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2140994ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (54957ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97255ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167646ms till timeout)
2022-03-28 14:37:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188663ms till timeout)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (511588ms till timeout)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:18 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:18 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:18 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:18 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 14:37:18 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254068ms till timeout)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 14:37:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-281
2022-03-28 14:37:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (53956ms till timeout)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2139874ms till timeout)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96143ms till timeout)
2022-03-28 14:37:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166535ms till timeout)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187446ms till timeout)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (510379ms till timeout)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:19 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:19 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:19 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:19 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 14:37:19 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (252956ms till timeout)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (52955ms till timeout)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-281
2022-03-28 14:37:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 14:37:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58df3988-kafka, strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2138758ms till timeout)
2022-03-28 14:37:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95020ms till timeout)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165420ms till timeout)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 14:37:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-292
2022-03-28 14:37:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (509265ms till timeout)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (51955ms till timeout)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186123ms till timeout)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:20 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:20 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:20 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 14:37:20 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251765ms till timeout)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-0 not ready: kafka)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-1 not ready: kafka)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-2 not ready: kafka)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-kafka-3 not ready: kafka)
2022-03-28 14:37:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-kafka-0, my-cluster-58df3988-kafka-1, my-cluster-58df3988-kafka-2, my-cluster-58df3988-kafka-3 are ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-58df3988 will have desired state: Ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-58df3988 will have desired state: Ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93889ms till timeout)
2022-03-28 14:37:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-292
2022-03-28 14:37:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-299
2022-03-28 14:37:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164295ms till timeout)
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-58df3988 is in desired state: Ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-58df3988 is ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-58df3988-kafka-clients in namespace namespace-9
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-58df3988-kafka-clients
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-58df3988-kafka-clients will be ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58df3988-kafka-clients will be ready
2022-03-28 14:37:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (50954ms till timeout)
2022-03-28 14:37:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (508117ms till timeout)
2022-03-28 14:37:21 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-kafka-clients will be ready not ready, will try again in 1000 ms (479785ms till timeout)
2022-03-28 14:37:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:21 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:21 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:21 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:21 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 14:37:21 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250615ms till timeout)
2022-03-28 14:37:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184758ms till timeout)
2022-03-28 14:37:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92777ms till timeout)
2022-03-28 14:37:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163169ms till timeout)
2022-03-28 14:37:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:37:22 [ForkJoinPool-1-worker-25] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testTlsExternalUser
2022-03-28 14:37:22 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (49954ms till timeout)
2022-03-28 14:37:22 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Namespace namespace-9 removal
2022-03-28 14:37:22 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (507009ms till timeout)
2022-03-28 14:37:22 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-kafka-clients will be ready not ready, will try again in 1000 ms (478675ms till timeout)
2022-03-28 14:37:22 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:22 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:22 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:22 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 14:37:22 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249505ms till timeout)
2022-03-28 14:37:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183439ms till timeout)
2022-03-28 14:37:23 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91589ms till timeout)
2022-03-28 14:37:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161984ms till timeout)
2022-03-28 14:37:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (48953ms till timeout)
2022-03-28 14:37:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (505901ms till timeout)
2022-03-28 14:37:23 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-58df3988-kafka-clients is ready
2022-03-28 14:37:23 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:24 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-58df3988-scraper in namespace namespace-6
2022-03-28 14:37:24 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 14:37:24 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:24 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:24 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:24 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 14:37:24 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248394ms till timeout)
2022-03-28 14:37:24 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-58df3988-scraper
2022-03-28 14:37:24 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-58df3988-scraper will be ready
2022-03-28 14:37:24 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58df3988-scraper will be ready
2022-03-28 14:37:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-scraper will be ready not ready, will try again in 1000 ms (479787ms till timeout)
2022-03-28 14:37:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90471ms till timeout)
2022-03-28 14:37:24 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (47952ms till timeout)
2022-03-28 14:37:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182105ms till timeout)
2022-03-28 14:37:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160870ms till timeout)
2022-03-28 14:37:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (504793ms till timeout)
2022-03-28 14:37:25 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:25 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:25 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:25 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:25 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 14:37:25 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247283ms till timeout)
2022-03-28 14:37:25 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (46952ms till timeout)
2022-03-28 14:37:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-scraper will be ready not ready, will try again in 1000 ms (478679ms till timeout)
2022-03-28 14:37:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89253ms till timeout)
2022-03-28 14:37:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159647ms till timeout)
2022-03-28 14:37:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180776ms till timeout)
2022-03-28 14:37:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (503684ms till timeout)
2022-03-28 14:37:26 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:26 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:26 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:26 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:26 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 14:37:26 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (246173ms till timeout)
2022-03-28 14:37:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (45951ms till timeout)
2022-03-28 14:37:26 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-58df3988-scraper is ready
2022-03-28 14:37:26 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-58df3988-scraper to be ready
2022-03-28 14:37:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-299
2022-03-28 14:37:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-3
2022-03-28 14:37:26 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-28 14:37:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88040ms till timeout)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599784ms till timeout)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158436ms till timeout)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (502499ms till timeout)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179355ms till timeout)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-3
2022-03-28 14:37:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 14:37:27 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:27 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:27 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:27 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 14:37:27 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244997ms till timeout)
2022-03-28 14:37:27 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (44951ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 14:37:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 14:37:28 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86928ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598674ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157219ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:28 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:28 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (474295ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (501382ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:28 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (43950ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:28 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:28 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:28 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 14:37:28 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (243874ms till timeout)
2022-03-28 14:37:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 14:37:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 14:37:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178018ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85818ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 14:37:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-302
2022-03-28 14:37:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597565ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156097ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (500271ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (42950ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94}
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] DEBUG [RollingUpdateUtils:50] At least my-cluster-da099474-kafka-1 hasn't rolled
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] INFO  [RollingUpdateUtils:143] {my-cluster-da099474-kafka-1=691ae3c8-c239-4195-b9f5-e88587ff83ec, my-cluster-da099474-kafka-0=d5b92dac-2571-4199-b2ee-0f48d0b84aa6, my-cluster-da099474-kafka-2=7e10d70e-e159-40e9-a7cc-0f6754640f94} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] INFO  [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-03-28 14:37:29 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:29 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:29 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (472839ms till timeout)
2022-03-28 14:37:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-302
2022-03-28 14:37:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 14:37:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:29 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:37:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176687ms till timeout)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:37:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 14:37:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 14:37:30 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84622ms till timeout)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:37:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:37:30 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 14:37:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596367ms till timeout)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of Kafka my-cluster-da099474 in namespace namespace-8
2022-03-28 14:37:30 [ForkJoinPool-1-worker-21] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-8, for cruise control Kafka cluster my-cluster-da099474
2022-03-28 14:37:30 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (41949ms till timeout)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154911ms till timeout)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (499071ms till timeout)
2022-03-28 14:37:30 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 14:37:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 14:37:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:31 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:31 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:31 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (471315ms till timeout)
2022-03-28 14:37:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175358ms till timeout)
2022-03-28 14:37:31 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83510ms till timeout)
2022-03-28 14:37:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 14:37:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-306
2022-03-28 14:37:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (40948ms till timeout)
2022-03-28 14:37:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:31 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595255ms till timeout)
2022-03-28 14:37:31 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-da099474
2022-03-28 14:37:31 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:37:31 [ForkJoinPool-1-worker-21] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 14:37:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (497808ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153502ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-28 14:37:32 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-306
2022-03-28 14:37:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-307
2022-03-28 14:37:32 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:32 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:32 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:32 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479512ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (39948ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82394ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594142ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174030ms till timeout)
2022-03-28 14:37:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-307
2022-03-28 14:37:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 14:37:32 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:32 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:32 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (469859ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (496700ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152321ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 14:37:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 14:37:33 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:33 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (38948ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81177ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593014ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172806ms till timeout)
2022-03-28 14:37:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 14:37:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 14:37:34 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:34 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:34 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478076ms till timeout)
2022-03-28 14:37:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (495592ms till timeout)
2022-03-28 14:37:34 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:34 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:34 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (468372ms till timeout)
2022-03-28 14:37:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151205ms till timeout)
2022-03-28 14:37:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 14:37:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-310
2022-03-28 14:37:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (37947ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79963ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591809ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-310
2022-03-28 14:37:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 14:37:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171489ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (494419ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150038ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:35 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:35 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476628ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (36947ms till timeout)
2022-03-28 14:37:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 14:37:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 14:37:35 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:35 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:35 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (466915ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 14:37:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 14:37:36 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78751ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590599ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (493312ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170169ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (35946ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148827ms till timeout)
2022-03-28 14:37:36 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 14:37:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-314
2022-03-28 14:37:36 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:36 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:36 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475148ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:37 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:37 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (465461ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-314
2022-03-28 14:37:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-315
2022-03-28 14:37:37 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77536ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-58df3988-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589379ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (492204ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (34946ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168954ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147611ms till timeout)
2022-03-28 14:37:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-315
2022-03-28 14:37:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-316
2022-03-28 14:37:37 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:38 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:38 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:38 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:38 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (473689ms till timeout)
2022-03-28 14:37:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-316
2022-03-28 14:37:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-317
2022-03-28 14:37:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (33945ms till timeout)
2022-03-28 14:37:38 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:38 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (463987ms till timeout)
2022-03-28 14:37:38 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76317ms till timeout)
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-scraper-79f75544d8-qjhp8 not ready: my-cluster-58df3988-scraper)
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-scraper-79f75544d8-qjhp8 are ready
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:197] Deployment my-cluster-58df3988-scraper is ready
2022-03-28 14:37:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (491088ms till timeout)
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-58df3988-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-58df3988-allow, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update NetworkPolicy my-cluster-58df3988-allow in namespace namespace-6
2022-03-28 14:37:38 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 14:37:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-58df3988-allow
2022-03-28 14:37:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146490ms till timeout)
2022-03-28 14:37:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-317
2022-03-28 14:37:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-318
2022-03-28 14:37:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167623ms till timeout)
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update KafkaConnect my-cluster-58df3988 in namespace namespace-9
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-28 14:37:39 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaConnect: my-cluster-58df3988 will have desired state: ReconciliationPaused
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaConnect: my-cluster-58df3988 will have desired state: ReconciliationPaused
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaConnect: my-cluster-58df3988 is in desired state: ReconciliationPaused
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [PodUtils:209] Wait until Pod my-cluster-58df3988-connect will have stable 0 replicas
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-58df3988-connect will have 0 replicas
2022-03-28 14:37:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (32944ms till timeout)
2022-03-28 14:37:39 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-318
2022-03-28 14:37:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 14:37:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (179787ms till timeout)
2022-03-28 14:37:39 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75205ms till timeout)
2022-03-28 14:37:39 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:39 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:39 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (472251ms till timeout)
2022-03-28 14:37:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (489875ms till timeout)
2022-03-28 14:37:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145378ms till timeout)
2022-03-28 14:37:40 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:40 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:40 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (462506ms till timeout)
2022-03-28 14:37:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 14:37:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-32
2022-03-28 14:37:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166398ms till timeout)
2022-03-28 14:37:40 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (31944ms till timeout)
2022-03-28 14:37:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-32
2022-03-28 14:37:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 14:37:40 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:41 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 14:37:41 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74003ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (178569ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (488676ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144260ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:41 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:41 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (470764ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 14:37:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 14:37:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165183ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (30943ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:41 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:41 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (461032ms till timeout)
2022-03-28 14:37:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 14:37:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 14:37:42 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 14:37:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (177355ms till timeout)
2022-03-28 14:37:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (487563ms till timeout)
2022-03-28 14:37:42 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72784ms till timeout)
2022-03-28 14:37:42 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143146ms till timeout)
2022-03-28 14:37:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 14:37:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 14:37:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (29943ms till timeout)
2022-03-28 14:37:42 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163965ms till timeout)
2022-03-28 14:37:42 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:42 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:42 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (469307ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 14:37:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 14:37:43 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:43 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:43 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (459558ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 14:37:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (176141ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (486350ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71572ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141974ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 14:37:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 14:37:43 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (28942ms till timeout)
2022-03-28 14:37:43 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162749ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 14:37:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 14:37:44 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:44 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:44 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (467807ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:44 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:44 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (458078ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (27941ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 14:37:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (174923ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (485130ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140857ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70256ms till timeout)
2022-03-28 14:37:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 14:37:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 14:37:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161533ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 14:37:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 14:37:45 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (26941ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:45 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:45 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (466321ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 14:37:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (173710ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (483918ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69139ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139549ms till timeout)
2022-03-28 14:37:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 14:37:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 14:37:46 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:46 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:46 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (456556ms till timeout)
2022-03-28 14:37:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160314ms till timeout)
2022-03-28 14:37:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 14:37:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-33
2022-03-28 14:37:46 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (25940ms till timeout)
2022-03-28 14:37:46 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:47 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 14:37:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (172493ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (482703ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138428ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-33
2022-03-28 14:37:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 14:37:47 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67826ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:47 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:47 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (464839ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:47 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:47 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:47 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (455065ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (24939ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159099ms till timeout)
2022-03-28 14:37:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 14:37:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 14:37:48 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:48 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 14:37:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (171279ms till timeout)
2022-03-28 14:37:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (481487ms till timeout)
2022-03-28 14:37:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137214ms till timeout)
2022-03-28 14:37:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 14:37:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 14:37:48 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66612ms till timeout)
2022-03-28 14:37:48 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (23938ms till timeout)
2022-03-28 14:37:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:48 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:48 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:48 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (463290ms till timeout)
2022-03-28 14:37:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157879ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 14:37:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 14:37:49 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:49 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:49 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (453479ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 14:37:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (170061ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (480271ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135902ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (22937ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65395ms till timeout)
2022-03-28 14:37:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 14:37:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 14:37:49 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156662ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 14:37:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 14:37:50 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:50 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:50 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (461748ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (21937ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:50 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:50 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (451952ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 14:37:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (168845ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (479055ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134781ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64179ms till timeout)
2022-03-28 14:37:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 14:37:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 14:37:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155446ms till timeout)
2022-03-28 14:37:51 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 14:37:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 14:37:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (20936ms till timeout)
2022-03-28 14:37:51 [ForkJoinPool-1-worker-25] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:51 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:51 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:51 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (460262ms till timeout)
2022-03-28 14:37:51 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 14:37:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (167631ms till timeout)
2022-03-28 14:37:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (477841ms till timeout)
2022-03-28 14:37:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133567ms till timeout)
2022-03-28 14:37:52 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62964ms till timeout)
2022-03-28 14:37:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 14:37:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Return code: 1
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-9" not found
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-8], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:267] testTlsExternalUser - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUser
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 8
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-03-28 14:37:52 [ForkJoinPool-1-worker-25] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for UserST
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-28 14:37:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:37:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (19936ms till timeout)
2022-03-28 14:37:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154067ms till timeout)
2022-03-28 14:37:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 14:37:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st removal
2022-03-28 14:37:52 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:52 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:53 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 14:37:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (166518ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (476623ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132345ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61837ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479497ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 14:37:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-34
2022-03-28 14:37:53 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:53 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:53 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (458762ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (18935ms till timeout)
2022-03-28 14:37:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-34
2022-03-28 14:37:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 14:37:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152849ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 14:37:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (165304ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (475513ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:54 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131137ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60630ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 14:37:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-35
2022-03-28 14:37:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (17934ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:54 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478037ms till timeout)
2022-03-28 14:37:54 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:54 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:54 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (457238ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-35
2022-03-28 14:37:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-351
2022-03-28 14:37:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151632ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-351
2022-03-28 14:37:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-354
2022-03-28 14:37:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129928ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (474198ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 14:37:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (163987ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59421ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (16934ms till timeout)
2022-03-28 14:37:55 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:55 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-354
2022-03-28 14:37:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-36
2022-03-28 14:37:56 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:56 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476574ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150416ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:56 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:56 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (455775ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (15933ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-36
2022-03-28 14:37:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-360
2022-03-28 14:37:56 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58092ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128598ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (472868ms till timeout)
2022-03-28 14:37:56 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 14:37:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (162657ms till timeout)
2022-03-28 14:37:57 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-360
2022-03-28 14:37:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 14:37:57 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149200ms till timeout)
2022-03-28 14:37:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (14933ms till timeout)
2022-03-28 14:37:57 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:57 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475116ms till timeout)
2022-03-28 14:37:57 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:57 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:57 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (454285ms till timeout)
2022-03-28 14:37:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 14:37:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 14:37:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (471759ms till timeout)
2022-03-28 14:37:58 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56981ms till timeout)
2022-03-28 14:37:58 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 14:37:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (161335ms till timeout)
2022-03-28 14:37:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127275ms till timeout)
2022-03-28 14:37:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 14:37:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-363
2022-03-28 14:37:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (13932ms till timeout)
2022-03-28 14:37:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:58 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147985ms till timeout)
2022-03-28 14:37:58 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-363
2022-03-28 14:37:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-364
2022-03-28 14:37:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (470650ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:37:59 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (473637ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55764ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:37:59 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:59 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (452772ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126059ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 14:37:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (160117ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (12932ms till timeout)
2022-03-28 14:37:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-364
2022-03-28 14:37:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:37:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-365
2022-03-28 14:37:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:37:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146768ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-365
2022-03-28 14:38:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 14:38:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (469544ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:00 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54653ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124946ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (472202ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (11931ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 14:38:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-367
2022-03-28 14:38:00 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 14:38:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (158794ms till timeout)
2022-03-28 14:38:00 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:00 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:00 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (451316ms till timeout)
2022-03-28 14:38:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145550ms till timeout)
2022-03-28 14:38:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-367
2022-03-28 14:38:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-369
2022-03-28 14:38:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (468437ms till timeout)
2022-03-28 14:38:01 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53542ms till timeout)
2022-03-28 14:38:01 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (10931ms till timeout)
2022-03-28 14:38:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123837ms till timeout)
2022-03-28 14:38:01 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-369
2022-03-28 14:38:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-37
2022-03-28 14:38:01 [ForkJoinPool-1-worker-3] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 14:38:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176]  Podmy-cluster-58df3988-connect will have 0 replicas not ready, will try again in 1000 ms (157579ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:02 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (470723ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:02 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:02 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (449844ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144335ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-37
2022-03-28 14:38:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-379
2022-03-28 14:38:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (467266ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (9930ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52381ms till timeout)
2022-03-28 14:38:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122724ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-379
2022-03-28 14:38:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-38
2022-03-28 14:38:03 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:03 [ForkJoinPool-1-worker-3] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 14:38:03 [ForkJoinPool-1-worker-3] INFO  [PodUtils:228] Pod my-cluster-58df3988-connect has 0 replicas
2022-03-28 14:38:03 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-03-28 14:38:03 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:03 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-58df3988-connect will be ready
2022-03-28 14:38:03 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58df3988-connect will be ready
2022-03-28 14:38:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (469247ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-38
2022-03-28 14:38:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 14:38:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (8930ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (479790ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (466141ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:03 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:03 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (448370ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142999ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51255ms till timeout)
2022-03-28 14:38:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121614ms till timeout)
2022-03-28 14:38:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 14:38:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 14:38:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (7929ms till timeout)
2022-03-28 14:38:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 14:38:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-39
2022-03-28 14:38:04 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (478681ms till timeout)
2022-03-28 14:38:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (464927ms till timeout)
2022-03-28 14:38:04 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50042ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (467789ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141674ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120439ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:05 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:05 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (446915ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-39
2022-03-28 14:38:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-391
2022-03-28 14:38:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (6929ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-391
2022-03-28 14:38:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-392
2022-03-28 14:38:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (477572ms till timeout)
2022-03-28 14:38:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (463818ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:06 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48931ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:06 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119224ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-392
2022-03-28 14:38:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-396
2022-03-28 14:38:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140352ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:06 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (466321ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (5928ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:06 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:06 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (445452ms till timeout)
2022-03-28 14:38:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-396
2022-03-28 14:38:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-398
2022-03-28 14:38:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (476462ms till timeout)
2022-03-28 14:38:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (462705ms till timeout)
2022-03-28 14:38:07 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47815ms till timeout)
2022-03-28 14:38:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118112ms till timeout)
2022-03-28 14:38:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-398
2022-03-28 14:38:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 14:38:07 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139130ms till timeout)
2022-03-28 14:38:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (4927ms till timeout)
2022-03-28 14:38:07 [ForkJoinPool-1-worker-21] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:07 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:07 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (464830ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 14:38:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-409
2022-03-28 14:38:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (475354ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Return code: 1
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:267] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-03-28 14:38:08 [ForkJoinPool-1-worker-21] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:38:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (461597ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46702ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117001ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (3927ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-409
2022-03-28 14:38:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-413
2022-03-28 14:38:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137912ms till timeout)
2022-03-28 14:38:08 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (474245ms till timeout)
2022-03-28 14:38:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-413
2022-03-28 14:38:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-416
2022-03-28 14:38:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (460485ms till timeout)
2022-03-28 14:38:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (463393ms till timeout)
2022-03-28 14:38:09 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45591ms till timeout)
2022-03-28 14:38:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (2926ms till timeout)
2022-03-28 14:38:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115890ms till timeout)
2022-03-28 14:38:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-416
2022-03-28 14:38:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-417
2022-03-28 14:38:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136687ms till timeout)
2022-03-28 14:38:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-417
2022-03-28 14:38:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 14:38:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (473137ms till timeout)
2022-03-28 14:38:10 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (459377ms till timeout)
2022-03-28 14:38:10 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44479ms till timeout)
2022-03-28 14:38:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (1926ms till timeout)
2022-03-28 14:38:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114778ms till timeout)
2022-03-28 14:38:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 14:38:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-42
2022-03-28 14:38:10 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:10 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (461944ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135467ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-42
2022-03-28 14:38:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 14:38:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (472029ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (458269ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties not ready, will try again in 925 ms (925ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43369ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113666ms till timeout)
2022-03-28 14:38:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 14:38:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 14:38:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (460471ms till timeout)
2022-03-28 14:38:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134249ms till timeout)
2022-03-28 14:38:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 14:38:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 14:38:12 [ForkJoinPool-1-worker-31] ERROR [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-b0221df6} has correct cruise control metric reporter properties, null
2022-03-28 14:38:12 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-03-28 14:38:12 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:38:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (470828ms till timeout)
2022-03-28 14:38:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (457074ms till timeout)
2022-03-28 14:38:12 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:38:12 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42183ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112481ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 14:38:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-03-28 14:38:13 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b0221df6-kafka rolling update
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for component with name my-cluster-b0221df6-kafka rolling update
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1799889ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (469719ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 14:38:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 14:38:13 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:13 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (459002ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (455962ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132925ms till timeout)
2022-03-28 14:38:13 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41070ms till timeout)
2022-03-28 14:38:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111371ms till timeout)
2022-03-28 14:38:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 14:38:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 14:38:14 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (468610ms till timeout)
2022-03-28 14:38:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 14:38:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 14:38:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (454854ms till timeout)
2022-03-28 14:38:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:15 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39856ms till timeout)
2022-03-28 14:38:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (457542ms till timeout)
2022-03-28 14:38:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131487ms till timeout)
2022-03-28 14:38:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110252ms till timeout)
2022-03-28 14:38:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 14:38:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 14:38:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (467502ms till timeout)
2022-03-28 14:38:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 14:38:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 14:38:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (453746ms till timeout)
2022-03-28 14:38:16 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:16 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38741ms till timeout)
2022-03-28 14:38:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109034ms till timeout)
2022-03-28 14:38:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130165ms till timeout)
2022-03-28 14:38:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 14:38:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-429
2022-03-28 14:38:16 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:16 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (456051ms till timeout)
2022-03-28 14:38:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (466392ms till timeout)
2022-03-28 14:38:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (452638ms till timeout)
2022-03-28 14:38:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-429
2022-03-28 14:38:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-43
2022-03-28 14:38:17 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37623ms till timeout)
2022-03-28 14:38:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107922ms till timeout)
2022-03-28 14:38:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-43
2022-03-28 14:38:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 14:38:17 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128941ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (465283ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (454537ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (451528ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 14:38:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 14:38:18 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:18 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36512ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:18 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:18 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1794756ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106800ms till timeout)
2022-03-28 14:38:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 14:38:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 14:38:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127726ms till timeout)
2022-03-28 14:38:19 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (464128ms till timeout)
2022-03-28 14:38:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (450376ms till timeout)
2022-03-28 14:38:19 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (35401ms till timeout)
2022-03-28 14:38:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 14:38:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 14:38:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (453039ms till timeout)
2022-03-28 14:38:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105690ms till timeout)
2022-03-28 14:38:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 14:38:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 14:38:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126509ms till timeout)
2022-03-28 14:38:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (463019ms till timeout)
2022-03-28 14:38:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (449263ms till timeout)
2022-03-28 14:38:20 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34289ms till timeout)
2022-03-28 14:38:20 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 14:38:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 14:38:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104577ms till timeout)
2022-03-28 14:38:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (451568ms till timeout)
2022-03-28 14:38:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 14:38:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-436
2022-03-28 14:38:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125294ms till timeout)
2022-03-28 14:38:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (461871ms till timeout)
2022-03-28 14:38:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (448118ms till timeout)
2022-03-28 14:38:21 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33177ms till timeout)
2022-03-28 14:38:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-436
2022-03-28 14:38:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 14:38:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103465ms till timeout)
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 14:38:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-438
2022-03-28 14:38:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (460758ms till timeout)
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:254] UserST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-28 14:38:22 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 4
[INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 615.598 s - in io.strimzi.systemtest.operators.user.UserST
2022-03-28 14:38:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (447004ms till timeout)
2022-03-28 14:38:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123967ms till timeout)
2022-03-28 14:38:22 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32067ms till timeout)
2022-03-28 14:38:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-438
2022-03-28 14:38:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-439
2022-03-28 14:38:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102252ms till timeout)
2022-03-28 14:38:23 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-439
2022-03-28 14:38:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 14:38:23 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:23 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:23 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1789541ms till timeout)
2022-03-28 14:38:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (459608ms till timeout)
2022-03-28 14:38:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (445856ms till timeout)
2022-03-28 14:38:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122713ms till timeout)
2022-03-28 14:38:24 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30853ms till timeout)
2022-03-28 14:38:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 14:38:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 14:38:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101141ms till timeout)
2022-03-28 14:38:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 14:38:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 14:38:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (458499ms till timeout)
2022-03-28 14:38:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (444745ms till timeout)
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:38:20Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:38:20Z, lastTransitionTime=2022-03-28T14:38:20Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:36:41Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:25 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29642ms till timeout)
2022-03-28 14:38:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 14:38:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:38:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99931ms till timeout)
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-d617cda6-kafka-clients-r7r29 log
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job create-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-03-28 14:38:25 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 14:38:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:38:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (457304ms till timeout)
2022-03-28 14:38:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (443550ms till timeout)
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 14:38:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 14:38:26 [ForkJoinPool-1-worker-23] INFO  [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-431951f6-kafka-clients-2jv4g log
2022-03-28 14:38:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219677ms till timeout)
2022-03-28 14:38:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98742ms till timeout)
2022-03-28 14:38:26 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-431951f6-kafka-clients deletion
2022-03-28 14:38:26 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-431951f6-kafka-clients to be deleted
2022-03-28 14:38:26 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-431951f6-kafka-clients to be deleted not ready, will try again in 5000 ms (179889ms till timeout)
2022-03-28 14:38:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 14:38:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 14:38:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (456151ms till timeout)
2022-03-28 14:38:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (442397ms till timeout)
2022-03-28 14:38:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 14:38:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 14:38:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218462ms till timeout)
2022-03-28 14:38:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97422ms till timeout)
2022-03-28 14:38:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 14:38:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 14:38:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (455041ms till timeout)
2022-03-28 14:38:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (441285ms till timeout)
2022-03-28 14:38:28 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 14:38:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 14:38:29 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:29 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:29 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:29 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1784326ms till timeout)
2022-03-28 14:38:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217194ms till timeout)
2022-03-28 14:38:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96243ms till timeout)
2022-03-28 14:38:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (453933ms till timeout)
2022-03-28 14:38:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 14:38:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 14:38:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (440176ms till timeout)
2022-03-28 14:38:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 14:38:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 14:38:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215977ms till timeout)
2022-03-28 14:38:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94939ms till timeout)
2022-03-28 14:38:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (452825ms till timeout)
2022-03-28 14:38:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 14:38:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 14:38:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (438927ms till timeout)
2022-03-28 14:38:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 14:38:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 14:38:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214760ms till timeout)
2022-03-28 14:38:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 14:38:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 14:38:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93721ms till timeout)
2022-03-28 14:38:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (451633ms till timeout)
2022-03-28 14:38:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (437819ms till timeout)
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] DEBUG [JobUtils:40] Job delete-admin-my-cluster-431951f6-kafka-clients was deleted
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-431951f6-kafka-clients will be in active state
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:38:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 14:38:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] INFO  [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-431951f6-kafka-clients to finished
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:27Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:32 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119786ms till timeout)
2022-03-28 14:38:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213506ms till timeout)
2022-03-28 14:38:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 14:38:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 14:38:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (450489ms till timeout)
2022-03-28 14:38:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92569ms till timeout)
2022-03-28 14:38:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (436711ms till timeout)
2022-03-28 14:38:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 14:38:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 14:38:33 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:27Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:33 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118570ms till timeout)
2022-03-28 14:38:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:34 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 14:38:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 14:38:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (449379ms till timeout)
2022-03-28 14:38:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212180ms till timeout)
2022-03-28 14:38:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91248ms till timeout)
2022-03-28 14:38:34 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:34 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:34 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:34 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1779096ms till timeout)
2022-03-28 14:38:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (435515ms till timeout)
2022-03-28 14:38:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 14:38:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 14:38:34 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:27Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:35 [ForkJoinPool-1-worker-23] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117354ms till timeout)
2022-03-28 14:38:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (448265ms till timeout)
2022-03-28 14:38:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 14:38:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-46
2022-03-28 14:38:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (434404ms till timeout)
2022-03-28 14:38:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90131ms till timeout)
2022-03-28 14:38:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210855ms till timeout)
2022-03-28 14:38:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-46
2022-03-28 14:38:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-473
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] DEBUG [ClientUtils:79] Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:27Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[393bdc33-c2d7-4d51-8b24-643c557e239c], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-473
2022-03-28 14:38:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-475
2022-03-28 14:38:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (447056ms till timeout)
2022-03-28 14:38:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (433196ms till timeout)
2022-03-28 14:38:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88922ms till timeout)
2022-03-28 14:38:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:36 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:36 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-431951f6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209539ms till timeout)
2022-03-28 14:38:36 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:38:36 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:38:36 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:38:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-431951f6-kafka-clients
2022-03-28 14:38:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-475
2022-03-28 14:38:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-478
2022-03-28 14:38:37 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1088363724-759527614 in namespace throttling-quota-st
2022-03-28 14:38:37 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1088363724-759527614
2022-03-28 14:38:37 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:38:37 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:267] testKafkaAdminTopicOperations - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:38:37 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testKafkaAdminTopicOperations
2022-03-28 14:38:37 [ForkJoinPool-1-worker-23] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 6
2022-03-28 14:38:37 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-03-28 14:38:37 [ForkJoinPool-1-worker-23] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:38:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (445947ms till timeout)
2022-03-28 14:38:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-478
2022-03-28 14:38:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-479
2022-03-28 14:38:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (432086ms till timeout)
2022-03-28 14:38:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87707ms till timeout)
2022-03-28 14:38:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208321ms till timeout)
2022-03-28 14:38:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-479
2022-03-28 14:38:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-48
2022-03-28 14:38:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (444836ms till timeout)
2022-03-28 14:38:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-48
2022-03-28 14:38:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-480
2022-03-28 14:38:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (430974ms till timeout)
2022-03-28 14:38:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86595ms till timeout)
2022-03-28 14:38:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-480
2022-03-28 14:38:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-481
2022-03-28 14:38:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207105ms till timeout)
2022-03-28 14:38:39 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:39 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:39 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:39 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1773882ms till timeout)
2022-03-28 14:38:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (443727ms till timeout)
2022-03-28 14:38:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-481
2022-03-28 14:38:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-482
2022-03-28 14:38:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (429866ms till timeout)
2022-03-28 14:38:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85485ms till timeout)
2022-03-28 14:38:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-482
2022-03-28 14:38:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-483
2022-03-28 14:38:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205888ms till timeout)
2022-03-28 14:38:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (442617ms till timeout)
2022-03-28 14:38:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-483
2022-03-28 14:38:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-484
2022-03-28 14:38:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (428758ms till timeout)
2022-03-28 14:38:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84373ms till timeout)
2022-03-28 14:38:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-484
2022-03-28 14:38:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-485
2022-03-28 14:38:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204673ms till timeout)
2022-03-28 14:38:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (441505ms till timeout)
2022-03-28 14:38:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-485
2022-03-28 14:38:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 14:38:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (427650ms till timeout)
2022-03-28 14:38:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83262ms till timeout)
2022-03-28 14:38:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 14:38:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-487
2022-03-28 14:38:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203451ms till timeout)
2022-03-28 14:38:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (440398ms till timeout)
2022-03-28 14:38:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-487
2022-03-28 14:38:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-488
2022-03-28 14:38:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (426541ms till timeout)
2022-03-28 14:38:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82151ms till timeout)
2022-03-28 14:38:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-488
2022-03-28 14:38:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-489
2022-03-28 14:38:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202235ms till timeout)
2022-03-28 14:38:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-489
2022-03-28 14:38:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-491
2022-03-28 14:38:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (439219ms till timeout)
2022-03-28 14:38:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (425433ms till timeout)
2022-03-28 14:38:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81040ms till timeout)
2022-03-28 14:38:44 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:44 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:44 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:44 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1768772ms till timeout)
2022-03-28 14:38:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-491
2022-03-28 14:38:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-495
2022-03-28 14:38:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-495
2022-03-28 14:38:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-499
2022-03-28 14:38:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (438108ms till timeout)
2022-03-28 14:38:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200912ms till timeout)
2022-03-28 14:38:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (424248ms till timeout)
2022-03-28 14:38:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79869ms till timeout)
2022-03-28 14:38:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-499
2022-03-28 14:38:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 14:38:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 14:38:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 14:38:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (436996ms till timeout)
2022-03-28 14:38:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (423137ms till timeout)
2022-03-28 14:38:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199589ms till timeout)
2022-03-28 14:38:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78756ms till timeout)
2022-03-28 14:38:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 14:38:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-60
2022-03-28 14:38:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-60
2022-03-28 14:38:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 14:38:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (435884ms till timeout)
2022-03-28 14:38:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (422027ms till timeout)
2022-03-28 14:38:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77538ms till timeout)
2022-03-28 14:38:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198256ms till timeout)
2022-03-28 14:38:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 14:38:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-64
2022-03-28 14:38:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (434774ms till timeout)
2022-03-28 14:38:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-64
2022-03-28 14:38:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-69
2022-03-28 14:38:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (420920ms till timeout)
2022-03-28 14:38:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76426ms till timeout)
2022-03-28 14:38:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197037ms till timeout)
2022-03-28 14:38:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-69
2022-03-28 14:38:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-7
2022-03-28 14:38:49 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:49 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:49 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:49 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:49 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1763559ms till timeout)
2022-03-28 14:38:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (433626ms till timeout)
2022-03-28 14:38:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (419812ms till timeout)
2022-03-28 14:38:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-7
2022-03-28 14:38:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-73
2022-03-28 14:38:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75309ms till timeout)
2022-03-28 14:38:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195820ms till timeout)
2022-03-28 14:38:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-73
2022-03-28 14:38:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 14:38:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (432508ms till timeout)
2022-03-28 14:38:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (418702ms till timeout)
2022-03-28 14:38:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 14:38:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-76
2022-03-28 14:38:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74197ms till timeout)
2022-03-28 14:38:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194602ms till timeout)
2022-03-28 14:38:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-76
2022-03-28 14:38:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-78
2022-03-28 14:38:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (431397ms till timeout)
2022-03-28 14:38:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (417593ms till timeout)
2022-03-28 14:38:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-78
2022-03-28 14:38:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 14:38:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73085ms till timeout)
2022-03-28 14:38:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193384ms till timeout)
2022-03-28 14:38:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 14:38:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 14:38:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (430290ms till timeout)
2022-03-28 14:38:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (416432ms till timeout)
2022-03-28 14:38:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71974ms till timeout)
2022-03-28 14:38:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 14:38:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 14:38:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192166ms till timeout)
2022-03-28 14:38:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 14:38:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-93
2022-03-28 14:38:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (429149ms till timeout)
2022-03-28 14:38:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (415324ms till timeout)
2022-03-28 14:38:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70861ms till timeout)
2022-03-28 14:38:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-93
2022-03-28 14:38:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 14:38:54 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:55 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:55 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:38:55 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:38:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1758344ms till timeout)
2022-03-28 14:38:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-97
2022-03-28 14:38:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (428042ms till timeout)
2022-03-28 14:38:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190846ms till timeout)
2022-03-28 14:38:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (414181ms till timeout)
2022-03-28 14:38:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69751ms till timeout)
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-97
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-1ac6bea1-kafka-clients in namespace throttling-quota-st
2022-03-28 14:38:55 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1691702842-2021683609 in namespace throttling-quota-st
2022-03-28 14:38:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-1ac6bea1-kafka-clients
2022-03-28 14:38:55 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1691702842-2021683609
2022-03-28 14:38:56 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:38:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateTopic - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:38:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateTopic
2022-03-28 14:38:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 5
2022-03-28 14:38:56 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-03-28 14:38:56 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:38:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (426932ms till timeout)
2022-03-28 14:38:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (413071ms till timeout)
2022-03-28 14:38:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189523ms till timeout)
2022-03-28 14:38:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68586ms till timeout)
2022-03-28 14:38:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (425821ms till timeout)
2022-03-28 14:38:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (411964ms till timeout)
2022-03-28 14:38:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67466ms till timeout)
2022-03-28 14:38:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188192ms till timeout)
2022-03-28 14:38:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (424712ms till timeout)
2022-03-28 14:38:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (410856ms till timeout)
2022-03-28 14:38:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66353ms till timeout)
2022-03-28 14:38:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:38:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186964ms till timeout)
2022-03-28 14:38:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (423604ms till timeout)
2022-03-28 14:39:00 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (409747ms till timeout)
2022-03-28 14:39:00 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:00 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:00 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:39:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1753116ms till timeout)
2022-03-28 14:39:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65242ms till timeout)
2022-03-28 14:39:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185747ms till timeout)
2022-03-28 14:39:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (422495ms till timeout)
2022-03-28 14:39:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (408636ms till timeout)
2022-03-28 14:39:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64132ms till timeout)
2022-03-28 14:39:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184529ms till timeout)
2022-03-28 14:39:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (421384ms till timeout)
2022-03-28 14:39:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (407529ms till timeout)
2022-03-28 14:39:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63014ms till timeout)
2022-03-28 14:39:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183312ms till timeout)
2022-03-28 14:39:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (420276ms till timeout)
2022-03-28 14:39:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (406421ms till timeout)
2022-03-28 14:39:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61903ms till timeout)
2022-03-28 14:39:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182091ms till timeout)
2022-03-28 14:39:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (419075ms till timeout)
2022-03-28 14:39:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (405312ms till timeout)
2022-03-28 14:39:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60792ms till timeout)
2022-03-28 14:39:05 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:05 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:05 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:05 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:39:05 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1747901ms till timeout)
2022-03-28 14:39:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (417865ms till timeout)
2022-03-28 14:39:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180669ms till timeout)
2022-03-28 14:39:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (404108ms till timeout)
2022-03-28 14:39:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59681ms till timeout)
2022-03-28 14:39:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (416754ms till timeout)
2022-03-28 14:39:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (403001ms till timeout)
2022-03-28 14:39:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179450ms till timeout)
2022-03-28 14:39:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58513ms till timeout)
2022-03-28 14:39:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (415642ms till timeout)
2022-03-28 14:39:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (401888ms till timeout)
2022-03-28 14:39:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57403ms till timeout)
2022-03-28 14:39:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178125ms till timeout)
2022-03-28 14:39:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (414533ms till timeout)
2022-03-28 14:39:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (400777ms till timeout)
2022-03-28 14:39:09 [ForkJoinPool-1-worker-5] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-93bfec4c-kafka-clients-rd6vg log
2022-03-28 14:39:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:09 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-93bfec4c-kafka-clients deletion
2022-03-28 14:39:09 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-93bfec4c-kafka-clients to be deleted
2022-03-28 14:39:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176800ms till timeout)
2022-03-28 14:39:09 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job create-admin-my-cluster-93bfec4c-kafka-clients was deleted
2022-03-28 14:39:09 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 14:39:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (413424ms till timeout)
2022-03-28 14:39:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (399668ms till timeout)
2022-03-28 14:39:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 14:39:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:39:10 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 14:39:10 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:10 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:10 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:10 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:39:10 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1742688ms till timeout)
2022-03-28 14:39:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175556ms till timeout)
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 14:39:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (412313ms till timeout)
2022-03-28 14:39:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (398559ms till timeout)
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-93bfec4c-kafka-clients in namespace throttling-quota-st
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-93bfec4c-kafka-clients
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-93bfec4c-kafka-clients will be in active state
2022-03-28 14:39:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:39:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:12 [ForkJoinPool-1-worker-5] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:39:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:39:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174133ms till timeout)
2022-03-28 14:39:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (411118ms till timeout)
2022-03-28 14:39:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299785ms till timeout)
2022-03-28 14:39:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (397363ms till timeout)
2022-03-28 14:39:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: my-cluster-58df3988-connect will be ready not ready, will try again in 1000 ms (410007ms till timeout)
2022-03-28 14:39:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (396253ms till timeout)
2022-03-28 14:39:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172810ms till timeout)
2022-03-28 14:39:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298570ms till timeout)
2022-03-28 14:39:14 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-58df3988-connect is ready
2022-03-28 14:39:14 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-58df3988-connect to be ready
2022-03-28 14:39:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (395138ms till timeout)
2022-03-28 14:39:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297451ms till timeout)
2022-03-28 14:39:14 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready
2022-03-28 14:39:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171484ms till timeout)
2022-03-28 14:39:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599786ms till timeout)
2022-03-28 14:39:15 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (394028ms till timeout)
2022-03-28 14:39:15 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:15 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:15 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-2 hasn't rolled
2022-03-28 14:39:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1737392ms till timeout)
2022-03-28 14:39:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296032ms till timeout)
2022-03-28 14:39:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598675ms till timeout)
2022-03-28 14:39:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170161ms till timeout)
2022-03-28 14:39:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (392919ms till timeout)
2022-03-28 14:39:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294918ms till timeout)
2022-03-28 14:39:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597459ms till timeout)
2022-03-28 14:39:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168940ms till timeout)
2022-03-28 14:39:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (391808ms till timeout)
2022-03-28 14:39:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293807ms till timeout)
2022-03-28 14:39:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596347ms till timeout)
2022-03-28 14:39:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167723ms till timeout)
2022-03-28 14:39:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (390699ms till timeout)
2022-03-28 14:39:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292694ms till timeout)
2022-03-28 14:39:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:19 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595234ms till timeout)
2022-03-28 14:39:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166465ms till timeout)
2022-03-28 14:39:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (389590ms till timeout)
2022-03-28 14:39:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291583ms till timeout)
2022-03-28 14:39:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594123ms till timeout)
2022-03-28 14:39:20 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:21 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:21 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:21 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-1 hasn't rolled
2022-03-28 14:39:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1732166ms till timeout)
2022-03-28 14:39:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165143ms till timeout)
2022-03-28 14:39:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (388482ms till timeout)
2022-03-28 14:39:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290471ms till timeout)
2022-03-28 14:39:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593010ms till timeout)
2022-03-28 14:39:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (387367ms till timeout)
2022-03-28 14:39:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163819ms till timeout)
2022-03-28 14:39:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289360ms till timeout)
2022-03-28 14:39:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591899ms till timeout)
2022-03-28 14:39:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (386256ms till timeout)
2022-03-28 14:39:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162596ms till timeout)
2022-03-28 14:39:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288248ms till timeout)
2022-03-28 14:39:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:23 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590787ms till timeout)
2022-03-28 14:39:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (385147ms till timeout)
2022-03-28 14:39:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287137ms till timeout)
2022-03-28 14:39:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161267ms till timeout)
2022-03-28 14:39:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:25 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58df3988, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-58df3988-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589571ms till timeout)
2022-03-28 14:39:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (384040ms till timeout)
2022-03-28 14:39:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286025ms till timeout)
2022-03-28 14:39:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:26 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58df3988-connect-76c5cb4848-ccn6g not ready: my-cluster-58df3988-connect)
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-58df3988-connect-76c5cb4848-ccn6g are ready
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:197] Deployment my-cluster-58df3988-connect is ready
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update KafkaConnector my-cluster-58df3988 in namespace namespace-9
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-28 14:39:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159944ms till timeout)
2022-03-28 14:39:26 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:26 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:26 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-1 hasn't rolled
2022-03-28 14:39:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1726861ms till timeout)
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-58df3988
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-58df3988 will have desired state: Ready
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-58df3988 will have desired state: Ready
2022-03-28 14:39:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaConnector: my-cluster-58df3988 will have desired state: Ready not ready, will try again in 1000 ms (239892ms till timeout)
2022-03-28 14:39:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (382833ms till timeout)
2022-03-28 14:39:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284913ms till timeout)
2022-03-28 14:39:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158723ms till timeout)
2022-03-28 14:39:27 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaConnector: my-cluster-58df3988 is in desired state: Ready
2022-03-28 14:39:28 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (381620ms till timeout)
2022-03-28 14:39:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283800ms till timeout)
2022-03-28 14:39:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157504ms till timeout)
2022-03-28 14:39:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (380512ms till timeout)
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-03-28 14:39:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282689ms till timeout)
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-58df3988 will have desired state: ReconciliationPaused
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-58df3988 will have desired state: ReconciliationPaused
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaConnector: my-cluster-58df3988 is in desired state: ReconciliationPaused
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Connector's spec will be stable
2022-03-28 14:39:29 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156276ms till timeout)
2022-03-28 14:39:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (379405ms till timeout)
2022-03-28 14:39:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281576ms till timeout)
2022-03-28 14:39:31 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:31 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:31 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-03-28 14:39:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (178785ms till timeout)
2022-03-28 14:39:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155060ms till timeout)
2022-03-28 14:39:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (378296ms till timeout)
2022-03-28 14:39:31 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:31 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:31 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:31 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-1 hasn't rolled
2022-03-28 14:39:31 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1721643ms till timeout)
2022-03-28 14:39:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280283ms till timeout)
2022-03-28 14:39:32 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153840ms till timeout)
2022-03-28 14:39:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (377176ms till timeout)
2022-03-28 14:39:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279171ms till timeout)
2022-03-28 14:39:33 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:33 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:33 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-03-28 14:39:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (176605ms till timeout)
2022-03-28 14:39:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (376067ms till timeout)
2022-03-28 14:39:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152519ms till timeout)
2022-03-28 14:39:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278061ms till timeout)
2022-03-28 14:39:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (374960ms till timeout)
2022-03-28 14:39:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151303ms till timeout)
2022-03-28 14:39:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276950ms till timeout)
2022-03-28 14:39:35 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:35 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:35 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-03-28 14:39:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (174408ms till timeout)
2022-03-28 14:39:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (373849ms till timeout)
2022-03-28 14:39:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150088ms till timeout)
2022-03-28 14:39:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275742ms till timeout)
2022-03-28 14:39:36 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:36 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-1 hasn't rolled
2022-03-28 14:39:36 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1716427ms till timeout)
2022-03-28 14:39:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (372739ms till timeout)
2022-03-28 14:39:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274625ms till timeout)
2022-03-28 14:39:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148758ms till timeout)
2022-03-28 14:39:37 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:37 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:37 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-03-28 14:39:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (172201ms till timeout)
2022-03-28 14:39:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (371631ms till timeout)
2022-03-28 14:39:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273516ms till timeout)
2022-03-28 14:39:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147538ms till timeout)
2022-03-28 14:39:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (370524ms till timeout)
2022-03-28 14:39:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272404ms till timeout)
2022-03-28 14:39:39 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:39 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:39 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-03-28 14:39:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (169982ms till timeout)
2022-03-28 14:39:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146322ms till timeout)
2022-03-28 14:39:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (369414ms till timeout)
2022-03-28 14:39:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271293ms till timeout)
2022-03-28 14:39:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145104ms till timeout)
2022-03-28 14:39:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (368305ms till timeout)
2022-03-28 14:39:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270182ms till timeout)
2022-03-28 14:39:41 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:42 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:42 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:42 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-03-28 14:39:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (167761ms till timeout)
2022-03-28 14:39:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:42 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-1 hasn't rolled
2022-03-28 14:39:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1711211ms till timeout)
2022-03-28 14:39:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143888ms till timeout)
2022-03-28 14:39:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (367198ms till timeout)
2022-03-28 14:39:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269071ms till timeout)
2022-03-28 14:39:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142670ms till timeout)
2022-03-28 14:39:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (366005ms till timeout)
2022-03-28 14:39:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267959ms till timeout)
2022-03-28 14:39:44 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:44 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:44 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-03-28 14:39:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (165539ms till timeout)
2022-03-28 14:39:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (364894ms till timeout)
2022-03-28 14:39:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141333ms till timeout)
2022-03-28 14:39:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266843ms till timeout)
2022-03-28 14:39:45 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (363785ms till timeout)
2022-03-28 14:39:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140118ms till timeout)
2022-03-28 14:39:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265733ms till timeout)
2022-03-28 14:39:46 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:46 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:46 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-03-28 14:39:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (163218ms till timeout)
2022-03-28 14:39:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (362676ms till timeout)
2022-03-28 14:39:47 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:47 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:47 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:47 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:50] At least my-cluster-b0221df6-kafka-1 hasn't rolled
2022-03-28 14:39:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] component with name my-cluster-b0221df6-kafka rolling update not ready, will try again in 5000 ms (1705997ms till timeout)
2022-03-28 14:39:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138865ms till timeout)
2022-03-28 14:39:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264517ms till timeout)
2022-03-28 14:39:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (361568ms till timeout)
2022-03-28 14:39:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137648ms till timeout)
2022-03-28 14:39:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263301ms till timeout)
2022-03-28 14:39:48 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:48 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:48 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-03-28 14:39:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (160924ms till timeout)
2022-03-28 14:39:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (360458ms till timeout)
2022-03-28 14:39:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136432ms till timeout)
2022-03-28 14:39:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262086ms till timeout)
2022-03-28 14:39:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (359348ms till timeout)
2022-03-28 14:39:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260969ms till timeout)
2022-03-28 14:39:51 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:51 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:51 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-03-28 14:39:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (158680ms till timeout)
2022-03-28 14:39:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135101ms till timeout)
2022-03-28 14:39:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (358239ms till timeout)
2022-03-28 14:39:52 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259858ms till timeout)
2022-03-28 14:39:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b0221df6-kafka-0=8d20c829-6be0-4832-93fe-c71bbb8d7bce, my-cluster-b0221df6-kafka-1=b27b36c4-cfa6-4663-8abe-ebcc1f1ac25a, my-cluster-b0221df6-kafka-2=68ad2424-a85c-4e36-bd82-59a22d3e873b}
2022-03-28 14:39:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133880ms till timeout)
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=e8e6aebe-0792-44fe-9a00-13188c04bd22, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b0221df6-kafka-0=3100e9ac-da8f-45ea-a2cb-e158ea4a63f8, my-cluster-b0221df6-kafka-1=e8e6aebe-0792-44fe-9a00-13188c04bd22, my-cluster-b0221df6-kafka-2=f2e3279b-b8c0-459a-a5d1-543fa8d6b2db}
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-b0221df6-kafka has been successfully rolled
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 14:39:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (357108ms till timeout)
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-b0221df6-kafka to be ready
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:39:52 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799891ms till timeout)
2022-03-28 14:39:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258748ms till timeout)
2022-03-28 14:39:53 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:53 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:53 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-03-28 14:39:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (156373ms till timeout)
2022-03-28 14:39:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132663ms till timeout)
2022-03-28 14:39:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (355998ms till timeout)
2022-03-28 14:39:54 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:39:54 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:39:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798780ms till timeout)
2022-03-28 14:39:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257633ms till timeout)
2022-03-28 14:39:54 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (354891ms till timeout)
2022-03-28 14:39:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131343ms till timeout)
2022-03-28 14:39:55 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:39:55 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:39:55 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797567ms till timeout)
2022-03-28 14:39:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256521ms till timeout)
2022-03-28 14:39:55 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:55 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:55 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-03-28 14:39:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (154101ms till timeout)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (353784ms till timeout)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130127ms till timeout)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796354ms till timeout)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255411ms till timeout)
2022-03-28 14:39:56 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (352674ms till timeout)
2022-03-28 14:39:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128908ms till timeout)
2022-03-28 14:39:57 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:39:57 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:39:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795140ms till timeout)
2022-03-28 14:39:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254297ms till timeout)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:58 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:39:58 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-03-28 14:39:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (151780ms till timeout)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (351565ms till timeout)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127691ms till timeout)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793922ms till timeout)
2022-03-28 14:39:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253137ms till timeout)
2022-03-28 14:39:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:39:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (350457ms till timeout)
2022-03-28 14:39:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:39:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126477ms till timeout)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792710ms till timeout)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251925ms till timeout)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:00 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:00 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-03-28 14:40:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (149537ms till timeout)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (349348ms till timeout)
2022-03-28 14:40:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125257ms till timeout)
2022-03-28 14:40:01 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:01 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:01 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791492ms till timeout)
2022-03-28 14:40:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250802ms till timeout)
2022-03-28 14:40:01 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (348239ms till timeout)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124043ms till timeout)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790277ms till timeout)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:02 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:02 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-03-28 14:40:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (147245ms till timeout)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249493ms till timeout)
2022-03-28 14:40:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (347126ms till timeout)
2022-03-28 14:40:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122826ms till timeout)
2022-03-28 14:40:03 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:03 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:03 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789060ms till timeout)
2022-03-28 14:40:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248342ms till timeout)
2022-03-28 14:40:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (345952ms till timeout)
2022-03-28 14:40:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121611ms till timeout)
2022-03-28 14:40:04 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:04 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:04 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-03-28 14:40:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (145023ms till timeout)
2022-03-28 14:40:04 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:04 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:04 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787844ms till timeout)
2022-03-28 14:40:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (344840ms till timeout)
2022-03-28 14:40:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247061ms till timeout)
2022-03-28 14:40:05 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:40:00Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:40:00Z, lastTransitionTime=2022-03-28T14:40:00Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:38:21Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (343627ms till timeout)
2022-03-28 14:40:06 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:06 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:06 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786629ms till timeout)
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:40:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245845ms till timeout)
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-d617cda6-kafka-clients-f98xh log
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job create-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:40:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219785ms till timeout)
2022-03-28 14:40:07 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:07 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:07 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-03-28 14:40:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (142804ms till timeout)
2022-03-28 14:40:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (342519ms till timeout)
2022-03-28 14:40:07 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:07 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:07 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785412ms till timeout)
2022-03-28 14:40:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244726ms till timeout)
2022-03-28 14:40:08 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218569ms till timeout)
2022-03-28 14:40:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (341412ms till timeout)
2022-03-28 14:40:08 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:08 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:08 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784200ms till timeout)
2022-03-28 14:40:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243510ms till timeout)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:09 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:09 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-03-28 14:40:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (140532ms till timeout)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (340305ms till timeout)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217242ms till timeout)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782988ms till timeout)
2022-03-28 14:40:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242204ms till timeout)
2022-03-28 14:40:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (339195ms till timeout)
2022-03-28 14:40:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216028ms till timeout)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-b0221df6-kafka-1)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781774ms till timeout)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240989ms till timeout)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:11 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:11 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-03-28 14:40:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (138293ms till timeout)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (338086ms till timeout)
2022-03-28 14:40:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214810ms till timeout)
2022-03-28 14:40:12 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:12 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:12 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:12 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:12 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780556ms till timeout)
2022-03-28 14:40:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239868ms till timeout)
2022-03-28 14:40:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (336978ms till timeout)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213595ms till timeout)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:13 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779340ms till timeout)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238556ms till timeout)
2022-03-28 14:40:13 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988
2022-03-28 14:40:13 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:13 [ForkJoinPool-1-worker-3] INFO  [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-03-28 14:40:13 [ForkJoinPool-1-worker-3] INFO  [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-03-28 14:40:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (335788ms till timeout)
2022-03-28 14:40:14 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for KafkaConnector config will contain desired config
2022-03-28 14:40:14 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988/config
2022-03-28 14:40:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212379ms till timeout)
2022-03-28 14:40:14 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:14 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:14 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:14 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:14 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778126ms till timeout)
2022-03-28 14:40:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237437ms till timeout)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (334679ms till timeout)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988/config
2022-03-28 14:40:15 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:15 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988/config
2022-03-28 14:40:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211165ms till timeout)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:15 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776910ms till timeout)
2022-03-28 14:40:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236126ms till timeout)
2022-03-28 14:40:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (333570ms till timeout)
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-6 exec my-cluster-58df3988-connect-76c5cb4848-ccn6g -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-58df3988/config
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of NetworkPolicy my-cluster-58df3988-allow in namespace namespace-6
2022-03-28 14:40:16 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of KafkaConnector my-cluster-58df3988 in namespace namespace-6
2022-03-28 14:40:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaConnect my-cluster-58df3988 in namespace namespace-6
2022-03-28 14:40:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-58df3988-scraper in namespace namespace-6
2022-03-28 14:40:16 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of Deployment my-cluster-58df3988-kafka-clients in namespace namespace-6
2022-03-28 14:40:16 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of Kafka my-cluster-58df3988 in namespace namespace-6
2022-03-28 14:40:16 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-58df3988-allow
2022-03-28 14:40:16 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-kafka-clients
2022-03-28 14:40:16 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-scraper
2022-03-28 14:40:16 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-58df3988
2022-03-28 14:40:16 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-58df3988
2022-03-28 14:40:16 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-58df3988
2022-03-28 14:40:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:16 [ForkJoinPool-1-worker-21] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-58df3988 not ready, will try again in 10000 ms (839790ms till timeout)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209793ms till timeout)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:17 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775748ms till timeout)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-scraper not ready, will try again in 10000 ms (479577ms till timeout)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-5] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-93bfec4c-kafka-clients-mkdw4 log
2022-03-28 14:40:17 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-kafka-clients not ready, will try again in 10000 ms (479455ms till timeout)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (332443ms till timeout)
2022-03-28 14:40:17 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-93bfec4c-kafka-clients deletion
2022-03-28 14:40:17 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-93bfec4c-kafka-clients to be deleted
2022-03-28 14:40:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-93bfec4c-kafka-clients to be deleted not ready, will try again in 5000 ms (179890ms till timeout)
2022-03-28 14:40:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:18 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:18 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:18 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:18 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:18 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774531ms till timeout)
2022-03-28 14:40:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208469ms till timeout)
2022-03-28 14:40:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (331317ms till timeout)
2022-03-28 14:40:19 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:19 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:19 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:19 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:19 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773318ms till timeout)
2022-03-28 14:40:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (330202ms till timeout)
2022-03-28 14:40:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207142ms till timeout)
2022-03-28 14:40:20 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:20 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:20 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:20 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:20 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772104ms till timeout)
2022-03-28 14:40:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (329092ms till timeout)
2022-03-28 14:40:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205923ms till timeout)
2022-03-28 14:40:21 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:21 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:21 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:21 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:21 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770883ms till timeout)
2022-03-28 14:40:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (327880ms till timeout)
2022-03-28 14:40:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204707ms till timeout)
2022-03-28 14:40:22 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job create-admin-my-cluster-93bfec4c-kafka-clients was deleted
2022-03-28 14:40:22 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job alter-admin-my-cluster-93bfec4c-kafka-clients in namespace throttling-quota-st
2022-03-28 14:40:22 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:alter-admin-my-cluster-93bfec4c-kafka-clients
2022-03-28 14:40:22 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: alter-admin-my-cluster-93bfec4c-kafka-clients will be in active state
2022-03-28 14:40:22 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:40:23 [ForkJoinPool-1-worker-5] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:40:23 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:40:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (326679ms till timeout)
2022-03-28 14:40:23 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:23 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:23 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:23 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:23 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-b0221df6-kafka, strimzi.io/cluster=my-cluster-b0221df6, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769576ms till timeout)
2022-03-28 14:40:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299780ms till timeout)
2022-03-28 14:40:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203407ms till timeout)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (325569ms till timeout)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-0 not ready: kafka)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-1 not ready: kafka)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b0221df6-kafka-2 not ready: kafka)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] DEBUG [PodUtils:106] Pods my-cluster-b0221df6-kafka-0, my-cluster-b0221df6-kafka-1, my-cluster-b0221df6-kafka-2 are ready
2022-03-28 14:40:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298568ms till timeout)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-b0221df6 will have desired state: Ready
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-b0221df6 will have desired state: Ready
2022-03-28 14:40:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202085ms till timeout)
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:444] Kafka: my-cluster-b0221df6 is in desired state: Ready
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-b0221df6 is ready
2022-03-28 14:40:24 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cruise.control.metrics.reporter.ssl.keystore.location=/tmp/kafka/cluster.keystore.p12, cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm=HTTPS, cruise.control.metrics.reporter.ssl.truststore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.reporter.ssl.truststore.type=PKCS12, cruise.control.metrics.reporter.ssl.keystore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.topic.replication.factor=3, cluster-name=my-cluster-b0221df6, cruise.control.metrics.reporter.security.protocol=SSL, cruise.control.metrics.reporter.ssl.keystore.type=PKCS12, cruise.control.metrics.topic.num.partitions=1, cruise.control.metrics.reporter.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12, cruise.control.metrics.topic=strimzi.cruisecontrol.metrics, cruise.control.metrics.topic.min.insync.replicas=1, cruise.control.metrics.reporter.bootstrap.servers=my-cluster-b0221df6-kafka-brokers:9091, cruise.control.metrics.topic.auto.create=true} has correct cruise control metric reporter properties
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] INFO  [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 14:40:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (324393ms till timeout)
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of Kafka my-cluster-b0221df6 in namespace namespace-3
2022-03-28 14:40:25 [ForkJoinPool-1-worker-31] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-3, for cruise control Kafka cluster my-cluster-b0221df6
2022-03-28 14:40:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297394ms till timeout)
2022-03-28 14:40:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200696ms till timeout)
2022-03-28 14:40:26 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b0221df6
2022-03-28 14:40:26 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b0221df6 not ready, will try again in 10000 ms (839887ms till timeout)
2022-03-28 14:40:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (323286ms till timeout)
2022-03-28 14:40:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296270ms till timeout)
2022-03-28 14:40:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199363ms till timeout)
2022-03-28 14:40:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-scraper not ready, will try again in 10000 ms (469135ms till timeout)
2022-03-28 14:40:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (322108ms till timeout)
2022-03-28 14:40:27 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-kafka-clients not ready, will try again in 10000 ms (468919ms till timeout)
2022-03-28 14:40:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295099ms till timeout)
2022-03-28 14:40:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198097ms till timeout)
2022-03-28 14:40:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (320952ms till timeout)
2022-03-28 14:40:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293989ms till timeout)
2022-03-28 14:40:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (319831ms till timeout)
2022-03-28 14:40:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196771ms till timeout)
2022-03-28 14:40:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292833ms till timeout)
2022-03-28 14:40:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (318721ms till timeout)
2022-03-28 14:40:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291722ms till timeout)
2022-03-28 14:40:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195446ms till timeout)
2022-03-28 14:40:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (317606ms till timeout)
2022-03-28 14:40:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290611ms till timeout)
2022-03-28 14:40:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194226ms till timeout)
2022-03-28 14:40:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (316496ms till timeout)
2022-03-28 14:40:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289496ms till timeout)
2022-03-28 14:40:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193011ms till timeout)
2022-03-28 14:40:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (315387ms till timeout)
2022-03-28 14:40:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288385ms till timeout)
2022-03-28 14:40:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191795ms till timeout)
2022-03-28 14:40:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (314278ms till timeout)
2022-03-28 14:40:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287274ms till timeout)
2022-03-28 14:40:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190580ms till timeout)
2022-03-28 14:40:36 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:40:36 [ForkJoinPool-1-worker-31] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 14:40:36 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-28 14:40:36 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (313095ms till timeout)
2022-03-28 14:40:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286163ms till timeout)
2022-03-28 14:40:37 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:37 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:37 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479532ms till timeout)
2022-03-28 14:40:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189363ms till timeout)
2022-03-28 14:40:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-scraper not ready, will try again in 10000 ms (458917ms till timeout)
2022-03-28 14:40:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (311886ms till timeout)
2022-03-28 14:40:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284986ms till timeout)
2022-03-28 14:40:38 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:38 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-kafka-clients not ready, will try again in 10000 ms (458473ms till timeout)
2022-03-28 14:40:38 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:38 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:38 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478091ms till timeout)
2022-03-28 14:40:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188147ms till timeout)
2022-03-28 14:40:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (310777ms till timeout)
2022-03-28 14:40:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283874ms till timeout)
2022-03-28 14:40:39 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186927ms till timeout)
2022-03-28 14:40:39 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:39 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:39 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476606ms till timeout)
2022-03-28 14:40:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (309667ms till timeout)
2022-03-28 14:40:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282764ms till timeout)
2022-03-28 14:40:40 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185711ms till timeout)
2022-03-28 14:40:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (308560ms till timeout)
2022-03-28 14:40:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281649ms till timeout)
2022-03-28 14:40:41 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:41 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:41 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475139ms till timeout)
2022-03-28 14:40:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (307444ms till timeout)
2022-03-28 14:40:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184381ms till timeout)
2022-03-28 14:40:42 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280442ms till timeout)
2022-03-28 14:40:42 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:42 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:42 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473662ms till timeout)
2022-03-28 14:40:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (306333ms till timeout)
2022-03-28 14:40:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279327ms till timeout)
2022-03-28 14:40:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183051ms till timeout)
2022-03-28 14:40:43 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:44 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:44 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:44 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472168ms till timeout)
2022-03-28 14:40:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (305225ms till timeout)
2022-03-28 14:40:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278217ms till timeout)
2022-03-28 14:40:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181832ms till timeout)
2022-03-28 14:40:45 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (304113ms till timeout)
2022-03-28 14:40:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277108ms till timeout)
2022-03-28 14:40:45 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:45 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:45 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470685ms till timeout)
2022-03-28 14:40:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180618ms till timeout)
2022-03-28 14:40:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (303000ms till timeout)
2022-03-28 14:40:46 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275996ms till timeout)
2022-03-28 14:40:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:47 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:47 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:47 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469211ms till timeout)
2022-03-28 14:40:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179399ms till timeout)
2022-03-28 14:40:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (301888ms till timeout)
2022-03-28 14:40:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-scraper not ready, will try again in 10000 ms (448598ms till timeout)
2022-03-28 14:40:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274780ms till timeout)
2022-03-28 14:40:48 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178062ms till timeout)
2022-03-28 14:40:48 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:48 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:48 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467732ms till timeout)
2022-03-28 14:40:48 [ForkJoinPool-1-worker-25] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58df3988-kafka-clients not ready, will try again in 10000 ms (447834ms till timeout)
2022-03-28 14:40:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (300780ms till timeout)
2022-03-28 14:40:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273667ms till timeout)
2022-03-28 14:40:49 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176844ms till timeout)
2022-03-28 14:40:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (299673ms till timeout)
2022-03-28 14:40:50 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:50 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:50 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (466215ms till timeout)
2022-03-28 14:40:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272557ms till timeout)
2022-03-28 14:40:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175609ms till timeout)
2022-03-28 14:40:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (298475ms till timeout)
2022-03-28 14:40:51 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271448ms till timeout)
2022-03-28 14:40:51 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:51 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:51 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464728ms till timeout)
2022-03-28 14:40:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (297337ms till timeout)
2022-03-28 14:40:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174275ms till timeout)
2022-03-28 14:40:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270331ms till timeout)
2022-03-28 14:40:52 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:53 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:53 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:53 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463239ms till timeout)
2022-03-28 14:40:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (296229ms till timeout)
2022-03-28 14:40:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173061ms till timeout)
2022-03-28 14:40:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269123ms till timeout)
2022-03-28 14:40:54 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (295117ms till timeout)
2022-03-28 14:40:54 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:54 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:54 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461800ms till timeout)
2022-03-28 14:40:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171845ms till timeout)
2022-03-28 14:40:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267908ms till timeout)
2022-03-28 14:40:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (294008ms till timeout)
2022-03-28 14:40:55 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266789ms till timeout)
2022-03-28 14:40:56 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:56 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:56 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460312ms till timeout)
2022-03-28 14:40:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170516ms till timeout)
2022-03-28 14:40:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (292900ms till timeout)
2022-03-28 14:40:57 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265678ms till timeout)
2022-03-28 14:40:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169290ms till timeout)
2022-03-28 14:40:57 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:57 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:57 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (458836ms till timeout)
2022-03-28 14:40:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (291792ms till timeout)
2022-03-28 14:40:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264529ms till timeout)
2022-03-28 14:40:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:58 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168074ms till timeout)
2022-03-28 14:40:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:40:58 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:40:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-28 14:40:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:40:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (290592ms till timeout)
2022-03-28 14:40:59 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:40:59 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:59 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457360ms till timeout)
2022-03-28 14:40:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:40:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:40:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479525ms till timeout)
2022-03-28 14:40:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263418ms till timeout)
2022-03-28 14:40:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:40:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166856ms till timeout)
2022-03-28 14:41:00 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (289483ms till timeout)
2022-03-28 14:41:00 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:00 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:00 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:00 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455915ms till timeout)
2022-03-28 14:41:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262306ms till timeout)
2022-03-28 14:41:01 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:01 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (478052ms till timeout)
2022-03-28 14:41:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165640ms till timeout)
2022-03-28 14:41:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (288373ms till timeout)
2022-03-28 14:41:01 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261197ms till timeout)
2022-03-28 14:41:02 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:02 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:02 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:02 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454454ms till timeout)
2022-03-28 14:41:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164421ms till timeout)
2022-03-28 14:41:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (287266ms till timeout)
2022-03-28 14:41:02 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:02 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476551ms till timeout)
2022-03-28 14:41:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260087ms till timeout)
2022-03-28 14:41:03 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:03 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:03 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:03 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:03 [ForkJoinPool-1-worker-31] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (453000ms till timeout)
2022-03-28 14:41:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (286160ms till timeout)
2022-03-28 14:41:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163100ms till timeout)
2022-03-28 14:41:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475109ms till timeout)
2022-03-28 14:41:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258971ms till timeout)
2022-03-28 14:41:04 [ForkJoinPool-1-worker-31] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (285049ms till timeout)
2022-03-28 14:41:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161879ms till timeout)
2022-03-28 14:41:04 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Return code: 1
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-6], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:267] testDeployAndUnDeployCruiseControl - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 4
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-03-28 14:41:05 [ForkJoinPool-1-worker-31] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:41:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257858ms till timeout)
2022-03-28 14:41:05 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:05 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473657ms till timeout)
2022-03-28 14:41:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (283941ms till timeout)
2022-03-28 14:41:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160665ms till timeout)
2022-03-28 14:41:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256728ms till timeout)
2022-03-28 14:41:06 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472206ms till timeout)
2022-03-28 14:41:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (282834ms till timeout)
2022-03-28 14:41:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255618ms till timeout)
2022-03-28 14:41:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159341ms till timeout)
2022-03-28 14:41:07 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (281726ms till timeout)
2022-03-28 14:41:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470720ms till timeout)
2022-03-28 14:41:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254507ms till timeout)
2022-03-28 14:41:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158124ms till timeout)
2022-03-28 14:41:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady not ready, will try again in 1000 ms (280617ms till timeout)
2022-03-28 14:41:09 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253393ms till timeout)
2022-03-28 14:41:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (469240ms till timeout)
2022-03-28 14:41:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156907ms till timeout)
2022-03-28 14:41:10 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-77376ada is in desired state: ProposalReady
2022-03-28 14:41:10 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-77376ada will have desired state: ReconciliationPaused
2022-03-28 14:41:10 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-77376ada will have desired state: ReconciliationPaused
2022-03-28 14:41:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252283ms till timeout)
2022-03-28 14:41:10 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-77376ada is in desired state: ReconciliationPaused
2022-03-28 14:41:10 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-4/my-cluster-77376ada): Annotating KafkaRebalance:my-cluster-77376ada with annotation approve
2022-03-28 14:41:10 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 annotate kafkarebalance my-cluster-77376ada strimzi.io/rebalance=approve
2022-03-28 14:41:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155693ms till timeout)
2022-03-28 14:41:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (467786ms till timeout)
2022-03-28 14:41:11 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-4 annotate kafkarebalance my-cluster-77376ada strimzi.io/rebalance=approve
2022-03-28 14:41:11 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:11 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaRebalance status will be stable
2022-03-28 14:41:11 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 19 polls
2022-03-28 14:41:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 14:41:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251171ms till timeout)
2022-03-28 14:41:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154477ms till timeout)
2022-03-28 14:41:12 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 18 polls
2022-03-28 14:41:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (178782ms till timeout)
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-4], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaAndKafkaConnectWithConnector - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 3
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-03-28 14:41:12 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:41:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250059ms till timeout)
2022-03-28 14:41:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153262ms till timeout)
2022-03-28 14:41:13 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 17 polls
2022-03-28 14:41:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (177673ms till timeout)
2022-03-28 14:41:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248948ms till timeout)
2022-03-28 14:41:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152042ms till timeout)
2022-03-28 14:41:14 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 16 polls
2022-03-28 14:41:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (176564ms till timeout)
2022-03-28 14:41:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247838ms till timeout)
2022-03-28 14:41:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150827ms till timeout)
2022-03-28 14:41:16 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 15 polls
2022-03-28 14:41:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (175405ms till timeout)
2022-03-28 14:41:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246726ms till timeout)
2022-03-28 14:41:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:17 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 14 polls
2022-03-28 14:41:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (174296ms till timeout)
2022-03-28 14:41:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149503ms till timeout)
2022-03-28 14:41:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245560ms till timeout)
2022-03-28 14:41:18 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 13 polls
2022-03-28 14:41:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (173187ms till timeout)
2022-03-28 14:41:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148288ms till timeout)
2022-03-28 14:41:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244348ms till timeout)
2022-03-28 14:41:19 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 12 polls
2022-03-28 14:41:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (172077ms till timeout)
2022-03-28 14:41:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243235ms till timeout)
2022-03-28 14:41:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146960ms till timeout)
2022-03-28 14:41:20 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 11 polls
2022-03-28 14:41:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (170965ms till timeout)
2022-03-28 14:41:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242124ms till timeout)
2022-03-28 14:41:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145740ms till timeout)
2022-03-28 14:41:21 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 10 polls
2022-03-28 14:41:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (169854ms till timeout)
2022-03-28 14:41:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241008ms till timeout)
2022-03-28 14:41:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144523ms till timeout)
2022-03-28 14:41:22 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 9 polls
2022-03-28 14:41:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (168744ms till timeout)
2022-03-28 14:41:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239893ms till timeout)
2022-03-28 14:41:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143304ms till timeout)
2022-03-28 14:41:23 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 8 polls
2022-03-28 14:41:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (167635ms till timeout)
2022-03-28 14:41:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238783ms till timeout)
2022-03-28 14:41:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142086ms till timeout)
2022-03-28 14:41:24 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 7 polls
2022-03-28 14:41:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (166526ms till timeout)
2022-03-28 14:41:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237671ms till timeout)
2022-03-28 14:41:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140870ms till timeout)
2022-03-28 14:41:26 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 6 polls
2022-03-28 14:41:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (165416ms till timeout)
2022-03-28 14:41:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236560ms till timeout)
2022-03-28 14:41:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139653ms till timeout)
2022-03-28 14:41:27 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 5 polls
2022-03-28 14:41:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (164233ms till timeout)
2022-03-28 14:41:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235450ms till timeout)
2022-03-28 14:41:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:28 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 4 polls
2022-03-28 14:41:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (163123ms till timeout)
2022-03-28 14:41:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138331ms till timeout)
2022-03-28 14:41:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234335ms till timeout)
2022-03-28 14:41:29 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 3 polls
2022-03-28 14:41:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (162012ms till timeout)
2022-03-28 14:41:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137114ms till timeout)
2022-03-28 14:41:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233172ms till timeout)
2022-03-28 14:41:30 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 2 polls
2022-03-28 14:41:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (160902ms till timeout)
2022-03-28 14:41:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135895ms till timeout)
2022-03-28 14:41:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231955ms till timeout)
2022-03-28 14:41:31 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status gonna be stable in 1 polls
2022-03-28 14:41:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (159792ms till timeout)
2022-03-28 14:41:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230843ms till timeout)
2022-03-28 14:41:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134568ms till timeout)
2022-03-28 14:41:32 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-4/my-cluster-77376ada): KafkaRebalance status is stable for 20 polls intervals
2022-03-28 14:41:32 [ForkJoinPool-1-worker-17] INFO  [ReconciliationST:178] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-03-28 14:41:33 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady
2022-03-28 14:41:33 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-77376ada will have desired state: ProposalReady
2022-03-28 14:41:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229728ms till timeout)
2022-03-28 14:41:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:33 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-77376ada is in desired state: ProposalReady
2022-03-28 14:41:33 [ForkJoinPool-1-worker-17] INFO  [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-4/my-cluster-77376ada): Annotating KafkaRebalance:my-cluster-77376ada with annotation approve
2022-03-28 14:41:33 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-4 annotate kafkarebalance my-cluster-77376ada strimzi.io/rebalance=approve
2022-03-28 14:41:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133345ms till timeout)
2022-03-28 14:41:34 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-4 annotate kafkarebalance my-cluster-77376ada strimzi.io/rebalance=approve
2022-03-28 14:41:34 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:41:34 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-77376ada will have desired state: Ready
2022-03-28 14:41:34 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-77376ada will have desired state: Ready
2022-03-28 14:41:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (599889ms till timeout)
2022-03-28 14:41:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228582ms till timeout)
2022-03-28 14:41:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132118ms till timeout)
2022-03-28 14:41:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (598780ms till timeout)
2022-03-28 14:41:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227471ms till timeout)
2022-03-28 14:41:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130901ms till timeout)
2022-03-28 14:41:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (597671ms till timeout)
2022-03-28 14:41:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226361ms till timeout)
2022-03-28 14:41:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129685ms till timeout)
2022-03-28 14:41:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (596561ms till timeout)
2022-03-28 14:41:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225250ms till timeout)
2022-03-28 14:41:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128469ms till timeout)
2022-03-28 14:41:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (595451ms till timeout)
2022-03-28 14:41:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224138ms till timeout)
2022-03-28 14:41:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127121ms till timeout)
2022-03-28 14:41:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (594343ms till timeout)
2022-03-28 14:41:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223027ms till timeout)
2022-03-28 14:41:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125903ms till timeout)
2022-03-28 14:41:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (593160ms till timeout)
2022-03-28 14:41:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221856ms till timeout)
2022-03-28 14:41:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (592046ms till timeout)
2022-03-28 14:41:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124577ms till timeout)
2022-03-28 14:41:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220644ms till timeout)
2022-03-28 14:41:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (590936ms till timeout)
2022-03-28 14:41:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219528ms till timeout)
2022-03-28 14:41:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123247ms till timeout)
2022-03-28 14:41:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (589826ms till timeout)
2022-03-28 14:41:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218416ms till timeout)
2022-03-28 14:41:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:40:02Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[9d157036-816b-409b-b17e-8e1895e3775e], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:44 [ForkJoinPool-1-worker-19] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:41:44 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-d617cda6-kafka-clients-sz6zn log
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job create-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:41:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (588614ms till timeout)
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:41:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217202ms till timeout)
2022-03-28 14:41:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219676ms till timeout)
2022-03-28 14:41:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (587502ms till timeout)
2022-03-28 14:41:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216091ms till timeout)
2022-03-28 14:41:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218462ms till timeout)
2022-03-28 14:41:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (586391ms till timeout)
2022-03-28 14:41:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214980ms till timeout)
2022-03-28 14:41:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217244ms till timeout)
2022-03-28 14:41:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (585282ms till timeout)
2022-03-28 14:41:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213868ms till timeout)
2022-03-28 14:41:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216028ms till timeout)
2022-03-28 14:41:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (584174ms till timeout)
2022-03-28 14:41:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212758ms till timeout)
2022-03-28 14:41:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214800ms till timeout)
2022-03-28 14:41:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (583065ms till timeout)
2022-03-28 14:41:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211649ms till timeout)
2022-03-28 14:41:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213581ms till timeout)
2022-03-28 14:41:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (581956ms till timeout)
2022-03-28 14:41:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210536ms till timeout)
2022-03-28 14:41:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (580825ms till timeout)
2022-03-28 14:41:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212212ms till timeout)
2022-03-28 14:41:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209414ms till timeout)
2022-03-28 14:41:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (579715ms till timeout)
2022-03-28 14:41:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210996ms till timeout)
2022-03-28 14:41:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208200ms till timeout)
2022-03-28 14:41:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (578605ms till timeout)
2022-03-28 14:41:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207090ms till timeout)
2022-03-28 14:41:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209671ms till timeout)
2022-03-28 14:41:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (577494ms till timeout)
2022-03-28 14:41:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205978ms till timeout)
2022-03-28 14:41:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208453ms till timeout)
2022-03-28 14:41:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (576385ms till timeout)
2022-03-28 14:41:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204864ms till timeout)
2022-03-28 14:41:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207236ms till timeout)
2022-03-28 14:41:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (575277ms till timeout)
2022-03-28 14:41:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203754ms till timeout)
2022-03-28 14:41:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:41:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206021ms till timeout)
2022-03-28 14:42:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (574168ms till timeout)
2022-03-28 14:42:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202643ms till timeout)
2022-03-28 14:42:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204804ms till timeout)
2022-03-28 14:42:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (573059ms till timeout)
2022-03-28 14:42:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201531ms till timeout)
2022-03-28 14:42:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203588ms till timeout)
2022-03-28 14:42:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (571951ms till timeout)
2022-03-28 14:42:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200420ms till timeout)
2022-03-28 14:42:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202369ms till timeout)
2022-03-28 14:42:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (570768ms till timeout)
2022-03-28 14:42:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199306ms till timeout)
2022-03-28 14:42:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (569657ms till timeout)
2022-03-28 14:42:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201043ms till timeout)
2022-03-28 14:42:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198196ms till timeout)
2022-03-28 14:42:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (568546ms till timeout)
2022-03-28 14:42:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199823ms till timeout)
2022-03-28 14:42:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197025ms till timeout)
2022-03-28 14:42:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (567436ms till timeout)
2022-03-28 14:42:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198608ms till timeout)
2022-03-28 14:42:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195810ms till timeout)
2022-03-28 14:42:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (566327ms till timeout)
2022-03-28 14:42:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197391ms till timeout)
2022-03-28 14:42:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194595ms till timeout)
2022-03-28 14:42:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (565218ms till timeout)
2022-03-28 14:42:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193484ms till timeout)
2022-03-28 14:42:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196067ms till timeout)
2022-03-28 14:42:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (564108ms till timeout)
2022-03-28 14:42:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192372ms till timeout)
2022-03-28 14:42:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194840ms till timeout)
2022-03-28 14:42:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (562998ms till timeout)
2022-03-28 14:42:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191262ms till timeout)
2022-03-28 14:42:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193625ms till timeout)
2022-03-28 14:42:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (561890ms till timeout)
2022-03-28 14:42:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190152ms till timeout)
2022-03-28 14:42:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192408ms till timeout)
2022-03-28 14:42:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (560781ms till timeout)
2022-03-28 14:42:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189039ms till timeout)
2022-03-28 14:42:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191191ms till timeout)
2022-03-28 14:42:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (559590ms till timeout)
2022-03-28 14:42:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187927ms till timeout)
2022-03-28 14:42:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (558480ms till timeout)
2022-03-28 14:42:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189867ms till timeout)
2022-03-28 14:42:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186815ms till timeout)
2022-03-28 14:42:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (557371ms till timeout)
2022-03-28 14:42:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188648ms till timeout)
2022-03-28 14:42:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185704ms till timeout)
2022-03-28 14:42:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (556259ms till timeout)
2022-03-28 14:42:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187430ms till timeout)
2022-03-28 14:42:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184590ms till timeout)
2022-03-28 14:42:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (555148ms till timeout)
2022-03-28 14:42:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186214ms till timeout)
2022-03-28 14:42:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183415ms till timeout)
2022-03-28 14:42:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (554038ms till timeout)
2022-03-28 14:42:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184997ms till timeout)
2022-03-28 14:42:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182200ms till timeout)
2022-03-28 14:42:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (552927ms till timeout)
2022-03-28 14:42:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183781ms till timeout)
2022-03-28 14:42:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180985ms till timeout)
2022-03-28 14:42:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (551818ms till timeout)
2022-03-28 14:42:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179875ms till timeout)
2022-03-28 14:42:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182455ms till timeout)
2022-03-28 14:42:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (550707ms till timeout)
2022-03-28 14:42:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178763ms till timeout)
2022-03-28 14:42:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181235ms till timeout)
2022-03-28 14:42:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (549599ms till timeout)
2022-03-28 14:42:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177650ms till timeout)
2022-03-28 14:42:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180017ms till timeout)
2022-03-28 14:42:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (548416ms till timeout)
2022-03-28 14:42:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176540ms till timeout)
2022-03-28 14:42:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (547306ms till timeout)
2022-03-28 14:42:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178694ms till timeout)
2022-03-28 14:42:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175429ms till timeout)
2022-03-28 14:42:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (546196ms till timeout)
2022-03-28 14:42:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177472ms till timeout)
2022-03-28 14:42:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174318ms till timeout)
2022-03-28 14:42:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (545087ms till timeout)
2022-03-28 14:42:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176257ms till timeout)
2022-03-28 14:42:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173208ms till timeout)
2022-03-28 14:42:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (543942ms till timeout)
2022-03-28 14:42:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175039ms till timeout)
2022-03-28 14:42:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172098ms till timeout)
2022-03-28 14:42:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (542833ms till timeout)
2022-03-28 14:42:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173825ms till timeout)
2022-03-28 14:42:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170987ms till timeout)
2022-03-28 14:42:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (541722ms till timeout)
2022-03-28 14:42:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172608ms till timeout)
2022-03-28 14:42:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169809ms till timeout)
2022-03-28 14:42:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (540615ms till timeout)
2022-03-28 14:42:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171392ms till timeout)
2022-03-28 14:42:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168560ms till timeout)
2022-03-28 14:42:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (539506ms till timeout)
2022-03-28 14:42:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170175ms till timeout)
2022-03-28 14:42:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167379ms till timeout)
2022-03-28 14:42:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (538397ms till timeout)
2022-03-28 14:42:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166268ms till timeout)
2022-03-28 14:42:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168851ms till timeout)
2022-03-28 14:42:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (537249ms till timeout)
2022-03-28 14:42:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165157ms till timeout)
2022-03-28 14:42:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (536134ms till timeout)
2022-03-28 14:42:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167521ms till timeout)
2022-03-28 14:42:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164045ms till timeout)
2022-03-28 14:42:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (535024ms till timeout)
2022-03-28 14:42:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166306ms till timeout)
2022-03-28 14:42:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162929ms till timeout)
2022-03-28 14:42:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (533915ms till timeout)
2022-03-28 14:42:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165090ms till timeout)
2022-03-28 14:42:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161818ms till timeout)
2022-03-28 14:42:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (532805ms till timeout)
2022-03-28 14:42:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163876ms till timeout)
2022-03-28 14:42:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160706ms till timeout)
2022-03-28 14:42:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (531696ms till timeout)
2022-03-28 14:42:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162659ms till timeout)
2022-03-28 14:42:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159595ms till timeout)
2022-03-28 14:42:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (530587ms till timeout)
2022-03-28 14:42:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161443ms till timeout)
2022-03-28 14:42:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158485ms till timeout)
2022-03-28 14:42:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (529478ms till timeout)
2022-03-28 14:42:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160225ms till timeout)
2022-03-28 14:42:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157375ms till timeout)
2022-03-28 14:42:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (528368ms till timeout)
2022-03-28 14:42:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159009ms till timeout)
2022-03-28 14:42:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156211ms till timeout)
2022-03-28 14:42:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (527261ms till timeout)
2022-03-28 14:42:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155099ms till timeout)
2022-03-28 14:42:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157681ms till timeout)
2022-03-28 14:42:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (526081ms till timeout)
2022-03-28 14:42:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153990ms till timeout)
2022-03-28 14:42:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (524967ms till timeout)
2022-03-28 14:42:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156355ms till timeout)
2022-03-28 14:42:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152879ms till timeout)
2022-03-28 14:42:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (523856ms till timeout)
2022-03-28 14:42:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155135ms till timeout)
2022-03-28 14:42:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151760ms till timeout)
2022-03-28 14:42:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (522748ms till timeout)
2022-03-28 14:42:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153922ms till timeout)
2022-03-28 14:42:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150650ms till timeout)
2022-03-28 14:42:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (521636ms till timeout)
2022-03-28 14:42:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152706ms till timeout)
2022-03-28 14:42:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149535ms till timeout)
2022-03-28 14:42:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (520526ms till timeout)
2022-03-28 14:42:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151491ms till timeout)
2022-03-28 14:42:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148424ms till timeout)
2022-03-28 14:42:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (519418ms till timeout)
2022-03-28 14:42:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150274ms till timeout)
2022-03-28 14:42:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147310ms till timeout)
2022-03-28 14:42:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (518310ms till timeout)
2022-03-28 14:42:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149059ms till timeout)
2022-03-28 14:42:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146201ms till timeout)
2022-03-28 14:42:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (517201ms till timeout)
2022-03-28 14:42:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147841ms till timeout)
2022-03-28 14:42:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145045ms till timeout)
2022-03-28 14:42:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (516091ms till timeout)
2022-03-28 14:42:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:42:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143905ms till timeout)
2022-03-28 14:42:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146488ms till timeout)
2022-03-28 14:42:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (514888ms till timeout)
2022-03-28 14:43:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142793ms till timeout)
2022-03-28 14:43:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (513774ms till timeout)
2022-03-28 14:43:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145162ms till timeout)
2022-03-28 14:43:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141683ms till timeout)
2022-03-28 14:43:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (512664ms till timeout)
2022-03-28 14:43:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143944ms till timeout)
2022-03-28 14:43:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140573ms till timeout)
2022-03-28 14:43:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (511553ms till timeout)
2022-03-28 14:43:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142728ms till timeout)
2022-03-28 14:43:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139461ms till timeout)
2022-03-28 14:43:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (510444ms till timeout)
2022-03-28 14:43:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141514ms till timeout)
2022-03-28 14:43:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138350ms till timeout)
2022-03-28 14:43:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (509336ms till timeout)
2022-03-28 14:43:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140298ms till timeout)
2022-03-28 14:43:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137240ms till timeout)
2022-03-28 14:43:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (508226ms till timeout)
2022-03-28 14:43:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139085ms till timeout)
2022-03-28 14:43:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136130ms till timeout)
2022-03-28 14:43:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (507117ms till timeout)
2022-03-28 14:43:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137868ms till timeout)
2022-03-28 14:43:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135018ms till timeout)
2022-03-28 14:43:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (506009ms till timeout)
2022-03-28 14:43:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136654ms till timeout)
2022-03-28 14:43:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133857ms till timeout)
2022-03-28 14:43:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (504901ms till timeout)
2022-03-28 14:43:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132746ms till timeout)
2022-03-28 14:43:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135327ms till timeout)
2022-03-28 14:43:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (503726ms till timeout)
2022-03-28 14:43:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131635ms till timeout)
2022-03-28 14:43:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (502615ms till timeout)
2022-03-28 14:43:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134003ms till timeout)
2022-03-28 14:43:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130522ms till timeout)
2022-03-28 14:43:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (501505ms till timeout)
2022-03-28 14:43:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132785ms till timeout)
2022-03-28 14:43:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129412ms till timeout)
2022-03-28 14:43:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (500395ms till timeout)
2022-03-28 14:43:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131570ms till timeout)
2022-03-28 14:43:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128303ms till timeout)
2022-03-28 14:43:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (499286ms till timeout)
2022-03-28 14:43:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130355ms till timeout)
2022-03-28 14:43:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127192ms till timeout)
2022-03-28 14:43:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (498176ms till timeout)
2022-03-28 14:43:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129141ms till timeout)
2022-03-28 14:43:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126080ms till timeout)
2022-03-28 14:43:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (497068ms till timeout)
2022-03-28 14:43:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127915ms till timeout)
2022-03-28 14:43:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124969ms till timeout)
2022-03-28 14:43:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (495947ms till timeout)
2022-03-28 14:43:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126698ms till timeout)
2022-03-28 14:43:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123859ms till timeout)
2022-03-28 14:43:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaRebalance: my-cluster-77376ada will have desired state: Ready not ready, will try again in 1000 ms (494834ms till timeout)
2022-03-28 14:43:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125480ms till timeout)
2022-03-28 14:43:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122684ms till timeout)
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-77376ada is in desired state: Ready
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-2100355713-204306190 in namespace namespace-4
2022-03-28 14:43:20 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-77376ada in namespace namespace-4
2022-03-28 14:43:20 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of KafkaRebalance my-cluster-77376ada in namespace namespace-4
2022-03-28 14:43:20 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-77376ada
2022-03-28 14:43:20 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2100355713-204306190
2022-03-28 14:43:20 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-77376ada
2022-03-28 14:43:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-77376ada
2022-03-28 14:43:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-77376ada not ready, will try again in 10000 ms (839886ms till timeout)
2022-03-28 14:43:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121546ms till timeout)
2022-03-28 14:43:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124126ms till timeout)
2022-03-28 14:43:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120434ms till timeout)
2022-03-28 14:43:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122906ms till timeout)
2022-03-28 14:43:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119324ms till timeout)
2022-03-28 14:43:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121689ms till timeout)
2022-03-28 14:43:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118213ms till timeout)
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:43:20Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:43:20Z, lastTransitionTime=2022-03-28T14:43:20Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:41:40Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-d617cda6-kafka-clients-smn86 log
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job create-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:43:25 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:43:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117019ms till timeout)
2022-03-28 14:43:26 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:43:26 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:43:26 [ForkJoinPool-1-worker-19] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 14:43:26 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 14:43:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299884ms till timeout)
2022-03-28 14:43:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115908ms till timeout)
2022-03-28 14:43:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298772ms till timeout)
2022-03-28 14:43:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114799ms till timeout)
2022-03-28 14:43:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297662ms till timeout)
2022-03-28 14:43:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113687ms till timeout)
2022-03-28 14:43:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296345ms till timeout)
2022-03-28 14:43:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112578ms till timeout)
2022-03-28 14:43:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295029ms till timeout)
2022-03-28 14:43:31 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:43:31 [ForkJoinPool-1-worker-17] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:43:31 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-28 14:43:31 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111466ms till timeout)
2022-03-28 14:43:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293713ms till timeout)
2022-03-28 14:43:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110348ms till timeout)
2022-03-28 14:43:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109094ms till timeout)
2022-03-28 14:43:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292397ms till timeout)
2022-03-28 14:43:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107983ms till timeout)
2022-03-28 14:43:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291074ms till timeout)
2022-03-28 14:43:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106871ms till timeout)
2022-03-28 14:43:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289858ms till timeout)
2022-03-28 14:43:36 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:36 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474413ms till timeout)
2022-03-28 14:43:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105753ms till timeout)
2022-03-28 14:43:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288541ms till timeout)
2022-03-28 14:43:37 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104643ms till timeout)
2022-03-28 14:43:38 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:38 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472938ms till timeout)
2022-03-28 14:43:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287223ms till timeout)
2022-03-28 14:43:39 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103532ms till timeout)
2022-03-28 14:43:39 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:39 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471507ms till timeout)
2022-03-28 14:43:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285909ms till timeout)
2022-03-28 14:43:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102422ms till timeout)
2022-03-28 14:43:40 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:41 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:41 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (470030ms till timeout)
2022-03-28 14:43:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101288ms till timeout)
2022-03-28 14:43:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284591ms till timeout)
2022-03-28 14:43:42 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100179ms till timeout)
2022-03-28 14:43:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (468530ms till timeout)
2022-03-28 14:43:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283270ms till timeout)
2022-03-28 14:43:43 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99067ms till timeout)
2022-03-28 14:43:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282056ms till timeout)
2022-03-28 14:43:44 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:44 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (467078ms till timeout)
2022-03-28 14:43:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97958ms till timeout)
2022-03-28 14:43:45 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280738ms till timeout)
2022-03-28 14:43:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (465625ms till timeout)
2022-03-28 14:43:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96846ms till timeout)
2022-03-28 14:43:46 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279422ms till timeout)
2022-03-28 14:43:47 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:47 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (464155ms till timeout)
2022-03-28 14:43:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95736ms till timeout)
2022-03-28 14:43:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278106ms till timeout)
2022-03-28 14:43:48 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94624ms till timeout)
2022-03-28 14:43:48 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:48 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (462682ms till timeout)
2022-03-28 14:43:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276790ms till timeout)
2022-03-28 14:43:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93487ms till timeout)
2022-03-28 14:43:49 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92377ms till timeout)
2022-03-28 14:43:50 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:50 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (460623ms till timeout)
2022-03-28 14:43:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275466ms till timeout)
2022-03-28 14:43:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91262ms till timeout)
2022-03-28 14:43:51 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274252ms till timeout)
2022-03-28 14:43:52 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:52 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (459140ms till timeout)
2022-03-28 14:43:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90150ms till timeout)
2022-03-28 14:43:53 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272832ms till timeout)
2022-03-28 14:43:53 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:53 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (457669ms till timeout)
2022-03-28 14:43:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89039ms till timeout)
2022-03-28 14:43:54 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271517ms till timeout)
2022-03-28 14:43:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87929ms till timeout)
2022-03-28 14:43:55 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:55 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (456202ms till timeout)
2022-03-28 14:43:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270198ms till timeout)
2022-03-28 14:43:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86818ms till timeout)
2022-03-28 14:43:56 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:56 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:56 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (454752ms till timeout)
2022-03-28 14:43:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85579ms till timeout)
2022-03-28 14:43:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268882ms till timeout)
2022-03-28 14:43:57 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:58 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:58 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (453305ms till timeout)
2022-03-28 14:43:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84467ms till timeout)
2022-03-28 14:43:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267553ms till timeout)
2022-03-28 14:43:59 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:59 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:43:59 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:43:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (451854ms till timeout)
2022-03-28 14:43:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83357ms till timeout)
2022-03-28 14:43:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266340ms till timeout)
2022-03-28 14:44:00 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82239ms till timeout)
2022-03-28 14:44:01 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:01 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (450379ms till timeout)
2022-03-28 14:44:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265016ms till timeout)
2022-03-28 14:44:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81130ms till timeout)
2022-03-28 14:44:02 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:02 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:02 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (448912ms till timeout)
2022-03-28 14:44:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263700ms till timeout)
2022-03-28 14:44:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80019ms till timeout)
2022-03-28 14:44:03 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:03 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:03 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (447477ms till timeout)
2022-03-28 14:44:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262385ms till timeout)
2022-03-28 14:44:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78910ms till timeout)
2022-03-28 14:44:04 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77766ms till timeout)
2022-03-28 14:44:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261069ms till timeout)
2022-03-28 14:44:05 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:05 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (446022ms till timeout)
2022-03-28 14:44:06 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76555ms till timeout)
2022-03-28 14:44:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259754ms till timeout)
2022-03-28 14:44:06 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:06 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (444538ms till timeout)
2022-03-28 14:44:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75443ms till timeout)
2022-03-28 14:44:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258534ms till timeout)
2022-03-28 14:44:07 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-4 -o yaml
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 1
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 2
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74333ms till timeout)
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479523ms till timeout)
2022-03-28 14:44:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257314ms till timeout)
2022-03-28 14:44:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73222ms till timeout)
2022-03-28 14:44:09 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255995ms till timeout)
2022-03-28 14:44:10 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:10 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478058ms till timeout)
2022-03-28 14:44:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72112ms till timeout)
2022-03-28 14:44:11 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254680ms till timeout)
2022-03-28 14:44:11 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:11 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (476587ms till timeout)
2022-03-28 14:44:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71001ms till timeout)
2022-03-28 14:44:12 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253362ms till timeout)
2022-03-28 14:44:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69890ms till timeout)
2022-03-28 14:44:13 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:13 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 14:44:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (475129ms till timeout)
2022-03-28 14:44:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252041ms till timeout)
2022-03-28 14:44:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68738ms till timeout)
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 1
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 14:44:14 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 3
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 967.661 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 14:44:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67627ms till timeout)
2022-03-28 14:44:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250724ms till timeout)
2022-03-28 14:44:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66517ms till timeout)
2022-03-28 14:44:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249511ms till timeout)
2022-03-28 14:44:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65403ms till timeout)
2022-03-28 14:44:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248190ms till timeout)
2022-03-28 14:44:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64293ms till timeout)
2022-03-28 14:44:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246870ms till timeout)
2022-03-28 14:44:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63182ms till timeout)
2022-03-28 14:44:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245554ms till timeout)
2022-03-28 14:44:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62071ms till timeout)
2022-03-28 14:44:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60932ms till timeout)
2022-03-28 14:44:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244236ms till timeout)
2022-03-28 14:44:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59635ms till timeout)
2022-03-28 14:44:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242835ms till timeout)
2022-03-28 14:44:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58522ms till timeout)
2022-03-28 14:44:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241611ms till timeout)
2022-03-28 14:44:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57411ms till timeout)
2022-03-28 14:44:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240397ms till timeout)
2022-03-28 14:44:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56300ms till timeout)
2022-03-28 14:44:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239081ms till timeout)
2022-03-28 14:44:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55188ms till timeout)
2022-03-28 14:44:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237763ms till timeout)
2022-03-28 14:44:28 [ForkJoinPool-1-worker-5] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-93bfec4c-kafka-clients-bkx9d log
2022-03-28 14:44:29 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment alter-admin-my-cluster-93bfec4c-kafka-clients deletion
2022-03-28 14:44:29 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet alter-admin-my-cluster-93bfec4c-kafka-clients to be deleted
2022-03-28 14:44:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] ReplicaSet alter-admin-my-cluster-93bfec4c-kafka-clients to be deleted not ready, will try again in 5000 ms (179891ms till timeout)
2022-03-28 14:44:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236445ms till timeout)
2022-03-28 14:44:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235128ms till timeout)
2022-03-28 14:44:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233777ms till timeout)
2022-03-28 14:44:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232461ms till timeout)
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job alter-admin-my-cluster-93bfec4c-kafka-clients was deleted
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:teardown-delete
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129785ms till timeout)
2022-03-28 14:44:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231248ms till timeout)
2022-03-28 14:44:35 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128569ms till timeout)
2022-03-28 14:44:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230032ms till timeout)
2022-03-28 14:44:37 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127354ms till timeout)
2022-03-28 14:44:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228713ms till timeout)
2022-03-28 14:44:38 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126138ms till timeout)
2022-03-28 14:44:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227398ms till timeout)
2022-03-28 14:44:39 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124919ms till timeout)
2022-03-28 14:44:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226081ms till timeout)
2022-03-28 14:44:40 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123702ms till timeout)
2022-03-28 14:44:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224765ms till timeout)
2022-03-28 14:44:42 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122485ms till timeout)
2022-03-28 14:44:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223448ms till timeout)
2022-03-28 14:44:43 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121264ms till timeout)
2022-03-28 14:44:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222133ms till timeout)
2022-03-28 14:44:44 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120044ms till timeout)
2022-03-28 14:44:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220817ms till timeout)
2022-03-28 14:44:45 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118828ms till timeout)
2022-03-28 14:44:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219499ms till timeout)
2022-03-28 14:44:46 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117613ms till timeout)
2022-03-28 14:44:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218179ms till timeout)
2022-03-28 14:44:48 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116363ms till timeout)
2022-03-28 14:44:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216864ms till timeout)
2022-03-28 14:44:49 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115049ms till timeout)
2022-03-28 14:44:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215548ms till timeout)
2022-03-28 14:44:50 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113731ms till timeout)
2022-03-28 14:44:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214231ms till timeout)
2022-03-28 14:44:52 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112416ms till timeout)
2022-03-28 14:44:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212834ms till timeout)
2022-03-28 14:44:53 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111018ms till timeout)
2022-03-28 14:44:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211519ms till timeout)
2022-03-28 14:44:54 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109704ms till timeout)
2022-03-28 14:44:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210203ms till timeout)
2022-03-28 14:44:56 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108387ms till timeout)
2022-03-28 14:44:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208883ms till timeout)
2022-03-28 14:44:57 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107061ms till timeout)
2022-03-28 14:44:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207565ms till timeout)
2022-03-28 14:44:58 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:44:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105747ms till timeout)
2022-03-28 14:45:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206248ms till timeout)
2022-03-28 14:45:00 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104432ms till timeout)
2022-03-28 14:45:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204932ms till timeout)
2022-03-28 14:45:01 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103116ms till timeout)
2022-03-28 14:45:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203614ms till timeout)
2022-03-28 14:45:02 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101799ms till timeout)
2022-03-28 14:45:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202296ms till timeout)
2022-03-28 14:45:04 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100477ms till timeout)
2022-03-28 14:45:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200980ms till timeout)
2022-03-28 14:45:05 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99165ms till timeout)
2022-03-28 14:45:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199658ms till timeout)
2022-03-28 14:45:06 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97840ms till timeout)
2022-03-28 14:45:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198341ms till timeout)
2022-03-28 14:45:07 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96524ms till timeout)
2022-03-28 14:45:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197023ms till timeout)
2022-03-28 14:45:09 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95206ms till timeout)
2022-03-28 14:45:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195706ms till timeout)
2022-03-28 14:45:10 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93891ms till timeout)
2022-03-28 14:45:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194385ms till timeout)
2022-03-28 14:45:11 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92569ms till timeout)
2022-03-28 14:45:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193065ms till timeout)
2022-03-28 14:45:13 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91250ms till timeout)
2022-03-28 14:45:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191748ms till timeout)
2022-03-28 14:45:14 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89932ms till timeout)
2022-03-28 14:45:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190431ms till timeout)
2022-03-28 14:45:15 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88615ms till timeout)
2022-03-28 14:45:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189113ms till timeout)
2022-03-28 14:45:17 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87296ms till timeout)
2022-03-28 14:45:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187796ms till timeout)
2022-03-28 14:45:18 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85978ms till timeout)
2022-03-28 14:45:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186480ms till timeout)
2022-03-28 14:45:19 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84645ms till timeout)
2022-03-28 14:45:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185164ms till timeout)
2022-03-28 14:45:21 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83345ms till timeout)
2022-03-28 14:45:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183847ms till timeout)
2022-03-28 14:45:22 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82030ms till timeout)
2022-03-28 14:45:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182530ms till timeout)
2022-03-28 14:45:23 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80714ms till timeout)
2022-03-28 14:45:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181212ms till timeout)
2022-03-28 14:45:25 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79395ms till timeout)
2022-03-28 14:45:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179896ms till timeout)
2022-03-28 14:45:26 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78081ms till timeout)
2022-03-28 14:45:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178579ms till timeout)
2022-03-28 14:45:27 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76761ms till timeout)
2022-03-28 14:45:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177262ms till timeout)
2022-03-28 14:45:29 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75446ms till timeout)
2022-03-28 14:45:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175947ms till timeout)
2022-03-28 14:45:30 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74131ms till timeout)
2022-03-28 14:45:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174623ms till timeout)
2022-03-28 14:45:31 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72808ms till timeout)
2022-03-28 14:45:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173305ms till timeout)
2022-03-28 14:45:33 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71488ms till timeout)
2022-03-28 14:45:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171989ms till timeout)
2022-03-28 14:45:34 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70173ms till timeout)
2022-03-28 14:45:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170672ms till timeout)
2022-03-28 14:45:35 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68855ms till timeout)
2022-03-28 14:45:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169355ms till timeout)
2022-03-28 14:45:36 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67537ms till timeout)
2022-03-28 14:45:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168038ms till timeout)
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:44:29Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment teardown-delete deletion
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet teardown-delete to be deleted
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job teardown-delete was deleted
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-93bfec4c-kafka-clients in namespace throttling-quota-st
2022-03-28 14:45:38 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-93bfec4c-kafka-clients in namespace throttling-quota-st
2022-03-28 14:45:38 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-03-28 14:45:38 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job alter-admin-my-cluster-93bfec4c-kafka-clients in namespace throttling-quota-st
2022-03-28 14:45:38 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1020460016-1914644203 in namespace throttling-quota-st
2022-03-28 14:45:38 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-93bfec4c-kafka-clients
2022-03-28 14:45:38 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-93bfec4c-kafka-clients
2022-03-28 14:45:38 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:alter-admin-my-cluster-93bfec4c-kafka-clients
2022-03-28 14:45:38 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:teardown-delete
2022-03-28 14:45:38 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1020460016-1914644203
2022-03-28 14:45:39 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:45:39 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateAlterPartitions - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:45:39 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 14:45:39 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 1
2022-03-28 14:45:39 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-03-28 14:45:39 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:45:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166825ms till timeout)
2022-03-28 14:45:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165504ms till timeout)
2022-03-28 14:45:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164183ms till timeout)
2022-03-28 14:45:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162866ms till timeout)
2022-03-28 14:45:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161549ms till timeout)
2022-03-28 14:45:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160233ms till timeout)
2022-03-28 14:45:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158916ms till timeout)
2022-03-28 14:45:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157600ms till timeout)
2022-03-28 14:45:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156282ms till timeout)
2022-03-28 14:45:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154967ms till timeout)
2022-03-28 14:45:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153647ms till timeout)
2022-03-28 14:45:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152330ms till timeout)
2022-03-28 14:45:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151014ms till timeout)
2022-03-28 14:45:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149697ms till timeout)
2022-03-28 14:45:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148380ms till timeout)
2022-03-28 14:45:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147060ms till timeout)
2022-03-28 14:46:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145744ms till timeout)
2022-03-28 14:46:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144429ms till timeout)
2022-03-28 14:46:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143113ms till timeout)
2022-03-28 14:46:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141795ms till timeout)
2022-03-28 14:46:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140469ms till timeout)
2022-03-28 14:46:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139154ms till timeout)
2022-03-28 14:46:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137838ms till timeout)
2022-03-28 14:46:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136520ms till timeout)
2022-03-28 14:46:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135204ms till timeout)
2022-03-28 14:46:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133884ms till timeout)
2022-03-28 14:46:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132569ms till timeout)
2022-03-28 14:46:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131252ms till timeout)
2022-03-28 14:46:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129934ms till timeout)
2022-03-28 14:46:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128616ms till timeout)
2022-03-28 14:46:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127300ms till timeout)
2022-03-28 14:46:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125984ms till timeout)
2022-03-28 14:46:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124667ms till timeout)
2022-03-28 14:46:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123351ms till timeout)
2022-03-28 14:46:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122034ms till timeout)
2022-03-28 14:46:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120718ms till timeout)
2022-03-28 14:46:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119401ms till timeout)
2022-03-28 14:46:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118086ms till timeout)
2022-03-28 14:46:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116771ms till timeout)
2022-03-28 14:46:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115451ms till timeout)
2022-03-28 14:46:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114134ms till timeout)
2022-03-28 14:46:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112820ms till timeout)
2022-03-28 14:46:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111502ms till timeout)
2022-03-28 14:46:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110184ms till timeout)
2022-03-28 14:46:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108868ms till timeout)
2022-03-28 14:46:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107526ms till timeout)
2022-03-28 14:46:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106210ms till timeout)
2022-03-28 14:46:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104894ms till timeout)
2022-03-28 14:46:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103577ms till timeout)
2022-03-28 14:46:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102262ms till timeout)
2022-03-28 14:46:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100943ms till timeout)
2022-03-28 14:46:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99620ms till timeout)
2022-03-28 14:46:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98304ms till timeout)
2022-03-28 14:46:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96988ms till timeout)
2022-03-28 14:46:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95667ms till timeout)
2022-03-28 14:46:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94351ms till timeout)
2022-03-28 14:46:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93034ms till timeout)
2022-03-28 14:46:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91719ms till timeout)
2022-03-28 14:46:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90401ms till timeout)
2022-03-28 14:46:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89080ms till timeout)
2022-03-28 14:46:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87762ms till timeout)
2022-03-28 14:46:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86447ms till timeout)
2022-03-28 14:47:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85129ms till timeout)
2022-03-28 14:47:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83812ms till timeout)
2022-03-28 14:47:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82496ms till timeout)
2022-03-28 14:47:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81179ms till timeout)
2022-03-28 14:47:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79864ms till timeout)
2022-03-28 14:47:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78546ms till timeout)
2022-03-28 14:47:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77231ms till timeout)
2022-03-28 14:47:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75914ms till timeout)
2022-03-28 14:47:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74596ms till timeout)
2022-03-28 14:47:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73278ms till timeout)
2022-03-28 14:47:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71963ms till timeout)
2022-03-28 14:47:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70642ms till timeout)
2022-03-28 14:47:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69323ms till timeout)
2022-03-28 14:47:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68006ms till timeout)
2022-03-28 14:47:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66691ms till timeout)
2022-03-28 14:47:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65375ms till timeout)
2022-03-28 14:47:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64054ms till timeout)
2022-03-28 14:47:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62737ms till timeout)
2022-03-28 14:47:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61421ms till timeout)
2022-03-28 14:47:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60105ms till timeout)
2022-03-28 14:47:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58782ms till timeout)
2022-03-28 14:47:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57467ms till timeout)
2022-03-28 14:47:30 [ForkJoinPool-1-worker-19] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-d617cda6-kafka-clients-bwrsb log
2022-03-28 14:47:30 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:47:30 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:47:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted not ready, will try again in 5000 ms (179891ms till timeout)
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job delete-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-d617cda6-kafka-clients.
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:47:35 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:47:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129643ms till timeout)
2022-03-28 14:47:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128422ms till timeout)
2022-03-28 14:47:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127179ms till timeout)
2022-03-28 14:47:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125959ms till timeout)
2022-03-28 14:47:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124744ms till timeout)
2022-03-28 14:47:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123526ms till timeout)
2022-03-28 14:47:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122310ms till timeout)
2022-03-28 14:47:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121092ms till timeout)
2022-03-28 14:47:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119875ms till timeout)
2022-03-28 14:47:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118657ms till timeout)
2022-03-28 14:47:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117441ms till timeout)
2022-03-28 14:47:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116223ms till timeout)
2022-03-28 14:47:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115005ms till timeout)
2022-03-28 14:47:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113784ms till timeout)
2022-03-28 14:47:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112466ms till timeout)
2022-03-28 14:47:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111246ms till timeout)
2022-03-28 14:47:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110028ms till timeout)
2022-03-28 14:47:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108810ms till timeout)
2022-03-28 14:47:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107594ms till timeout)
2022-03-28 14:47:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:47:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106376ms till timeout)
2022-03-28 14:48:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105159ms till timeout)
2022-03-28 14:48:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103938ms till timeout)
2022-03-28 14:48:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102722ms till timeout)
2022-03-28 14:48:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101501ms till timeout)
2022-03-28 14:48:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100283ms till timeout)
2022-03-28 14:48:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99063ms till timeout)
2022-03-28 14:48:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97845ms till timeout)
2022-03-28 14:48:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96626ms till timeout)
2022-03-28 14:48:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95410ms till timeout)
2022-03-28 14:48:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94193ms till timeout)
2022-03-28 14:48:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92976ms till timeout)
2022-03-28 14:48:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91753ms till timeout)
2022-03-28 14:48:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90537ms till timeout)
2022-03-28 14:48:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89319ms till timeout)
2022-03-28 14:48:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88103ms till timeout)
2022-03-28 14:48:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86883ms till timeout)
2022-03-28 14:48:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85666ms till timeout)
2022-03-28 14:48:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84450ms till timeout)
2022-03-28 14:48:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83233ms till timeout)
2022-03-28 14:48:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82014ms till timeout)
2022-03-28 14:48:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80799ms till timeout)
2022-03-28 14:48:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79584ms till timeout)
2022-03-28 14:48:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78368ms till timeout)
2022-03-28 14:48:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77152ms till timeout)
2022-03-28 14:48:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75935ms till timeout)
2022-03-28 14:48:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74717ms till timeout)
2022-03-28 14:48:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73500ms till timeout)
2022-03-28 14:48:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72283ms till timeout)
2022-03-28 14:48:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71065ms till timeout)
2022-03-28 14:48:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69848ms till timeout)
2022-03-28 14:48:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68631ms till timeout)
2022-03-28 14:48:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67415ms till timeout)
2022-03-28 14:48:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66198ms till timeout)
2022-03-28 14:48:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:48:35Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:48:35Z, lastTransitionTime=2022-03-28T14:48:35Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:47:31Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:40 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:48:40 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job delete-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-d617cda6-kafka-clients.
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129786ms till timeout)
2022-03-28 14:48:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128570ms till timeout)
2022-03-28 14:48:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127349ms till timeout)
2022-03-28 14:48:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126131ms till timeout)
2022-03-28 14:48:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124915ms till timeout)
2022-03-28 14:48:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123699ms till timeout)
2022-03-28 14:48:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122482ms till timeout)
2022-03-28 14:48:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121266ms till timeout)
2022-03-28 14:48:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120047ms till timeout)
2022-03-28 14:48:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118829ms till timeout)
2022-03-28 14:48:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117611ms till timeout)
2022-03-28 14:48:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116396ms till timeout)
2022-03-28 14:48:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115178ms till timeout)
2022-03-28 14:48:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113961ms till timeout)
2022-03-28 14:48:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112743ms till timeout)
2022-03-28 14:48:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:48:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111526ms till timeout)
2022-03-28 14:49:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110307ms till timeout)
2022-03-28 14:49:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109089ms till timeout)
2022-03-28 14:49:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107871ms till timeout)
2022-03-28 14:49:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106653ms till timeout)
2022-03-28 14:49:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105435ms till timeout)
2022-03-28 14:49:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104213ms till timeout)
2022-03-28 14:49:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102996ms till timeout)
2022-03-28 14:49:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101781ms till timeout)
2022-03-28 14:49:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100563ms till timeout)
2022-03-28 14:49:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99346ms till timeout)
2022-03-28 14:49:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98128ms till timeout)
2022-03-28 14:49:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96911ms till timeout)
2022-03-28 14:49:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95693ms till timeout)
2022-03-28 14:49:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94475ms till timeout)
2022-03-28 14:49:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93257ms till timeout)
2022-03-28 14:49:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92040ms till timeout)
2022-03-28 14:49:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90819ms till timeout)
2022-03-28 14:49:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89602ms till timeout)
2022-03-28 14:49:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88380ms till timeout)
2022-03-28 14:49:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87164ms till timeout)
2022-03-28 14:49:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85947ms till timeout)
2022-03-28 14:49:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84732ms till timeout)
2022-03-28 14:49:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83514ms till timeout)
2022-03-28 14:49:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82295ms till timeout)
2022-03-28 14:49:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81074ms till timeout)
2022-03-28 14:49:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79857ms till timeout)
2022-03-28 14:49:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78637ms till timeout)
2022-03-28 14:49:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77420ms till timeout)
2022-03-28 14:49:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76203ms till timeout)
2022-03-28 14:49:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74988ms till timeout)
2022-03-28 14:49:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73507ms till timeout)
2022-03-28 14:49:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72290ms till timeout)
2022-03-28 14:49:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71072ms till timeout)
2022-03-28 14:49:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69854ms till timeout)
2022-03-28 14:49:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68637ms till timeout)
2022-03-28 14:49:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67419ms till timeout)
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:49:40Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:49:40Z, lastTransitionTime=2022-03-28T14:49:40Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:48:36Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job delete-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-d617cda6-kafka-clients.
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129785ms till timeout)
2022-03-28 14:49:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128566ms till timeout)
2022-03-28 14:49:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127350ms till timeout)
2022-03-28 14:49:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126131ms till timeout)
2022-03-28 14:49:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124914ms till timeout)
2022-03-28 14:49:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123696ms till timeout)
2022-03-28 14:49:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122404ms till timeout)
2022-03-28 14:49:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121186ms till timeout)
2022-03-28 14:49:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119970ms till timeout)
2022-03-28 14:49:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118756ms till timeout)
2022-03-28 14:49:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117539ms till timeout)
2022-03-28 14:49:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:49:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116323ms till timeout)
2022-03-28 14:50:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115106ms till timeout)
2022-03-28 14:50:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113891ms till timeout)
2022-03-28 14:50:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112676ms till timeout)
2022-03-28 14:50:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111462ms till timeout)
2022-03-28 14:50:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110244ms till timeout)
2022-03-28 14:50:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109027ms till timeout)
2022-03-28 14:50:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107806ms till timeout)
2022-03-28 14:50:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106590ms till timeout)
2022-03-28 14:50:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105370ms till timeout)
2022-03-28 14:50:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104154ms till timeout)
2022-03-28 14:50:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102936ms till timeout)
2022-03-28 14:50:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101720ms till timeout)
2022-03-28 14:50:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100502ms till timeout)
2022-03-28 14:50:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99286ms till timeout)
2022-03-28 14:50:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98069ms till timeout)
2022-03-28 14:50:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96855ms till timeout)
2022-03-28 14:50:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95636ms till timeout)
2022-03-28 14:50:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94415ms till timeout)
2022-03-28 14:50:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93195ms till timeout)
2022-03-28 14:50:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91976ms till timeout)
2022-03-28 14:50:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90758ms till timeout)
2022-03-28 14:50:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89543ms till timeout)
2022-03-28 14:50:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88325ms till timeout)
2022-03-28 14:50:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87110ms till timeout)
2022-03-28 14:50:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85892ms till timeout)
2022-03-28 14:50:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84674ms till timeout)
2022-03-28 14:50:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83457ms till timeout)
2022-03-28 14:50:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82242ms till timeout)
2022-03-28 14:50:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81024ms till timeout)
2022-03-28 14:50:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79808ms till timeout)
2022-03-28 14:50:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78590ms till timeout)
2022-03-28 14:50:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77375ms till timeout)
2022-03-28 14:50:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76157ms till timeout)
2022-03-28 14:50:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74938ms till timeout)
2022-03-28 14:50:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73720ms till timeout)
2022-03-28 14:50:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72505ms till timeout)
2022-03-28 14:50:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71286ms till timeout)
2022-03-28 14:50:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70072ms till timeout)
2022-03-28 14:50:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68853ms till timeout)
2022-03-28 14:50:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67637ms till timeout)
2022-03-28 14:50:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66419ms till timeout)
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:50:45Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:50:45Z, lastTransitionTime=2022-03-28T14:50:45Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:49:41Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job delete-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-d617cda6-kafka-clients.
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:50:50 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:50:51 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:50:51 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:50:51 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:50:51 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:50:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129785ms till timeout)
2022-03-28 14:50:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128568ms till timeout)
2022-03-28 14:50:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127349ms till timeout)
2022-03-28 14:50:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126133ms till timeout)
2022-03-28 14:50:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124918ms till timeout)
2022-03-28 14:50:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123704ms till timeout)
2022-03-28 14:50:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122487ms till timeout)
2022-03-28 14:50:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:50:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121271ms till timeout)
2022-03-28 14:51:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120055ms till timeout)
2022-03-28 14:51:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118837ms till timeout)
2022-03-28 14:51:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117620ms till timeout)
2022-03-28 14:51:04 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116401ms till timeout)
2022-03-28 14:51:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115184ms till timeout)
2022-03-28 14:51:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113968ms till timeout)
2022-03-28 14:51:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112751ms till timeout)
2022-03-28 14:51:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:09 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111534ms till timeout)
2022-03-28 14:51:10 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110318ms till timeout)
2022-03-28 14:51:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109102ms till timeout)
2022-03-28 14:51:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107885ms till timeout)
2022-03-28 14:51:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106668ms till timeout)
2022-03-28 14:51:15 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:15 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105449ms till timeout)
2022-03-28 14:51:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104231ms till timeout)
2022-03-28 14:51:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103013ms till timeout)
2022-03-28 14:51:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101795ms till timeout)
2022-03-28 14:51:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:20 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100579ms till timeout)
2022-03-28 14:51:21 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99360ms till timeout)
2022-03-28 14:51:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98140ms till timeout)
2022-03-28 14:51:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96919ms till timeout)
2022-03-28 14:51:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95701ms till timeout)
2022-03-28 14:51:26 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:26 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94484ms till timeout)
2022-03-28 14:51:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93264ms till timeout)
2022-03-28 14:51:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92048ms till timeout)
2022-03-28 14:51:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90833ms till timeout)
2022-03-28 14:51:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89617ms till timeout)
2022-03-28 14:51:32 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:32 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88400ms till timeout)
2022-03-28 14:51:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87180ms till timeout)
2022-03-28 14:51:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85960ms till timeout)
2022-03-28 14:51:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84744ms till timeout)
2022-03-28 14:51:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:37 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83528ms till timeout)
2022-03-28 14:51:38 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82310ms till timeout)
2022-03-28 14:51:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81094ms till timeout)
2022-03-28 14:51:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79876ms till timeout)
2022-03-28 14:51:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78660ms till timeout)
2022-03-28 14:51:43 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:43 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77440ms till timeout)
2022-03-28 14:51:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76224ms till timeout)
2022-03-28 14:51:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75006ms till timeout)
2022-03-28 14:51:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73786ms till timeout)
2022-03-28 14:51:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:48 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72570ms till timeout)
2022-03-28 14:51:49 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71352ms till timeout)
2022-03-28 14:51:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70116ms till timeout)
2022-03-28 14:51:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68900ms till timeout)
2022-03-28 14:51:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67681ms till timeout)
2022-03-28 14:51:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:54 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66465ms till timeout)
2022-03-28 14:51:55 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:51:50Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:51:50Z, lastTransitionTime=2022-03-28T14:51:50Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:50:46Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job delete-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] INFO  [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-d617cda6-kafka-clients.
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-d617cda6-kafka-clients will be in active state
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-d617cda6-kafka-clients to finished
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129786ms till timeout)
2022-03-28 14:51:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128569ms till timeout)
2022-03-28 14:51:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:51:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127353ms till timeout)
2022-03-28 14:52:00 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:00 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126136ms till timeout)
2022-03-28 14:52:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:01 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124919ms till timeout)
2022-03-28 14:52:02 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123699ms till timeout)
2022-03-28 14:52:03 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:03 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122482ms till timeout)
2022-03-28 14:52:05 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121265ms till timeout)
2022-03-28 14:52:06 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:06 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120047ms till timeout)
2022-03-28 14:52:07 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118827ms till timeout)
2022-03-28 14:52:08 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:08 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117609ms till timeout)
2022-03-28 14:52:09 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:10 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116391ms till timeout)
2022-03-28 14:52:11 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:11 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115175ms till timeout)
2022-03-28 14:52:12 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:12 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113958ms till timeout)
2022-03-28 14:52:13 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:13 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112740ms till timeout)
2022-03-28 14:52:14 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:14 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111523ms till timeout)
2022-03-28 14:52:16 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:16 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110300ms till timeout)
2022-03-28 14:52:17 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:17 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109083ms till timeout)
2022-03-28 14:52:18 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:18 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107866ms till timeout)
2022-03-28 14:52:19 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:19 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106650ms till timeout)
2022-03-28 14:52:20 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:21 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105434ms till timeout)
2022-03-28 14:52:22 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:22 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104218ms till timeout)
2022-03-28 14:52:23 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:23 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103002ms till timeout)
2022-03-28 14:52:24 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:24 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101785ms till timeout)
2022-03-28 14:52:25 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:25 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100570ms till timeout)
2022-03-28 14:52:27 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:27 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99352ms till timeout)
2022-03-28 14:52:28 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:28 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98135ms till timeout)
2022-03-28 14:52:29 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:29 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96918ms till timeout)
2022-03-28 14:52:30 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:30 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95702ms till timeout)
2022-03-28 14:52:31 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:31 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94485ms till timeout)
2022-03-28 14:52:33 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:33 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93269ms till timeout)
2022-03-28 14:52:34 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:34 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92051ms till timeout)
2022-03-28 14:52:35 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:35 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90830ms till timeout)
2022-03-28 14:52:36 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:36 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89614ms till timeout)
2022-03-28 14:52:37 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:38 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88397ms till timeout)
2022-03-28 14:52:39 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:39 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87181ms till timeout)
2022-03-28 14:52:40 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:40 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85963ms till timeout)
2022-03-28 14:52:41 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:41 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84746ms till timeout)
2022-03-28 14:52:42 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:42 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83529ms till timeout)
2022-03-28 14:52:44 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:44 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82313ms till timeout)
2022-03-28 14:52:45 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:45 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81094ms till timeout)
2022-03-28 14:52:46 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:46 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79875ms till timeout)
2022-03-28 14:52:47 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:47 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78658ms till timeout)
2022-03-28 14:52:48 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:49 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77442ms till timeout)
2022-03-28 14:52:50 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:50 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76227ms till timeout)
2022-03-28 14:52:51 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:51 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75009ms till timeout)
2022-03-28 14:52:52 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:52 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73793ms till timeout)
2022-03-28 14:52:53 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:53 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72579ms till timeout)
2022-03-28 14:52:54 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:55 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71362ms till timeout)
2022-03-28 14:52:56 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:56 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70145ms till timeout)
2022-03-28 14:52:57 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:57 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68927ms till timeout)
2022-03-28 14:52:58 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:58 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67710ms till timeout)
2022-03-28 14:52:59 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:52:59 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66492ms till timeout)
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T14:52:55Z, conditions=[JobCondition(lastProbeTime=2022-03-28T14:52:55Z, lastTransitionTime=2022-03-28T14:52:55Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T14:51:51Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-d617cda6-kafka-clients deletion
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-d617cda6-kafka-clients to be deleted
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [JobUtils:40] Job delete-admin-my-cluster-d617cda6-kafka-clients was deleted
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-03-28 14:53:01 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-21] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1322201233-666179901 in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-21] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1322201233-666179901
2022-03-28 14:53:01 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-d617cda6-kafka-clients in namespace throttling-quota-st
2022-03-28 14:53:01 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-d617cda6-kafka-clients
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:267] testThrottlingQuotasDeleteTopic - Notifies waiting test cases:[testConfigurationFileIsCreated, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testTopicModificationOfReplicationFactor, testMoreReplicasThanAvailableBrokers, testCreateTopicViaKafka, testCreateTopicAfterUnsupportedOperation, testTlsExternalUserWithQuotas, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 0
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [AbstractST:690] [cruisecontrol.CruiseControlConfigurationST - After All] - Clean up after test suite
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:53:02 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:353] Tearing down resources after all test
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:23] ############################################################################
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:658] ============================================================================
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c, testSendSimpleMessageTls=my-cluster-d09fdfab, testConfigurationReflection=my-cluster-9ef63e31, testUserWithNameMoreThan64Chars=my-cluster-8957492e, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6, testCapacityFile=my-cluster-d0923d27, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474, testTlsExternalUser=my-cluster-d51fc880, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf, testScramUserWithQuotas=my-cluster-f9f41b57, testTopicModificationOfReplicationFactor=my-cluster-ea63363f, testKafkaAdminTopicOperations=my-cluster-431951f6, testConfigurationFileIsCreated=my-cluster-0d1aa6d1, testCreateTopicViaKafka=my-cluster-0823f23d, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6, testUpdateUser=my-cluster-3e2cd433, testDeleteTopicEnableFalse=my-cluster-0922d459, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada, testTlsExternalUserWithQuotas=my-cluster-c096be16, testTlsUserWithQuotas=my-cluster-551ec9f6, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9, testConfigurationPerformanceOptions=my-cluster-f3d16cd7, testReceiveSimpleMessageTls=my-cluster-1d8c5392, testUserTemplate=my-cluster-e309ead0, testMoreReplicasThanAvailableBrokers=my-cluster-99352753}
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-2133939201-214791003, testThrottlingQuotasCreateAlterPartitions=my-user-1020460016-1914644203, testSendSimpleMessageTls=my-user-233981758-1294043980, testConfigurationReflection=my-user-113568301-1979160930, testUserWithNameMoreThan64Chars=my-user-501712433-1492391134, testDeployAndUnDeployCruiseControl=my-user-55391922-841210962, testCapacityFile=my-user-1855618805-723404278, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-820080083-1932015530, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2000638446-1907426278, testTlsExternalUser=my-user-2138474265-243832882, testSendSimpleMessageTlsScramSha=my-user-1770026086-750434952, testCreateTopicAfterUnsupportedOperation=my-user-2061605033-733406310, testScramUserWithQuotas=my-user-689134389-2001947957, testTopicModificationOfReplicationFactor=my-user-1731585046-1796966149, testKafkaAdminTopicOperations=my-user-1088363724-759527614, testConfigurationFileIsCreated=my-user-1519556343-1892250062, testCreateTopicViaKafka=my-user-1999863857-2146449693, testCreatingUsersWithSecretPrefix=my-user-368786116-39994006, testThrottlingQuotasDeleteTopic=my-user-1322201233-666179901, testUpdateUser=my-user-1450522583-540195646, testDeleteTopicEnableFalse=my-user-542704195-903659833, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-391311399-186390465, testTlsExternalUserWithQuotas=my-user-759590758-326116942, testTlsUserWithQuotas=my-user-823839014-420067000, testThrottlingQuotasCreateTopic=my-user-1691702842-2021683609, testSendingMessagesToNonExistingTopic=my-user-1907547474-1370079061, testConfigurationPerformanceOptions=my-user-235473874-831358867, testReceiveSimpleMessageTls=my-user-1897514695-803234736, testUserTemplate=my-user-1093693051-959868521, testMoreReplicasThanAvailableBrokers=my-user-413375925-136690822}
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-675773940-427919380, testThrottlingQuotasCreateAlterPartitions=my-topic-793443308-329931995, testSendSimpleMessageTls=my-topic-320598430-716771605, testConfigurationReflection=my-topic-1952538393-1689955181, testUserWithNameMoreThan64Chars=my-topic-1535830174-1459104465, testDeployAndUnDeployCruiseControl=my-topic-157345456-783585048, testCapacityFile=my-topic-785425830-966293436, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-933864866-1285129585, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1908879771-839772534, testTlsExternalUser=my-topic-568688711-1515642318, testSendSimpleMessageTlsScramSha=my-topic-1905303054-628697374, testCreateTopicAfterUnsupportedOperation=my-topic-571299783-1237438525, testScramUserWithQuotas=my-topic-1667902716-1547137080, testTopicModificationOfReplicationFactor=my-topic-725749039-421471693, testKafkaAdminTopicOperations=my-topic-26610931-2003761370, testConfigurationFileIsCreated=my-topic-672889728-2023534698, testCreateTopicViaKafka=my-topic-282023942-123831967, testCreatingUsersWithSecretPrefix=my-topic-850384724-1448934442, testThrottlingQuotasDeleteTopic=my-topic-486487316-227663391, testUpdateUser=my-topic-1915950564-404568310, testDeleteTopicEnableFalse=my-topic-220387463-308544022, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-2100355713-204306190, testTlsExternalUserWithQuotas=my-topic-570007939-2077615654, testTlsUserWithQuotas=my-topic-1134599290-536078433, testThrottlingQuotasCreateTopic=my-topic-419360028-2042314394, testSendingMessagesToNonExistingTopic=my-topic-1290220113-1164879046, testConfigurationPerformanceOptions=my-topic-317471915-1577545431, testReceiveSimpleMessageTls=my-topic-1978042062-1108116206, testUserTemplate=my-topic-1669116120-245826018, testMoreReplicasThanAvailableBrokers=my-topic-450528877-1105063654}
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-13dac608-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-93bfec4c-kafka-clients, testSendSimpleMessageTls=my-cluster-d09fdfab-kafka-clients, testConfigurationReflection=my-cluster-9ef63e31-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-8957492e-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-b0221df6-kafka-clients, testCapacityFile=my-cluster-d0923d27-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-58df3988-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-da099474-kafka-clients, testTlsExternalUser=my-cluster-d51fc880-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-7e69d1eb-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-3e5c1ebf-kafka-clients, testScramUserWithQuotas=my-cluster-f9f41b57-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-ea63363f-kafka-clients, testKafkaAdminTopicOperations=my-cluster-431951f6-kafka-clients, testConfigurationFileIsCreated=my-cluster-0d1aa6d1-kafka-clients, testCreateTopicViaKafka=my-cluster-0823f23d-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-67532fa3-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-d617cda6-kafka-clients, testUpdateUser=my-cluster-3e2cd433-kafka-clients, testDeleteTopicEnableFalse=my-cluster-0922d459-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-77376ada-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-c096be16-kafka-clients, testTlsUserWithQuotas=my-cluster-551ec9f6-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-1ac6bea1-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-0d9af7d9-kafka-clients, testConfigurationPerformanceOptions=my-cluster-f3d16cd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-1d8c5392-kafka-clients, testUserTemplate=my-cluster-e309ead0-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-99352753-kafka-clients}
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-0922d459-isolated in namespace topic-st
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st removal
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0922d459-isolated
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0922d459-isolated will have desired state: Ready
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0922d459-isolated will have desired state: Ready
2022-03-28 14:53:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (839794ms till timeout)
2022-03-28 14:53:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-122
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:02 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (479490ms till timeout)
2022-03-28 14:53:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-122
2022-03-28 14:53:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-124
2022-03-28 14:53:03 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-124
2022-03-28 14:53:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 14:53:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (838683ms till timeout)
2022-03-28 14:53:04 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:04 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:04 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (477992ms till timeout)
2022-03-28 14:53:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 14:53:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-102
2022-03-28 14:53:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-102
2022-03-28 14:53:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 14:53:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (837573ms till timeout)
2022-03-28 14:53:05 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 14:53:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 14:53:05 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:05 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:05 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (476512ms till timeout)
2022-03-28 14:53:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (836465ms till timeout)
2022-03-28 14:53:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 14:53:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 14:53:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 14:53:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 14:53:06 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (835355ms till timeout)
2022-03-28 14:53:07 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:07 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:07 [ForkJoinPool-1-worker-19] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (475016ms till timeout)
2022-03-28 14:53:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 14:53:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 14:53:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 14:53:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (834244ms till timeout)
2022-03-28 14:53:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 14:53:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Return code: 1
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] Error from server (NotFound): namespaces "cruise-control-configuration-st" not found
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:254] CruiseControlConfigurationST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:85] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel suite: CruiseControlConfigurationST
2022-03-28 14:53:08 [ForkJoinPool-1-worker-19] DEBUG [SuiteThreadController:89] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 2
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,501.76 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 14:53:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 14:53:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 14:53:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (833136ms till timeout)
2022-03-28 14:53:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 14:53:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 14:53:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 14:53:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 14:53:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (832027ms till timeout)
2022-03-28 14:53:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 14:53:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 14:53:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 14:53:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 14:53:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (830917ms till timeout)
2022-03-28 14:53:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 14:53:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 14:53:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 14:53:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 14:53:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (829809ms till timeout)
2022-03-28 14:53:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 14:53:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 14:53:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 14:53:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 14:53:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (828698ms till timeout)
2022-03-28 14:53:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 14:53:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 14:53:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 14:53:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 14:53:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (827590ms till timeout)
2022-03-28 14:53:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 14:53:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 14:53:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 14:53:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 14:53:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (826480ms till timeout)
2022-03-28 14:53:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 14:53:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 14:53:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 14:53:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 14:53:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (825370ms till timeout)
2022-03-28 14:53:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 14:53:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 14:53:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 14:53:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 14:53:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (824260ms till timeout)
2022-03-28 14:53:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 14:53:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 14:53:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (823152ms till timeout)
2022-03-28 14:53:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 14:53:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 14:53:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (822041ms till timeout)
2022-03-28 14:53:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (820926ms till timeout)
2022-03-28 14:53:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (819817ms till timeout)
2022-03-28 14:53:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (818709ms till timeout)
2022-03-28 14:53:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (817595ms till timeout)
2022-03-28 14:53:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 14:53:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 14:53:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (816483ms till timeout)
2022-03-28 14:53:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (815370ms till timeout)
2022-03-28 14:53:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (814262ms till timeout)
2022-03-28 14:53:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (813153ms till timeout)
2022-03-28 14:53:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (812042ms till timeout)
2022-03-28 14:53:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 14:53:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 14:53:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (810932ms till timeout)
2022-03-28 14:53:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 14:53:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 14:53:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (809823ms till timeout)
2022-03-28 14:53:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (808712ms till timeout)
2022-03-28 14:53:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (807602ms till timeout)
2022-03-28 14:53:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (806492ms till timeout)
2022-03-28 14:53:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (805382ms till timeout)
2022-03-28 14:53:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 14:53:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 14:53:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 14:53:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 14:53:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (804271ms till timeout)
2022-03-28 14:53:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 14:53:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 14:53:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (803163ms till timeout)
2022-03-28 14:53:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 14:53:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 14:53:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 14:53:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 14:53:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (802055ms till timeout)
2022-03-28 14:53:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (800944ms till timeout)
2022-03-28 14:53:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (799834ms till timeout)
2022-03-28 14:53:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (798723ms till timeout)
2022-03-28 14:53:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (797610ms till timeout)
2022-03-28 14:53:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 14:53:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 14:53:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (796501ms till timeout)
2022-03-28 14:53:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 14:53:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 14:53:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 14:53:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 14:53:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (795390ms till timeout)
2022-03-28 14:53:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 14:53:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 14:53:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 14:53:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 14:53:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (794282ms till timeout)
2022-03-28 14:53:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 14:53:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 14:53:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 14:53:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 14:53:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (793174ms till timeout)
2022-03-28 14:53:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 14:53:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 14:53:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 14:53:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 14:53:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (792061ms till timeout)
2022-03-28 14:53:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (790950ms till timeout)
2022-03-28 14:53:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (789840ms till timeout)
2022-03-28 14:53:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (788731ms till timeout)
2022-03-28 14:53:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (787621ms till timeout)
2022-03-28 14:53:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (786512ms till timeout)
2022-03-28 14:53:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 14:53:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 14:53:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (785404ms till timeout)
2022-03-28 14:53:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 14:53:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:53:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 14:53:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (784293ms till timeout)
2022-03-28 14:53:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (783181ms till timeout)
2022-03-28 14:54:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (782071ms till timeout)
2022-03-28 14:54:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (780961ms till timeout)
2022-03-28 14:54:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (779851ms till timeout)
2022-03-28 14:54:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 14:54:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 14:54:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (778742ms till timeout)
2022-03-28 14:54:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (777632ms till timeout)
2022-03-28 14:54:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (776523ms till timeout)
2022-03-28 14:54:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (775408ms till timeout)
2022-03-28 14:54:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (774300ms till timeout)
2022-03-28 14:54:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 14:54:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 14:54:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 14:54:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 14:54:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Kafka: my-cluster-0922d459-isolated will have desired state: Ready not ready, will try again in 1000 ms (773190ms till timeout)
2022-03-28 14:54:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 14:54:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 14:54:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 14:54:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 14:54:10 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:444] Kafka: my-cluster-0922d459-isolated is in desired state: Ready
2022-03-28 14:54:10 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-0922d459-isolated-kafka-clients in namespace topic-st
2022-03-28 14:54:10 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0922d459-isolated-kafka-clients
2022-03-28 14:54:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 14:54:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 14:54:11 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-0922d459-isolated-kafka-clients will be ready
2022-03-28 14:54:11 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0922d459-isolated-kafka-clients will be ready
2022-03-28 14:54:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0922d459-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (479892ms till timeout)
2022-03-28 14:54:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 14:54:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 14:54:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 14:54:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 14:54:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0922d459-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (478783ms till timeout)
2022-03-28 14:54:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 14:54:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 14:54:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 14:54:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 14:54:13 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0922d459-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (477674ms till timeout)
2022-03-28 14:54:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 14:54:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 14:54:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 14:54:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 14:54:14 [ForkJoinPool-1-worker-27] INFO  [DeploymentUtils:168] Deployment: my-cluster-0922d459-isolated-kafka-clients is ready
2022-03-28 14:54:14 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-220387463-308544022 in namespace topic-st
2022-03-28 14:54:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 14:54:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 14:54:14 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-220387463-308544022
2022-03-28 14:54:14 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-220387463-308544022 will have desired state: Ready
2022-03-28 14:54:14 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-220387463-308544022 will have desired state: Ready
2022-03-28 14:54:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] KafkaTopic: my-topic-220387463-308544022 will have desired state: Ready not ready, will try again in 1000 ms (179893ms till timeout)
2022-03-28 14:54:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 14:54:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 14:54:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 14:54:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 14:54:15 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:444] KafkaTopic: my-topic-220387463-308544022 is in desired state: Ready
2022-03-28 14:54:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 14:54:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 14:54:16 [ForkJoinPool-1-worker-27] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 14:54:16 [ForkJoinPool-1-worker-27] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14331702, which are set.
2022-03-28 14:54:16 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4e950b9a, messages=[], arguments=[--bootstrap-server, my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-220387463-308544022, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb', podNamespace='topic-st', bootstrapServer='my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-220387463-308544022', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14331702}
2022-03-28 14:54:16 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-220387463-308544022 from pod my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb
2022-03-28 14:54:16 [ForkJoinPool-1-worker-27] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb -n topic-st -- /opt/kafka/producer.sh --bootstrap-server my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-220387463-308544022 --max-messages 100
2022-03-28 14:54:16 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc exec my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb -n topic-st -- /opt/kafka/producer.sh --bootstrap-server my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-220387463-308544022 --max-messages 100
2022-03-28 14:54:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 14:54:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 14:54:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 14:54:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 14:54:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 14:54:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 14:54:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 14:54:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 14:54:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 14:54:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 14:54:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 14:54:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 14:54:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 14:54:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 14:54:20 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 14:54:20 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 14:54:20 [ForkJoinPool-1-worker-27] INFO  [TopicST:395] Deleting KafkaTopic: my-topic-220387463-308544022
2022-03-28 14:54:20 [ForkJoinPool-1-worker-27] INFO  [TopicST:397] KafkaTopic my-topic-220387463-308544022 deleted
2022-03-28 14:54:20 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Topic my-topic-220387463-308544022 has rolled
2022-03-28 14:54:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (299892ms till timeout)
2022-03-28 14:54:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 14:54:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 14:54:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 14:54:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 14:54:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (298784ms till timeout)
2022-03-28 14:54:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 14:54:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 14:54:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 14:54:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 14:54:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (297676ms till timeout)
2022-03-28 14:54:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 14:54:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 14:54:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 14:54:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 14:54:23 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (296567ms till timeout)
2022-03-28 14:54:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 14:54:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 14:54:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 14:54:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 14:54:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (295457ms till timeout)
2022-03-28 14:54:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 14:54:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 14:54:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 14:54:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 14:54:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (294348ms till timeout)
2022-03-28 14:54:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 14:54:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 14:54:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 14:54:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 14:54:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (293240ms till timeout)
2022-03-28 14:54:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 14:54:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 14:54:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 14:54:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-223
2022-03-28 14:54:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (292126ms till timeout)
2022-03-28 14:54:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-223
2022-03-28 14:54:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-224
2022-03-28 14:54:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-224
2022-03-28 14:54:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 14:54:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (291011ms till timeout)
2022-03-28 14:54:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 14:54:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 14:54:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 14:54:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 14:54:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (289902ms till timeout)
2022-03-28 14:54:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 14:54:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 14:54:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 14:54:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 14:54:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (288794ms till timeout)
2022-03-28 14:54:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 14:54:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 14:54:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 14:54:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 14:54:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (287684ms till timeout)
2022-03-28 14:54:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 14:54:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 14:54:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 14:54:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 14:54:33 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (286574ms till timeout)
2022-03-28 14:54:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 14:54:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 14:54:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 14:54:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 14:54:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (285466ms till timeout)
2022-03-28 14:54:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 14:54:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 14:54:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 14:54:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 14:54:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (284357ms till timeout)
2022-03-28 14:54:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 14:54:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 14:54:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 14:54:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 14:54:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (283246ms till timeout)
2022-03-28 14:54:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 14:54:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 14:54:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 14:54:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 14:54:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (282139ms till timeout)
2022-03-28 14:54:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 14:54:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 14:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 14:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 14:54:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (281031ms till timeout)
2022-03-28 14:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 14:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 14:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 14:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 14:54:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (279918ms till timeout)
2022-03-28 14:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 14:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 14:54:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 14:54:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 14:54:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (278810ms till timeout)
2022-03-28 14:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 14:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 14:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 14:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 14:54:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (277701ms till timeout)
2022-03-28 14:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 14:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 14:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 14:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 14:54:43 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (276593ms till timeout)
2022-03-28 14:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 14:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 14:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 14:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 14:54:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (275484ms till timeout)
2022-03-28 14:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 14:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 14:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 14:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 14:54:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (274375ms till timeout)
2022-03-28 14:54:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 14:54:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 14:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 14:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 14:54:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (273263ms till timeout)
2022-03-28 14:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 14:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 14:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 14:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 14:54:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (272155ms till timeout)
2022-03-28 14:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 14:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 14:54:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 14:54:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 14:54:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (271048ms till timeout)
2022-03-28 14:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 14:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 14:54:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (269938ms till timeout)
2022-03-28 14:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 14:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 14:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 14:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 14:54:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (268828ms till timeout)
2022-03-28 14:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 14:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 14:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 14:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 14:54:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (267720ms till timeout)
2022-03-28 14:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 14:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 14:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 14:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 14:54:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (266612ms till timeout)
2022-03-28 14:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 14:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 14:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 14:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 14:54:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (265503ms till timeout)
2022-03-28 14:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 14:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 14:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 14:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 14:54:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (264388ms till timeout)
2022-03-28 14:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 14:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 14:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 14:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 14:54:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (263276ms till timeout)
2022-03-28 14:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 14:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 14:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 14:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 14:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 14:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 14:54:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (262061ms till timeout)
2022-03-28 14:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 14:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 14:54:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (260953ms till timeout)
2022-03-28 14:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 14:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:54:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 14:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 14:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 14:55:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (259844ms till timeout)
2022-03-28 14:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 14:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 14:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 14:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 14:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 14:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 14:55:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (258730ms till timeout)
2022-03-28 14:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 14:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 14:55:02 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (257620ms till timeout)
2022-03-28 14:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 14:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 14:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 14:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 14:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 14:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 14:55:04 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (256512ms till timeout)
2022-03-28 14:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 14:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 14:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 14:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 14:55:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (255399ms till timeout)
2022-03-28 14:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 14:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 14:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 14:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 14:55:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (254290ms till timeout)
2022-03-28 14:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 14:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 14:55:07 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (253180ms till timeout)
2022-03-28 14:55:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 14:55:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 14:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 14:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 14:55:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (252046ms till timeout)
2022-03-28 14:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 14:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 14:55:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 14:55:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 14:55:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (250936ms till timeout)
2022-03-28 14:55:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 14:55:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 14:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 14:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 14:55:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (249830ms till timeout)
2022-03-28 14:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 14:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 14:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 14:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 14:55:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (248719ms till timeout)
2022-03-28 14:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 14:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 14:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 14:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 14:55:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (247610ms till timeout)
2022-03-28 14:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 14:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 14:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 14:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 14:55:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (246501ms till timeout)
2022-03-28 14:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 14:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-344
2022-03-28 14:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-344
2022-03-28 14:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 14:55:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (245390ms till timeout)
2022-03-28 14:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 14:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 14:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 14:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 14:55:16 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (244278ms till timeout)
2022-03-28 14:55:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 14:55:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 14:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 14:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 14:55:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (243168ms till timeout)
2022-03-28 14:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 14:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 14:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 14:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 14:55:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (242058ms till timeout)
2022-03-28 14:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 14:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 14:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 14:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 14:55:19 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (240951ms till timeout)
2022-03-28 14:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 14:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 14:55:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 14:55:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 14:55:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (239843ms till timeout)
2022-03-28 14:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 14:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 14:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 14:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 14:55:21 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (238737ms till timeout)
2022-03-28 14:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 14:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 14:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 14:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 14:55:22 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (237627ms till timeout)
2022-03-28 14:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 14:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 14:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 14:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 14:55:24 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (236519ms till timeout)
2022-03-28 14:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 14:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 14:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 14:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 14:55:25 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (235401ms till timeout)
2022-03-28 14:55:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 14:55:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 14:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 14:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 14:55:26 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (234292ms till timeout)
2022-03-28 14:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 14:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 14:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 14:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 14:55:27 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (233183ms till timeout)
2022-03-28 14:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 14:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 14:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 14:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 14:55:28 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (232071ms till timeout)
2022-03-28 14:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 14:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 14:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 14:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 14:55:29 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (230962ms till timeout)
2022-03-28 14:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 14:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 14:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 14:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 14:55:30 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (229853ms till timeout)
2022-03-28 14:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 14:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 14:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 14:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 14:55:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (228745ms till timeout)
2022-03-28 14:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 14:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 14:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 14:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 14:55:32 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (227635ms till timeout)
2022-03-28 14:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 14:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 14:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 14:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 14:55:34 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (226528ms till timeout)
2022-03-28 14:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 14:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 14:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 14:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 14:55:35 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (225418ms till timeout)
2022-03-28 14:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 14:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 14:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 14:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 14:55:36 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (224311ms till timeout)
2022-03-28 14:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 14:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 14:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 14:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 14:55:37 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (223203ms till timeout)
2022-03-28 14:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 14:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 14:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 14:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 14:55:38 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (222094ms till timeout)
2022-03-28 14:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 14:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 14:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 14:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 14:55:39 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (220985ms till timeout)
2022-03-28 14:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 14:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 14:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 14:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 14:55:40 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (219877ms till timeout)
2022-03-28 14:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 14:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 14:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 14:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 14:55:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (218766ms till timeout)
2022-03-28 14:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 14:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 14:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 14:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 14:55:42 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (217650ms till timeout)
2022-03-28 14:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 14:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 14:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 14:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 14:55:44 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (216541ms till timeout)
2022-03-28 14:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 14:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 14:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 14:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 14:55:45 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (215432ms till timeout)
2022-03-28 14:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 14:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 14:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 14:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 14:55:46 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (214324ms till timeout)
2022-03-28 14:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 14:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 14:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 14:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 14:55:47 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (213214ms till timeout)
2022-03-28 14:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 14:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 14:55:48 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (212106ms till timeout)
2022-03-28 14:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 14:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 14:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 14:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 14:55:49 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (210997ms till timeout)
2022-03-28 14:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 14:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 14:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 14:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 14:55:50 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (209888ms till timeout)
2022-03-28 14:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 14:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 14:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 14:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 14:55:51 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (208782ms till timeout)
2022-03-28 14:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 14:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 14:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 14:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 14:55:52 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (207674ms till timeout)
2022-03-28 14:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 14:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 14:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 14:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 14:55:53 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (206567ms till timeout)
2022-03-28 14:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 14:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 14:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 14:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 14:55:55 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (205456ms till timeout)
2022-03-28 14:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 14:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 14:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 14:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 14:55:56 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (204347ms till timeout)
2022-03-28 14:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 14:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 14:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 14:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 14:55:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (203240ms till timeout)
2022-03-28 14:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 14:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-464
2022-03-28 14:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-464
2022-03-28 14:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-465
2022-03-28 14:55:58 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (202128ms till timeout)
2022-03-28 14:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-465
2022-03-28 14:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 14:55:59 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (201018ms till timeout)
2022-03-28 14:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 14:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:55:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 14:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 14:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 14:56:00 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (199910ms till timeout)
2022-03-28 14:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 14:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 14:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 14:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 14:56:01 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Topic my-topic-220387463-308544022 has rolled not ready, will try again in 1000 ms (198802ms till timeout)
2022-03-28 14:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 14:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 14:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 14:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 14:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 14:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 14:56:02 [ForkJoinPool-1-worker-27] INFO  [TopicST:401] Wait KafkaTopic my-topic-220387463-308544022 recreation
2022-03-28 14:56:02 [ForkJoinPool-1-worker-27] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-220387463-308544022 creation 
2022-03-28 14:56:02 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-220387463-308544022
2022-03-28 14:56:03 [ForkJoinPool-1-worker-27] INFO  [TopicST:403] KafkaTopic my-topic-220387463-308544022 recreated
2022-03-28 14:56:03 [ForkJoinPool-1-worker-27] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@54639489, which are set.
2022-03-28 14:56:03 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@32cc9c44, messages=[], arguments=[--bootstrap-server, my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092, --group-id, my-consumer-group-459428287, --group-instance-id, instance1449388394, --topic, my-topic-220387463-308544022, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb', podNamespace='topic-st', bootstrapServer='my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-220387463-308544022', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-459428287', consumerInstanceId='instance1449388394', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@54639489}
2022-03-28 14:56:03 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-220387463-308544022 from pod my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb
2022-03-28 14:56:03 [ForkJoinPool-1-worker-27] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-459428287 --group-instance-id instance1449388394 --topic my-topic-220387463-308544022 --max-messages 100
2022-03-28 14:56:03 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc exec my-cluster-0922d459-isolated-kafka-clients-7f8c495f7-2h8lb -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-0922d459-isolated-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-459428287 --group-instance-id instance1449388394 --topic my-topic-220387463-308544022 --max-messages 100
2022-03-28 14:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 14:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 14:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 14:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 14:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 14:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 14:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 14:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 14:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 14:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 14:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 14:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 14:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 14:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 14:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 14:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 14:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 14:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 14:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 14:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 14:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 14:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:674] ============================================================================
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-03-28 14:56:09 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of Kafka my-cluster-0922d459-isolated in namespace topic-st
2022-03-28 14:56:09 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-220387463-308544022 in namespace topic-st
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of Deployment my-cluster-0922d459-isolated-kafka-clients in namespace topic-st
2022-03-28 14:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 14:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 14:56:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-220387463-308544022
2022-03-28 14:56:09 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0922d459-isolated
2022-03-28 14:56:09 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0922d459-isolated-kafka-clients
2022-03-28 14:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 14:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 14:56:10 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0922d459-isolated-kafka-clients not ready, will try again in 10000 ms (479491ms till timeout)
2022-03-28 14:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 14:56:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 14:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 14:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 14:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 14:56:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 14:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 14:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 14:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 14:56:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 14:56:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 14:56:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 14:56:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 14:56:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 14:56:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 14:56:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 14:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 14:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 14:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 14:56:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:690] [operators.topic.ThrottlingQuotaST - After All] - Clean up after test suite
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-03-28 14:56:16 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 14:56:17 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:quota-cluster
2022-03-28 14:56:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:quota-cluster not ready, will try again in 10000 ms (839892ms till timeout)
2022-03-28 14:56:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0922d459-isolated-kafka-clients not ready, will try again in 10000 ms (469057ms till timeout)
2022-03-28 14:56:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:56:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st removal
2022-03-28 14:56:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:56:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:56:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (479524ms till timeout)
2022-03-28 14:56:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:56:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:56:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (478037ms till timeout)
2022-03-28 14:56:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:56:31 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0922d459-isolated-kafka-clients not ready, will try again in 10000 ms (458622ms till timeout)
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 1
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Error from server (NotFound): namespaces "throttling-quota-st" not found
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:254] ThrottlingQuotaST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:85] [operators.topic.ThrottlingQuotaST] - Removing parallel suite: ThrottlingQuotaST
2022-03-28 14:56:36 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:89] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 1
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,708.96 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] INFO  [TestSeparator:30] ############################################################################
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:689] ============================================================================
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] DEBUG [AbstractST:690] [operators.topic.TopicST - After All] - Clean up after test suite
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:348] Delete all resources for TopicST
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 14:56:41 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name not ready, will try again in 10000 ms (839892ms till timeout)
2022-03-28 14:56:51 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:56:51 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Namespace topic-st removal
2022-03-28 14:56:51 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:56:57 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:56:57 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:56:57 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (474474ms till timeout)
2022-03-28 14:56:58 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:03 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:03 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:03 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (467939ms till timeout)
2022-03-28 14:57:04 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:05 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:05 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:05 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (466456ms till timeout)
2022-03-28 14:57:06 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:06 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:06 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:06 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (465004ms till timeout)
2022-03-28 14:57:07 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:08 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:08 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:08 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (463523ms till timeout)
2022-03-28 14:57:09 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:09 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:09 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:09 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (462056ms till timeout)
2022-03-28 14:57:10 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:11 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:11 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:11 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (460591ms till timeout)
2022-03-28 14:57:12 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:12 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:12 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:12 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (459110ms till timeout)
2022-03-28 14:57:13 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:14 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:14 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:14 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (457636ms till timeout)
2022-03-28 14:57:15 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:15 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:15 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:15 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (456170ms till timeout)
2022-03-28 14:57:16 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:17 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:17 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:17 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (454710ms till timeout)
2022-03-28 14:57:18 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:18 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:18 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:18 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (453259ms till timeout)
2022-03-28 14:57:19 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:20 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:20 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 0
2022-03-28 14:57:20 [ForkJoinPool-1-worker-27] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (451762ms till timeout)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Return code: 1
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] Error from server (NotFound): namespaces "topic-st" not found
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [Exec:419] ======STDERR END======
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:254] TopicST - Notifies waiting test suites:[TopicST, HttpBridgeScramShaST, CruiseControlConfigurationST, HttpBridgeTlsST, ThrottlingQuotaST, ReconciliationST, UserST] to and randomly select one to start execution
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:85] [operators.topic.TopicST] - Removing parallel suite: TopicST
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] DEBUG [SuiteThreadController:89] [operators.topic.TopicST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,754.394 s - in io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:344] ############################################################################
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:21 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 14:57:21 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 14:57:21 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 14:57:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 14:57:21 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 14:57:21 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 14:57:21 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 14:57:21 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 14:57:22 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 14:57:22 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 14:57:22 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:57:22 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:57:22 [ForkJoinPool-1-worker-31] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 14:57:22 [ForkJoinPool-1-worker-23] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 14:57:22 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 14:57:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 14:57:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 14:57:22 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 14:57:22 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 14:57:22 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:57:22 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 14:57:22 [ForkJoinPool-1-worker-23] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 14:57:22 [ForkJoinPool-1-worker-31] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 14:57:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 14:57:22 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 14:57:22 [ForkJoinPool-1-worker-27] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 14:57:23 [ForkJoinPool-1-worker-25] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 14:57:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179374ms till timeout)
2022-03-28 14:57:23 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 14:57:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 14:57:23 [ForkJoinPool-1-worker-19] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 14:57:23 [ForkJoinPool-1-worker-27] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 14:57:23 [ForkJoinPool-1-worker-25] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 14:57:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179113ms till timeout)
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] INFO  [ResourceManager:369] ############################################################################
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v138191
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v138191
2022-03-28 14:57:34 [ForkJoinPool-1-worker-19] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-410.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=138191&allowWatchBookmarks=true&watch=true...
2022-03-28 14:57:35 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 14:57:40 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 138263
2022-03-28 14:57:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 138304
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v138263 in namespace default
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@5e69280d
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1e3e4c30
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1e3e4c30
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1e3e4c30
2022-03-28 14:57:46 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 14:57:46 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 14:57:46 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 14:57:46 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 14:57:46 [main] INFO  [TestExecutionListener:44] =======================================================================
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 138305
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 30, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ systemtest ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ systemtest ---
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 14:57:46 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
[INFO] No dependency problems found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift . SUCCESS [  3.851 s]
[INFO] test ............................................... SUCCESS [  5.550 s]
[INFO] crd-annotations .................................... SUCCESS [  4.151 s]
[INFO] crd-generator ...................................... SUCCESS [  6.629 s]
[INFO] api ................................................ SUCCESS [ 28.986 s]
[INFO] mockkube ........................................... SUCCESS [  3.978 s]
[INFO] config-model ....................................... SUCCESS [  3.338 s]
[INFO] certificate-manager ................................ SUCCESS [  3.841 s]
[INFO] operator-common .................................... SUCCESS [  7.421 s]
[INFO] systemtest ......................................... SUCCESS [29:49 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  30:57 min
[INFO] Finished at: 2022-03-28T10:57:46-04:00
[INFO] ------------------------------------------------------------------------
