[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, generex-1.0.2.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 61 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] kubernetes-model-storageclass-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-apiextensions-5.12.0.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-discovery-5.12.0.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-metrics-5.12.0.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-flowcontrol-5.12.0.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-events-5.12.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] automaton-1.11-8.jar, crd-generator-0.29.0-SNAPSHOT.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 114 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-coordination-5.12.0.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] zjsonpatch-0.3.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] kubernetes-model-admissionregistration-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-certificates-5.12.0.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-datatype-jsr310-2.13.1.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] crd-annotations-0.29.0-SNAPSHOT.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] jackson-databind-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 700 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 690 more...
[[1;33mWARNING[m] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[[1;33mWARNING[m]   - 7 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.36.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-node-5.12.0.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, snakeyaml-1.27.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] kubernetes-model-policy-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 32 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/ec2-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-31T06-10-03_653-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-03-31 06:10:21 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.MetricsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:219] Used environment variables:
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:220] CONFIG: /home/ec2-user/strimzi-kafka-operator/systemtest/config.json
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] SKIP_TEARDOWN: false
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] LB_FINALIZERS: false
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] DOCKER_ORG: strimzi
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_LOG_DIR: /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] DOCKER_TAG: latest
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-31 06:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [BeforeAllOnce:51] ============================================================================
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [BeforeAllOnce:52] [io.strimzi.systemtest.cruisecontrol.CruiseControlApiST - Before Suite] - Setup Suite environment
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeCluster:71] Cluster minikube is installed
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - minikube status
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: minikube status
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeCluster:73] Cluster minikube is running
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:10:22 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace default get Namespace infra-namespace -o json
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace default get Namespace infra-namespace -o json
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:23Z",
        "name": "infra-namespace",
        "resourceVersion": "131960",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "5b889d90-f976-4b45-b127-06164831ce52"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:10:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:10:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479994ms till timeout)
2022-03-31 06:10:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478990ms till timeout)
2022-03-31 06:10:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477983ms till timeout)
2022-03-31 06:10:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476979ms till timeout)
2022-03-31 06:10:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475973ms till timeout)
2022-03-31 06:10:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474968ms till timeout)
2022-03-31 06:10:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473964ms till timeout)
2022-03-31 06:10:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472958ms till timeout)
2022-03-31 06:10:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471953ms till timeout)
2022-03-31 06:10:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470947ms till timeout)
2022-03-31 06:10:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469936ms till timeout)
2022-03-31 06:10:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468929ms till timeout)
2022-03-31 06:10:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467921ms till timeout)
2022-03-31 06:10:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466917ms till timeout)
2022-03-31 06:10:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465911ms till timeout)
2022-03-31 06:10:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464907ms till timeout)
2022-03-31 06:10:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463902ms till timeout)
2022-03-31 06:10:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462897ms till timeout)
2022-03-31 06:10:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461893ms till timeout)
2022-03-31 06:10:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460888ms till timeout)
2022-03-31 06:10:45 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:10:45 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:10:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:10:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599981ms till timeout)
2022-03-31 06:10:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598976ms till timeout)
2022-03-31 06:10:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597970ms till timeout)
2022-03-31 06:10:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596964ms till timeout)
2022-03-31 06:10:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595959ms till timeout)
2022-03-31 06:10:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594954ms till timeout)
2022-03-31 06:10:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593949ms till timeout)
2022-03-31 06:10:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592943ms till timeout)
2022-03-31 06:10:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591937ms till timeout)
2022-03-31 06:10:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590932ms till timeout)
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-rr22j not ready: strimzi-cluster-operator)
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-rr22j are ready
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlApiST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [metrics.MetricsIsolatedST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:667] [kafka.listeners.ListenersST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlApiST] - Adding parallel suite: CruiseControlApiST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:69] [kafka.listeners.ListenersST] - Adding parallel suite: ListenersST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlST] - Adding parallel suite: CruiseControlST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 1
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlST] - Parallel suites count: 3
2022-03-31 06:10:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 4
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:73] [kafka.listeners.ListenersST] - Parallel suites count: 2
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:184] CruiseControlST suite now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:184] CruiseControlApiST suite now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:184] ListenersST suite now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `ListenersST` creates these additional namespaces:[listeners-st]
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlST` creates these additional namespaces:[cruise-control-st]
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlApiST` creates these additional namespaces:[cruise-control-api-st]
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: user-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-api-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: listeners-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o json
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:55Z",
        "name": "listeners-st",
        "resourceVersion": "132064",
        "selfLink": "/api/v1/namespaces/listeners-st",
        "uid": "c62cfd47-c5e1-4590-b5b8-7c5f70ef0b51"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: listeners-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=listeners-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:667] [rollingupdate.RollingUpdateST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:667] [security.SecurityST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:69] [rollingupdate.RollingUpdateST] - Adding parallel suite: RollingUpdateST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:69] [security.SecurityST] - Adding parallel suite: SecurityST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:73] [rollingupdate.RollingUpdateST] - Parallel suites count: 6
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:55Z",
        "name": "cruise-control-st",
        "resourceVersion": "132063",
        "selfLink": "/api/v1/namespaces/cruise-control-st",
        "uid": "f609aeff-b505-472f-8ba6-926dd4b61999"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:73] [security.SecurityST] - Parallel suites count: 7
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:159] [RollingUpdateST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:10:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:159] [SecurityST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: listeners-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:55Z",
        "name": "cruise-control-api-st",
        "resourceVersion": "132061",
        "selfLink": "/api/v1/namespaces/cruise-control-api-st",
        "uid": "e435f8f6-b451-4af1-ae53-f880dec0f431"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-api-st
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-api-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-api-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:667] [specific.SpecificIsolatedST - Before All] - Setup test suite environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:10:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:55 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-STARTED
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-STARTED
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 1
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequestsWithSecurityDisabled test now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:55 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesTlsScramSha
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:55Z",
        "name": "http-bridge-tls-st",
        "resourceVersion": "132062",
        "selfLink": "/api/v1/namespaces/http-bridge-tls-st",
        "uid": "d19ecf3a-7ae3-4dde-b7f7-1651c1a587ba"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 2
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesTlsScramSha test now can proceed its execution
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:55Z",
        "name": "user-st",
        "resourceVersion": "132060",
        "selfLink": "/api/v1/namespaces/user-st",
        "uid": "0de4d23a-37de-4270-921e-c0fa85ab3ab0"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: user-st
2022-03-31 06:10:55 [ForkJoinPool-3-worker-7] [32mINFO [m [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:55 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace namespace-0 -o json
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace namespace-0 -o json
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:56Z",
        "name": "namespace-0",
        "resourceVersion": "132080",
        "selfLink": "/api/v1/namespaces/namespace-0",
        "uid": "30d9a234-cd65-44e5-bc8f-d548af32b991"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendMessagesTlsScramSha=my-cluster-7009fdd7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testSendMessagesTlsScramSha
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:10:56Z",
        "name": "namespace-1",
        "resourceVersion": "132084",
        "selfLink": "/api/v1/namespaces/namespace-1",
        "uid": "5c6ffb55-cb21-4292-b646-a05c8c6826a8"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka cruise-control-api-cluster-name in namespace namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-03-31 06:10:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7009fdd7 in namespace namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-31 06:10:56 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-31 06:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-31 06:10:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7009fdd7
2022-03-31 06:10:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7009fdd7 will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7009fdd7 will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839994ms till timeout)
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (839994ms till timeout)
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-31 06:10:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839993ms till timeout)
2022-03-31 06:10:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1319987ms till timeout)
2022-03-31 06:10:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838989ms till timeout)
2022-03-31 06:10:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (838989ms till timeout)
2022-03-31 06:10:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838988ms till timeout)
2022-03-31 06:10:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1318983ms till timeout)
2022-03-31 06:10:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837984ms till timeout)
2022-03-31 06:10:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837983ms till timeout)
2022-03-31 06:10:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (837984ms till timeout)
2022-03-31 06:10:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1317980ms till timeout)
2022-03-31 06:10:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836980ms till timeout)
2022-03-31 06:10:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (836980ms till timeout)
2022-03-31 06:10:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836978ms till timeout)
2022-03-31 06:10:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1316977ms till timeout)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835975ms till timeout)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835974ms till timeout)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (835974ms till timeout)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1315973ms till timeout)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834971ms till timeout)
2022-03-31 06:11:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834969ms till timeout)
2022-03-31 06:11:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (834970ms till timeout)
2022-03-31 06:11:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1314968ms till timeout)
2022-03-31 06:11:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833967ms till timeout)
2022-03-31 06:11:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833965ms till timeout)
2022-03-31 06:11:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (833966ms till timeout)
2022-03-31 06:11:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1313965ms till timeout)
2022-03-31 06:11:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832962ms till timeout)
2022-03-31 06:11:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (832961ms till timeout)
2022-03-31 06:11:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832960ms till timeout)
2022-03-31 06:11:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1312962ms till timeout)
2022-03-31 06:11:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831957ms till timeout)
2022-03-31 06:11:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831956ms till timeout)
2022-03-31 06:11:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (831957ms till timeout)
2022-03-31 06:11:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1311959ms till timeout)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830954ms till timeout)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830952ms till timeout)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (830953ms till timeout)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1310956ms till timeout)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829949ms till timeout)
2022-03-31 06:11:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (829947ms till timeout)
2022-03-31 06:11:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829946ms till timeout)
2022-03-31 06:11:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1309951ms till timeout)
2022-03-31 06:11:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828944ms till timeout)
2022-03-31 06:11:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828942ms till timeout)
2022-03-31 06:11:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (828941ms till timeout)
2022-03-31 06:11:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1308946ms till timeout)
2022-03-31 06:11:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827940ms till timeout)
2022-03-31 06:11:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827938ms till timeout)
2022-03-31 06:11:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (827936ms till timeout)
2022-03-31 06:11:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1307942ms till timeout)
2022-03-31 06:11:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826936ms till timeout)
2022-03-31 06:11:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826935ms till timeout)
2022-03-31 06:11:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (826933ms till timeout)
2022-03-31 06:11:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1306936ms till timeout)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825929ms till timeout)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (825924ms till timeout)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825922ms till timeout)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1305928ms till timeout)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824924ms till timeout)
2022-03-31 06:11:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (824918ms till timeout)
2022-03-31 06:11:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1304922ms till timeout)
2022-03-31 06:11:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824915ms till timeout)
2022-03-31 06:11:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823919ms till timeout)
2022-03-31 06:11:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (823915ms till timeout)
2022-03-31 06:11:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823909ms till timeout)
2022-03-31 06:11:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1303915ms till timeout)
2022-03-31 06:11:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822913ms till timeout)
2022-03-31 06:11:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (822892ms till timeout)
2022-03-31 06:11:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1302890ms till timeout)
2022-03-31 06:11:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822883ms till timeout)
2022-03-31 06:11:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821907ms till timeout)
2022-03-31 06:11:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (821875ms till timeout)
2022-03-31 06:11:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1301879ms till timeout)
2022-03-31 06:11:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821871ms till timeout)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820898ms till timeout)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (820866ms till timeout)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820860ms till timeout)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1300866ms till timeout)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819893ms till timeout)
2022-03-31 06:11:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (819862ms till timeout)
2022-03-31 06:11:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819856ms till timeout)
2022-03-31 06:11:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1299861ms till timeout)
2022-03-31 06:11:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818889ms till timeout)
2022-03-31 06:11:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (818858ms till timeout)
2022-03-31 06:11:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818853ms till timeout)
2022-03-31 06:11:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1298858ms till timeout)
2022-03-31 06:11:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817884ms till timeout)
2022-03-31 06:11:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (817854ms till timeout)
2022-03-31 06:11:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817849ms till timeout)
2022-03-31 06:11:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1297854ms till timeout)
2022-03-31 06:11:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816879ms till timeout)
2022-03-31 06:11:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (816844ms till timeout)
2022-03-31 06:11:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816841ms till timeout)
2022-03-31 06:11:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1296848ms till timeout)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815868ms till timeout)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (815839ms till timeout)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815835ms till timeout)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1295841ms till timeout)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814863ms till timeout)
2022-03-31 06:11:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (814824ms till timeout)
2022-03-31 06:11:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1294827ms till timeout)
2022-03-31 06:11:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814815ms till timeout)
2022-03-31 06:11:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813858ms till timeout)
2022-03-31 06:11:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (813809ms till timeout)
2022-03-31 06:11:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1293803ms till timeout)
2022-03-31 06:11:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813795ms till timeout)
2022-03-31 06:11:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812853ms till timeout)
2022-03-31 06:11:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (812805ms till timeout)
2022-03-31 06:11:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1292799ms till timeout)
2022-03-31 06:11:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812790ms till timeout)
2022-03-31 06:11:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811849ms till timeout)
2022-03-31 06:11:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (811802ms till timeout)
2022-03-31 06:11:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1291796ms till timeout)
2022-03-31 06:11:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811787ms till timeout)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810845ms till timeout)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (810799ms till timeout)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1290793ms till timeout)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810784ms till timeout)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809841ms till timeout)
2022-03-31 06:11:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (809796ms till timeout)
2022-03-31 06:11:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1289790ms till timeout)
2022-03-31 06:11:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809781ms till timeout)
2022-03-31 06:11:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808837ms till timeout)
2022-03-31 06:11:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (808792ms till timeout)
2022-03-31 06:11:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1288787ms till timeout)
2022-03-31 06:11:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808778ms till timeout)
2022-03-31 06:11:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807833ms till timeout)
2022-03-31 06:11:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (807789ms till timeout)
2022-03-31 06:11:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1287784ms till timeout)
2022-03-31 06:11:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807775ms till timeout)
2022-03-31 06:11:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806829ms till timeout)
2022-03-31 06:11:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (806786ms till timeout)
2022-03-31 06:11:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1286781ms till timeout)
2022-03-31 06:11:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806772ms till timeout)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805824ms till timeout)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (805783ms till timeout)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1285778ms till timeout)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805767ms till timeout)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804819ms till timeout)
2022-03-31 06:11:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (804779ms till timeout)
2022-03-31 06:11:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1284775ms till timeout)
2022-03-31 06:11:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804764ms till timeout)
2022-03-31 06:11:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803815ms till timeout)
2022-03-31 06:11:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (803776ms till timeout)
2022-03-31 06:11:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1283772ms till timeout)
2022-03-31 06:11:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803761ms till timeout)
2022-03-31 06:11:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802811ms till timeout)
2022-03-31 06:11:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (802773ms till timeout)
2022-03-31 06:11:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1282769ms till timeout)
2022-03-31 06:11:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802758ms till timeout)
2022-03-31 06:11:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801807ms till timeout)
2022-03-31 06:11:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (801770ms till timeout)
2022-03-31 06:11:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1281766ms till timeout)
2022-03-31 06:11:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801755ms till timeout)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800803ms till timeout)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (800766ms till timeout)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1280763ms till timeout)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800752ms till timeout)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799799ms till timeout)
2022-03-31 06:11:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (799763ms till timeout)
2022-03-31 06:11:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1279756ms till timeout)
2022-03-31 06:11:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799747ms till timeout)
2022-03-31 06:11:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798795ms till timeout)
2022-03-31 06:11:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (798759ms till timeout)
2022-03-31 06:11:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1278753ms till timeout)
2022-03-31 06:11:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798744ms till timeout)
2022-03-31 06:11:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797791ms till timeout)
2022-03-31 06:11:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (797756ms till timeout)
2022-03-31 06:11:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1277750ms till timeout)
2022-03-31 06:11:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797741ms till timeout)
2022-03-31 06:11:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796787ms till timeout)
2022-03-31 06:11:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (796752ms till timeout)
2022-03-31 06:11:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1276747ms till timeout)
2022-03-31 06:11:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796738ms till timeout)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795783ms till timeout)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (795749ms till timeout)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1275744ms till timeout)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795735ms till timeout)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794775ms till timeout)
2022-03-31 06:11:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (794746ms till timeout)
2022-03-31 06:11:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1274741ms till timeout)
2022-03-31 06:11:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794732ms till timeout)
2022-03-31 06:11:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793770ms till timeout)
2022-03-31 06:11:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (793742ms till timeout)
2022-03-31 06:11:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1273738ms till timeout)
2022-03-31 06:11:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793729ms till timeout)
2022-03-31 06:11:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792766ms till timeout)
2022-03-31 06:11:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (792739ms till timeout)
2022-03-31 06:11:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1272735ms till timeout)
2022-03-31 06:11:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792726ms till timeout)
2022-03-31 06:11:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791762ms till timeout)
2022-03-31 06:11:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (791735ms till timeout)
2022-03-31 06:11:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1271732ms till timeout)
2022-03-31 06:11:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791723ms till timeout)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790758ms till timeout)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (790732ms till timeout)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1270729ms till timeout)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790721ms till timeout)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789754ms till timeout)
2022-03-31 06:11:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (789729ms till timeout)
2022-03-31 06:11:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1269725ms till timeout)
2022-03-31 06:11:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789718ms till timeout)
2022-03-31 06:11:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788750ms till timeout)
2022-03-31 06:11:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (788726ms till timeout)
2022-03-31 06:11:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1268723ms till timeout)
2022-03-31 06:11:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788714ms till timeout)
2022-03-31 06:11:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787746ms till timeout)
2022-03-31 06:11:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (787723ms till timeout)
2022-03-31 06:11:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1267720ms till timeout)
2022-03-31 06:11:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787711ms till timeout)
2022-03-31 06:11:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786738ms till timeout)
2022-03-31 06:11:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (786719ms till timeout)
2022-03-31 06:11:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1266716ms till timeout)
2022-03-31 06:11:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786708ms till timeout)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785734ms till timeout)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (785716ms till timeout)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1265713ms till timeout)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785704ms till timeout)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784729ms till timeout)
2022-03-31 06:11:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (784712ms till timeout)
2022-03-31 06:11:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1264707ms till timeout)
2022-03-31 06:11:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784699ms till timeout)
2022-03-31 06:11:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783721ms till timeout)
2022-03-31 06:11:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (783705ms till timeout)
2022-03-31 06:11:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1263703ms till timeout)
2022-03-31 06:11:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783695ms till timeout)
2022-03-31 06:11:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782717ms till timeout)
2022-03-31 06:11:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (782702ms till timeout)
2022-03-31 06:11:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1262700ms till timeout)
2022-03-31 06:11:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782692ms till timeout)
2022-03-31 06:11:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781713ms till timeout)
2022-03-31 06:11:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (781698ms till timeout)
2022-03-31 06:11:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1261697ms till timeout)
2022-03-31 06:11:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781687ms till timeout)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780709ms till timeout)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (780695ms till timeout)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1260695ms till timeout)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780684ms till timeout)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:11:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:11:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:11:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779705ms till timeout)
2022-03-31 06:11:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (779692ms till timeout)
2022-03-31 06:11:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1259691ms till timeout)
2022-03-31 06:11:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779682ms till timeout)
2022-03-31 06:11:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778701ms till timeout)
2022-03-31 06:11:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (778689ms till timeout)
2022-03-31 06:11:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1258689ms till timeout)
2022-03-31 06:11:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778678ms till timeout)
2022-03-31 06:11:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777697ms till timeout)
2022-03-31 06:11:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (777686ms till timeout)
2022-03-31 06:11:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1257686ms till timeout)
2022-03-31 06:11:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777675ms till timeout)
2022-03-31 06:11:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776693ms till timeout)
2022-03-31 06:11:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (776683ms till timeout)
2022-03-31 06:11:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1256683ms till timeout)
2022-03-31 06:11:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776673ms till timeout)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775689ms till timeout)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (775679ms till timeout)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1255680ms till timeout)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775669ms till timeout)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774685ms till timeout)
2022-03-31 06:12:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (774676ms till timeout)
2022-03-31 06:12:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1254677ms till timeout)
2022-03-31 06:12:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774667ms till timeout)
2022-03-31 06:12:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773680ms till timeout)
2022-03-31 06:12:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (773673ms till timeout)
2022-03-31 06:12:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1253674ms till timeout)
2022-03-31 06:12:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773663ms till timeout)
2022-03-31 06:12:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772677ms till timeout)
2022-03-31 06:12:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (772670ms till timeout)
2022-03-31 06:12:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1252672ms till timeout)
2022-03-31 06:12:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772661ms till timeout)
2022-03-31 06:12:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771672ms till timeout)
2022-03-31 06:12:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (771667ms till timeout)
2022-03-31 06:12:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1251669ms till timeout)
2022-03-31 06:12:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771658ms till timeout)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770667ms till timeout)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (770665ms till timeout)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1250666ms till timeout)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770655ms till timeout)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769664ms till timeout)
2022-03-31 06:12:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (769662ms till timeout)
2022-03-31 06:12:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1249663ms till timeout)
2022-03-31 06:12:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769652ms till timeout)
2022-03-31 06:12:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768659ms till timeout)
2022-03-31 06:12:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (768659ms till timeout)
2022-03-31 06:12:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1248660ms till timeout)
2022-03-31 06:12:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768650ms till timeout)
2022-03-31 06:12:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767656ms till timeout)
2022-03-31 06:12:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (767655ms till timeout)
2022-03-31 06:12:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1247657ms till timeout)
2022-03-31 06:12:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767647ms till timeout)
2022-03-31 06:12:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766652ms till timeout)
2022-03-31 06:12:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (766651ms till timeout)
2022-03-31 06:12:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1246654ms till timeout)
2022-03-31 06:12:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766645ms till timeout)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765648ms till timeout)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (765647ms till timeout)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1245651ms till timeout)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765642ms till timeout)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764644ms till timeout)
2022-03-31 06:12:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (764644ms till timeout)
2022-03-31 06:12:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1244648ms till timeout)
2022-03-31 06:12:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764639ms till timeout)
2022-03-31 06:12:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763637ms till timeout)
2022-03-31 06:12:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (763636ms till timeout)
2022-03-31 06:12:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763635ms till timeout)
2022-03-31 06:12:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1243641ms till timeout)
2022-03-31 06:12:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762633ms till timeout)
2022-03-31 06:12:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (762629ms till timeout)
2022-03-31 06:12:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1242634ms till timeout)
2022-03-31 06:12:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762627ms till timeout)
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-31 06:12:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (761625ms till timeout)
2022-03-31 06:12:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1241629ms till timeout)
2022-03-31 06:12:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761621ms till timeout)
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-182343961-967786628 in namespace http-bridge-tls-st
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-182343961-967786628
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-182343961-967786628 will have desired state: Ready
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-182343961-967786628 will have desired state: Ready
2022-03-31 06:12:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-182343961-967786628 will have desired state: Ready not ready, will try again in 1000 ms (179995ms till timeout)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (760620ms till timeout)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760614ms till timeout)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1240621ms till timeout)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-182343961-967786628 will have desired state: Ready not ready, will try again in 1000 ms (178988ms till timeout)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (759616ms till timeout)
2022-03-31 06:12:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1239614ms till timeout)
2022-03-31 06:12:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759602ms till timeout)
2022-03-31 06:12:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-182343961-967786628 is in desired state: Ready
2022-03-31 06:12:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-31 06:12:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-31 06:12:16 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-31 06:12:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-31 06:12:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479994ms till timeout)
2022-03-31 06:12:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (758610ms till timeout)
2022-03-31 06:12:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1238604ms till timeout)
2022-03-31 06:12:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758594ms till timeout)
2022-03-31 06:12:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478986ms till timeout)
2022-03-31 06:12:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (757597ms till timeout)
2022-03-31 06:12:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1237598ms till timeout)
2022-03-31 06:12:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757588ms till timeout)
2022-03-31 06:12:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (477981ms till timeout)
2022-03-31 06:12:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1236593ms till timeout)
2022-03-31 06:12:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (756587ms till timeout)
2022-03-31 06:12:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756585ms till timeout)
2022-03-31 06:12:19 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-31 06:12:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-31 06:12:20 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-31 06:12:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-31 06:12:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-31 06:12:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-31 06:12:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-31 06:12:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (755573ms till timeout)
2022-03-31 06:12:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755569ms till timeout)
2022-03-31 06:12:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1235575ms till timeout)
2022-03-31 06:12:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478987ms till timeout)
2022-03-31 06:12:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (754567ms till timeout)
2022-03-31 06:12:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754563ms till timeout)
2022-03-31 06:12:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1234569ms till timeout)
2022-03-31 06:12:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477977ms till timeout)
2022-03-31 06:12:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (753562ms till timeout)
2022-03-31 06:12:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753548ms till timeout)
2022-03-31 06:12:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1233554ms till timeout)
2022-03-31 06:12:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476972ms till timeout)
2022-03-31 06:12:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (752556ms till timeout)
2022-03-31 06:12:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752544ms till timeout)
2022-03-31 06:12:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1232549ms till timeout)
2022-03-31 06:12:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475961ms till timeout)
2022-03-31 06:12:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (751550ms till timeout)
2022-03-31 06:12:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751536ms till timeout)
2022-03-31 06:12:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1231538ms till timeout)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474957ms till timeout)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (750543ms till timeout)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750531ms till timeout)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1230532ms till timeout)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473953ms till timeout)
2022-03-31 06:12:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (749539ms till timeout)
2022-03-31 06:12:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749519ms till timeout)
2022-03-31 06:12:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1229525ms till timeout)
2022-03-31 06:12:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472940ms till timeout)
2022-03-31 06:12:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (748535ms till timeout)
2022-03-31 06:12:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748516ms till timeout)
2022-03-31 06:12:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1228520ms till timeout)
2022-03-31 06:12:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471927ms till timeout)
2022-03-31 06:12:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (747531ms till timeout)
2022-03-31 06:12:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747512ms till timeout)
2022-03-31 06:12:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1227517ms till timeout)
2022-03-31 06:12:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470923ms till timeout)
2022-03-31 06:12:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (746527ms till timeout)
2022-03-31 06:12:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746509ms till timeout)
2022-03-31 06:12:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1226515ms till timeout)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469920ms till timeout)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (745523ms till timeout)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745507ms till timeout)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1225512ms till timeout)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468915ms till timeout)
2022-03-31 06:12:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (744519ms till timeout)
2022-03-31 06:12:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744504ms till timeout)
2022-03-31 06:12:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1224509ms till timeout)
2022-03-31 06:12:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467909ms till timeout)
2022-03-31 06:12:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (743516ms till timeout)
2022-03-31 06:12:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743501ms till timeout)
2022-03-31 06:12:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1223506ms till timeout)
2022-03-31 06:12:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466905ms till timeout)
2022-03-31 06:12:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (742513ms till timeout)
2022-03-31 06:12:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742498ms till timeout)
2022-03-31 06:12:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1222503ms till timeout)
2022-03-31 06:12:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465901ms till timeout)
2022-03-31 06:12:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (741508ms till timeout)
2022-03-31 06:12:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741488ms till timeout)
2022-03-31 06:12:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1221494ms till timeout)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464897ms till timeout)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (740504ms till timeout)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740485ms till timeout)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1220489ms till timeout)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463894ms till timeout)
2022-03-31 06:12:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (739499ms till timeout)
2022-03-31 06:12:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739480ms till timeout)
2022-03-31 06:12:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1219486ms till timeout)
2022-03-31 06:12:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462891ms till timeout)
2022-03-31 06:12:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (738493ms till timeout)
2022-03-31 06:12:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738476ms till timeout)
2022-03-31 06:12:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1218480ms till timeout)
2022-03-31 06:12:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461887ms till timeout)
2022-03-31 06:12:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (737489ms till timeout)
2022-03-31 06:12:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737472ms till timeout)
2022-03-31 06:12:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1217478ms till timeout)
2022-03-31 06:12:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460873ms till timeout)
2022-03-31 06:12:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (736483ms till timeout)
2022-03-31 06:12:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736466ms till timeout)
2022-03-31 06:12:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1216471ms till timeout)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459868ms till timeout)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (735475ms till timeout)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735461ms till timeout)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1215466ms till timeout)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458863ms till timeout)
2022-03-31 06:12:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (734468ms till timeout)
2022-03-31 06:12:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734456ms till timeout)
2022-03-31 06:12:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1214461ms till timeout)
2022-03-31 06:12:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (457858ms till timeout)
2022-03-31 06:12:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (733459ms till timeout)
2022-03-31 06:12:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733451ms till timeout)
2022-03-31 06:12:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1213456ms till timeout)
2022-03-31 06:12:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (456853ms till timeout)
2022-03-31 06:12:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (732454ms till timeout)
2022-03-31 06:12:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732448ms till timeout)
2022-03-31 06:12:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1212448ms till timeout)
2022-03-31 06:12:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (455848ms till timeout)
2022-03-31 06:12:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (731447ms till timeout)
2022-03-31 06:12:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731443ms till timeout)
2022-03-31 06:12:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1211442ms till timeout)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (454843ms till timeout)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (730444ms till timeout)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730441ms till timeout)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1210439ms till timeout)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 3
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-717762053-1645501097 in namespace http-bridge-tls-st
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-717762053-1645501097
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-717762053-1645501097 will have desired state: Ready
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-717762053-1645501097 will have desired state: Ready
2022-03-31 06:12:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-717762053-1645501097 will have desired state: Ready not ready, will try again in 1000 ms (179992ms till timeout)
2022-03-31 06:12:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (729440ms till timeout)
2022-03-31 06:12:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729436ms till timeout)
2022-03-31 06:12:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1209436ms till timeout)
2022-03-31 06:12:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-717762053-1645501097 is in desired state: Ready
2022-03-31 06:12:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1239134595 in namespace http-bridge-tls-st
2022-03-31 06:12:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1239134595
2022-03-31 06:12:47 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: producer-1239134595 will be in active state
2022-03-31 06:12:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 06:12:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-31 06:12:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (728436ms till timeout)
2022-03-31 06:12:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728433ms till timeout)
2022-03-31 06:12:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1208434ms till timeout)
2022-03-31 06:12:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-1239134595 to finished
2022-03-31 06:12:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-31 06:12:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-1239134595 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219995ms till timeout)
2022-03-31 06:12:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (727432ms till timeout)
2022-03-31 06:12:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727428ms till timeout)
2022-03-31 06:12:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1207429ms till timeout)
2022-03-31 06:12:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-1239134595 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218989ms till timeout)
2022-03-31 06:12:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (726427ms till timeout)
2022-03-31 06:12:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726424ms till timeout)
2022-03-31 06:12:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1206427ms till timeout)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-1239134595 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217980ms till timeout)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (725422ms till timeout)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725420ms till timeout)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1205423ms till timeout)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-1239134595 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216974ms till timeout)
2022-03-31 06:12:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (724419ms till timeout)
2022-03-31 06:12:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724417ms till timeout)
2022-03-31 06:12:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1204421ms till timeout)
2022-03-31 06:12:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-1239134595 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215968ms till timeout)
2022-03-31 06:12:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (723415ms till timeout)
2022-03-31 06:12:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723413ms till timeout)
2022-03-31 06:12:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1203418ms till timeout)
2022-03-31 06:12:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-1239134595 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-31T06:12:52Z, conditions=[JobCondition(lastProbeTime=2022-03-31T06:12:52Z, lastTransitionTime=2022-03-31T06:12:52Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-31T06:12:47Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1239134595 deletion
2022-03-31 06:12:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-1239134595 to be deleted
2022-03-31 06:12:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-1239134595 to be deleted not ready, will try again in 5000 ms (179992ms till timeout)
2022-03-31 06:12:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (722411ms till timeout)
2022-03-31 06:12:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722407ms till timeout)
2022-03-31 06:12:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1202414ms till timeout)
2022-03-31 06:12:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (721407ms till timeout)
2022-03-31 06:12:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721404ms till timeout)
2022-03-31 06:12:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1201411ms till timeout)
2022-03-31 06:12:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (720403ms till timeout)
2022-03-31 06:12:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1200407ms till timeout)
2022-03-31 06:12:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720400ms till timeout)
2022-03-31 06:12:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:12:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:12:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:12:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (719399ms till timeout)
2022-03-31 06:12:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719397ms till timeout)
2022-03-31 06:12:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1199403ms till timeout)
2022-03-31 06:12:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7009fdd7 will have desired state: Ready not ready, will try again in 1000 ms (718396ms till timeout)
2022-03-31 06:12:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1198400ms till timeout)
2022-03-31 06:12:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718393ms till timeout)
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job producer-1239134595 was deleted
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1410293327 in namespace http-bridge-tls-st
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1410293327
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1410293327 will be in active state
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 06:12:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-31 06:12:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7009fdd7 is in desired state: Ready
2022-03-31 06:12:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1777395573-775924041 in namespace namespace-1
2022-03-31 06:12:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-31 06:12:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717376ms till timeout)
2022-03-31 06:12:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1777395573-775924041
2022-03-31 06:12:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1197382ms till timeout)
2022-03-31 06:12:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1777395573-775924041 will have desired state: Ready
2022-03-31 06:12:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1777395573-775924041 will have desired state: Ready
2022-03-31 06:12:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1777395573-775924041 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-31 06:12:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-1410293327 to finished
2022-03-31 06:12:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-31 06:12:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:12:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219989ms till timeout)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716370ms till timeout)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1196376ms till timeout)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1777395573-775924041 is in desired state: Ready
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2082882415-1062946918 in namespace namespace-1
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-2082882415-1062946918
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2082882415-1062946918 will have desired state: Ready
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-2082882415-1062946918 will have desired state: Ready
2022-03-31 06:13:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-2082882415-1062946918 will have desired state: Ready not ready, will try again in 1000 ms (179995ms till timeout)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218971ms till timeout)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715366ms till timeout)
2022-03-31 06:13:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1195371ms till timeout)
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2082882415-1062946918 is in desired state: Ready
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7009fdd7-kafka-clients in namespace namespace-1
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-7009fdd7-kafka-clients
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7009fdd7-kafka-clients will be ready
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-7009fdd7-kafka-clients will be ready
2022-03-31 06:13:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7009fdd7-kafka-clients will be ready not ready, will try again in 1000 ms (479991ms till timeout)
2022-03-31 06:13:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217963ms till timeout)
2022-03-31 06:13:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714359ms till timeout)
2022-03-31 06:13:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1194360ms till timeout)
2022-03-31 06:13:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7009fdd7-kafka-clients will be ready not ready, will try again in 1000 ms (478987ms till timeout)
2022-03-31 06:13:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216953ms till timeout)
2022-03-31 06:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713355ms till timeout)
2022-03-31 06:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1193357ms till timeout)
2022-03-31 06:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7009fdd7-kafka-clients will be ready not ready, will try again in 1000 ms (477983ms till timeout)
2022-03-31 06:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215945ms till timeout)
2022-03-31 06:13:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712351ms till timeout)
2022-03-31 06:13:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1192354ms till timeout)
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7009fdd7-kafka-clients is ready
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ListenersST:370] Checking produced and consumed messages to pod:my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3bd7dedc, which are set.
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@50ecece5, messages=[], arguments=[--bootstrap-server, my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096, --topic, my-topic-1777395573-775924041, --max-messages, 100, USER=my_user_2082882415_1062946918], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h', podNamespace='namespace-1', bootstrapServer='my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096', topicName='my-topic-1777395573-775924041', maxMessages=100, kafkaUsername='my-user-2082882415-1062946918', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3bd7dedc}
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096:my-topic-1777395573-775924041 from pod my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h -n namespace-1 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096 --topic my-topic-1777395573-775924041 --max-messages 100 USER=my_user_2082882415_1062946918
2022-03-31 06:13:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h -n namespace-1 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096 --topic my-topic-1777395573-775924041 --max-messages 100 USER=my_user_2082882415_1062946918
2022-03-31 06:13:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (214936ms till timeout)
2022-03-31 06:13:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711334ms till timeout)
2022-03-31 06:13:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1191338ms till timeout)
2022-03-31 06:13:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (213928ms till timeout)
2022-03-31 06:13:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710330ms till timeout)
2022-03-31 06:13:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1190335ms till timeout)
2022-03-31 06:13:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-1410293327 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-31T06:13:06Z, conditions=[JobCondition(lastProbeTime=2022-03-31T06:13:06Z, lastTransitionTime=2022-03-31T06:13:06Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-31T06:12:58Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-31 06:13:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1410293327 deletion
2022-03-31 06:13:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-1410293327 to be deleted
2022-03-31 06:13:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-1410293327 to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-31 06:13:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709326ms till timeout)
2022-03-31 06:13:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1189332ms till timeout)
2022-03-31 06:13:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708319ms till timeout)
2022-03-31 06:13:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1188325ms till timeout)
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3772eb15, which are set.
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3d9e658d, messages=[], arguments=[--bootstrap-server, my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096, --group-id, my-consumer-group-439904148, --topic, my-topic-1777395573-775924041, --max-messages, 100, USER=my_user_2082882415_1062946918, --group-instance-id, instance518389305], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h', podNamespace='namespace-1', bootstrapServer='my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096', topicName='my-topic-1777395573-775924041', maxMessages=100, kafkaUsername='my-user-2082882415-1062946918', consumerGroupName='my-consumer-group-439904148', consumerInstanceId='instance518389305', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3772eb15}
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096:my-topic-1777395573-775924041 from pod my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h -n namespace-1 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096 --group-id my-consumer-group-439904148 --topic my-topic-1777395573-775924041 --max-messages 100 USER=my_user_2082882415_1062946918 --group-instance-id instance518389305
2022-03-31 06:13:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7009fdd7-kafka-clients-645c6855b5-4fm2h -n namespace-1 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7009fdd7-kafka-bootstrap.namespace-1.svc:9096 --group-id my-consumer-group-439904148 --topic my-topic-1777395573-775924041 --max-messages 100 USER=my_user_2082882415_1062946918 --group-instance-id instance518389305
2022-03-31 06:13:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707314ms till timeout)
2022-03-31 06:13:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1187320ms till timeout)
2022-03-31 06:13:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706311ms till timeout)
2022-03-31 06:13:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1186316ms till timeout)
2022-03-31 06:13:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705307ms till timeout)
2022-03-31 06:13:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1185307ms till timeout)
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job consumer-1410293327 was deleted
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job producer-1239134595 in namespace http-bridge-tls-st
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1239134595
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1410293327 in namespace http-bridge-tls-st
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1410293327
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-717762053-1645501097 in namespace http-bridge-tls-st
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-717762053-1645501097
2022-03-31 06:13:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-717762053-1645501097 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:13:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (704303ms till timeout)
2022-03-31 06:13:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1184304ms till timeout)
2022-03-31 06:13:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (703299ms till timeout)
2022-03-31 06:13:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1183301ms till timeout)
2022-03-31 06:13:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (702296ms till timeout)
2022-03-31 06:13:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1182299ms till timeout)
2022-03-31 06:13:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (701283ms till timeout)
2022-03-31 06:13:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1181285ms till timeout)
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ListenersST:377] Checking if generated password has 25 characters
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsScramSha
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2082882415-1062946918 in namespace namespace-1
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2082882415-1062946918
2022-03-31 06:13:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2082882415-1062946918 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-31 06:13:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (700280ms till timeout)
2022-03-31 06:13:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1180282ms till timeout)
2022-03-31 06:13:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (699276ms till timeout)
2022-03-31 06:13:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1179280ms till timeout)
2022-03-31 06:13:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (698272ms till timeout)
2022-03-31 06:13:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1178277ms till timeout)
2022-03-31 06:13:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (697268ms till timeout)
2022-03-31 06:13:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1177274ms till timeout)
2022-03-31 06:13:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (696265ms till timeout)
2022-03-31 06:13:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1176271ms till timeout)
2022-03-31 06:13:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (695262ms till timeout)
2022-03-31 06:13:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1175267ms till timeout)
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls] to and randomly select one to start execution
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 2
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 3
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2026479541-2064760562 in namespace http-bridge-tls-st
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2026479541-2064760562
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2026479541-2064760562 will have desired state: Ready
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2026479541-2064760562 will have desired state: Ready
2022-03-31 06:13:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2026479541-2064760562 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:13:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (694258ms till timeout)
2022-03-31 06:13:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1174263ms till timeout)
2022-03-31 06:13:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2026479541-2064760562 is in desired state: Ready
2022-03-31 06:13:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1231125982 in namespace http-bridge-tls-st
2022-03-31 06:13:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1231125982
2022-03-31 06:13:22 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1231125982 will be in active state
2022-03-31 06:13:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 06:13:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179994ms till timeout)
2022-03-31 06:13:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (693254ms till timeout)
2022-03-31 06:13:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1173260ms till timeout)
2022-03-31 06:13:23 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-31 06:13:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job producer-513676144 in namespace http-bridge-tls-st
2022-03-31 06:13:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-513676144
2022-03-31 06:13:23 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: producer-513676144 will be in active state
2022-03-31 06:13:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 06:13:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179994ms till timeout)
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:13:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1172255ms till timeout)
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 4
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-134946258-722731532 in namespace user-st
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-134946258-722731532
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-134946258-722731532 will have desired state: Ready
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-134946258-722731532 will have desired state: Ready
2022-03-31 06:13:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-134946258-722731532 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:13:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:61] Waiting till producer producer-513676144 and consumer consumer-1231125982 finish
2022-03-31 06:13:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-31 06:13:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219994ms till timeout)
2022-03-31 06:13:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1171251ms till timeout)
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-134946258-722731532 is in desired state: Ready
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-134946258-722731532
2022-03-31 06:13:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] increase observation generation from 1 for user my-user-134946258-722731532 not ready, will try again in 1000 ms (179991ms till timeout)
2022-03-31 06:13:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218991ms till timeout)
2022-03-31 06:13:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7009fdd7-kafka-clients in namespace namespace-1
2022-03-31 06:13:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7009fdd7-kafka-clients
2022-03-31 06:13:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7009fdd7-kafka-clients not ready, will try again in 10000 ms (479987ms till timeout)
2022-03-31 06:13:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1170247ms till timeout)
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-134946258-722731532
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Expected secret my-user-134946258-722731532 exists
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:50] Secret my-user-134946258-722731532 created
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-134946258-722731532 will have desired state: Ready
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-134946258-722731532 will have desired state: Ready
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-134946258-722731532 is in desired state: Ready
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-134946258-722731532
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser deletion my-user-134946258-722731532
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-134946258-722731532 deleted
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-134946258-722731532 in namespace user-st
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-134946258-722731532
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser] to and randomly select one to start execution
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 3
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for UserST
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-31 06:13:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-31 06:13:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217987ms till timeout)
2022-03-31 06:13:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1169244ms till timeout)
2022-03-31 06:13:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216984ms till timeout)
2022-03-31 06:13:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1168238ms till timeout)
2022-03-31 06:13:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215978ms till timeout)
2022-03-31 06:13:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1167234ms till timeout)
2022-03-31 06:13:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214973ms till timeout)
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: cruise-control-api-cluster-name is in desired state: Ready
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:153] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec cruise-control-api-cluster-name-cruise-control-659db85689-c6924 -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec cruise-control-api-cluster-name-cruise-control-659db85689-c6924 -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:157] Verifying that Cruise Control REST API is available using HTTP request without credentials
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka cruise-control-api-cluster-name in namespace namespace-0
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster cruise-control-api-cluster-name
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-31 06:13:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-31 06:13:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-513676144 deletion
2022-03-31 06:13:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-513676144 to be deleted
2022-03-31 06:13:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-513676144 to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-31 06:13:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-513676144 to be deleted not ready, will try again in 5000 ms (174991ms till timeout)
2022-03-31 06:13:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7009fdd7-kafka-clients not ready, will try again in 10000 ms (469977ms till timeout)
2022-03-31 06:13:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:13:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st removal
2022-03-31 06:13:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479884ms till timeout)
2022-03-31 06:13:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478810ms till timeout)
2022-03-31 06:13:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (477729ms till timeout)
2022-03-31 06:13:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476636ms till timeout)
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479908ms till timeout)
2022-03-31 06:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job producer-513676144 was deleted
2022-03-31 06:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1231125982 deletion
2022-03-31 06:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-1231125982 to be deleted
2022-03-31 06:13:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-1231125982 to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-31 06:13:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475560ms till timeout)
2022-03-31 06:13:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (478833ms till timeout)
2022-03-31 06:13:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (474486ms till timeout)
2022-03-31 06:13:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (477759ms till timeout)
2022-03-31 06:13:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (473405ms till timeout)
2022-03-31 06:13:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476674ms till timeout)
2022-03-31 06:13:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (472330ms till timeout)
2022-03-31 06:13:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475599ms till timeout)
2022-03-31 06:13:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (471252ms till timeout)
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job consumer-1231125982 was deleted
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1231125982 in namespace http-bridge-tls-st
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1231125982
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job producer-513676144 in namespace http-bridge-tls-st
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-513676144
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2026479541-2064760562 in namespace http-bridge-tls-st
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2026479541-2064760562
2022-03-31 06:13:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2026479541-2064760562 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-31 06:13:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7009fdd7-kafka-clients not ready, will try again in 10000 ms (459967ms till timeout)
2022-03-31 06:13:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (474519ms till timeout)
2022-03-31 06:13:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-31 06:13:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 7
2022-03-31 06:13:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (470169ms till timeout)
2022-03-31 06:13:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473446ms till timeout)
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-1 get Namespace user-st -o yaml
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:254] UserST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST, SecurityST] to and randomly select one to start execution
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 6
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 205.7 s - in io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:667] [watcher.AllNamespaceIsolatedST - Before All] - Setup test suite environment
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:13:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472367ms till timeout)
2022-03-31 06:13:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (471294ms till timeout)
2022-03-31 06:13:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (470216ms till timeout)
2022-03-31 06:13:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:184] SecurityST suite now can proceed its execution
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:184] RollingUpdateST suite now can proceed its execution
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `SecurityST` creates these additional namespaces:[security-st]
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `RollingUpdateST` creates these additional namespaces:[rolling-update-st]
2022-03-31 06:13:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: rolling-update-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:13:51Z",
        "name": "security-st",
        "resourceVersion": "133471",
        "selfLink": "/api/v1/namespaces/security-st",
        "uid": "ff601c71-1c3b-480f-a041-b64ef73586d0"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=security-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:659] [security.SecurityST - Before Each] - Setup test case environment
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:77] [security.SecurityST] - Adding parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:81] [security.SecurityST] - Parallel test count: 4
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:230] testAutoRenewAllCaCertsTriggeredByAnno test now can proceed its execution
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:13:51Z",
        "name": "rolling-update-st",
        "resourceVersion": "133470",
        "selfLink": "/api/v1/namespaces/rolling-update-st",
        "uid": "344f7424-352a-4ee3-8287-b7b0aa7d1bcd"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: rolling-update-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=rolling-update-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: rolling-update-st
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-STARTED
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [rollingupdate.RollingUpdateST - Before Each] - Setup test case environment
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [rollingupdate.RollingUpdateST] - Adding parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [rollingupdate.RollingUpdateST] - Parallel test count: 5
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaAndZookeeperScaleUpScaleDown test now can proceed its execution
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace rolling-update-st get Namespace namespace-2 -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace rolling-update-st get Namespace namespace-2 -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:13:51Z",
        "name": "namespace-2",
        "resourceVersion": "133478",
        "selfLink": "/api/v1/namespaces/namespace-2",
        "uid": "97119a1a-7a03-4fce-bd87-1048a3dc4a6b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:596] Creating a cluster
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2def1ad9 in namespace namespace-2
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-2def1ad9
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2def1ad9 will have desired state: Ready
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-2def1ad9 will have desired state: Ready
2022-03-31 06:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1799997ms till timeout)
2022-03-31 06:13:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:13:51Z",
        "name": "namespace-3",
        "resourceVersion": "133482",
        "selfLink": "/api/v1/namespaces/namespace-3",
        "uid": "cfe9c989-dd81-4ec1-b8e7-f81e0a0b7eb1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-012ef870 in namespace namespace-3
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-012ef870
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:13:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:13:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (469122ms till timeout)
2022-03-31 06:13:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1798993ms till timeout)
2022-03-31 06:13:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-31 06:13:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (468049ms till timeout)
2022-03-31 06:13:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1797990ms till timeout)
2022-03-31 06:13:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-31 06:13:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466962ms till timeout)
2022-03-31 06:13:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1796986ms till timeout)
2022-03-31 06:13:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:13:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465889ms till timeout)
2022-03-31 06:13:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1795982ms till timeout)
2022-03-31 06:13:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-31 06:13:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (464815ms till timeout)
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 4
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-31 06:13:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479945ms till timeout)
2022-03-31 06:13:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7009fdd7-kafka-clients not ready, will try again in 10000 ms (449957ms till timeout)
2022-03-31 06:13:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1794978ms till timeout)
2022-03-31 06:13:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-31 06:13:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463745ms till timeout)
2022-03-31 06:13:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1793975ms till timeout)
2022-03-31 06:13:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:13:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:13:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-31 06:13:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462669ms till timeout)
2022-03-31 06:13:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1792971ms till timeout)
2022-03-31 06:13:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-31 06:13:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (461586ms till timeout)
2022-03-31 06:13:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1791968ms till timeout)
2022-03-31 06:13:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (831971ms till timeout)
2022-03-31 06:13:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:13:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:13:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (460493ms till timeout)
2022-03-31 06:14:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1790964ms till timeout)
2022-03-31 06:14:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-31 06:14:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459407ms till timeout)
2022-03-31 06:14:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1789960ms till timeout)
2022-03-31 06:14:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (829963ms till timeout)
2022-03-31 06:14:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (458281ms till timeout)
2022-03-31 06:14:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1788914ms till timeout)
2022-03-31 06:14:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (828959ms till timeout)
2022-03-31 06:14:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1787906ms till timeout)
2022-03-31 06:14:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457157ms till timeout)
2022-03-31 06:14:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (827955ms till timeout)
2022-03-31 06:14:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1786902ms till timeout)
2022-03-31 06:14:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (826943ms till timeout)
2022-03-31 06:14:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456051ms till timeout)
2022-03-31 06:14:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1785897ms till timeout)
2022-03-31 06:14:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (825939ms till timeout)
2022-03-31 06:14:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (454956ms till timeout)
2022-03-31 06:14:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (469930ms till timeout)
2022-03-31 06:14:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1777395573-775924041 in namespace namespace-1
2022-03-31 06:14:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1777395573-775924041
2022-03-31 06:14:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1777395573-775924041 not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-31 06:14:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1784893ms till timeout)
2022-03-31 06:14:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (824929ms till timeout)
2022-03-31 06:14:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453863ms till timeout)
2022-03-31 06:14:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1783881ms till timeout)
2022-03-31 06:14:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (823923ms till timeout)
2022-03-31 06:14:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (452765ms till timeout)
2022-03-31 06:14:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1782877ms till timeout)
2022-03-31 06:14:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (822920ms till timeout)
2022-03-31 06:14:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (451688ms till timeout)
2022-03-31 06:14:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1781874ms till timeout)
2022-03-31 06:14:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (821916ms till timeout)
2022-03-31 06:14:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450610ms till timeout)
2022-03-31 06:14:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1780868ms till timeout)
2022-03-31 06:14:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (820912ms till timeout)
2022-03-31 06:14:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449520ms till timeout)
2022-03-31 06:14:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1779864ms till timeout)
2022-03-31 06:14:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (819906ms till timeout)
2022-03-31 06:14:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (448412ms till timeout)
2022-03-31 06:14:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1778859ms till timeout)
2022-03-31 06:14:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (818902ms till timeout)
2022-03-31 06:14:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447310ms till timeout)
2022-03-31 06:14:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1777855ms till timeout)
2022-03-31 06:14:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (817896ms till timeout)
2022-03-31 06:14:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446214ms till timeout)
2022-03-31 06:14:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1776851ms till timeout)
2022-03-31 06:14:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (816885ms till timeout)
2022-03-31 06:14:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (445132ms till timeout)
2022-03-31 06:14:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1775847ms till timeout)
2022-03-31 06:14:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (815881ms till timeout)
2022-03-31 06:14:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (459917ms till timeout)
2022-03-31 06:14:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7009fdd7 in namespace namespace-1
2022-03-31 06:14:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7009fdd7
2022-03-31 06:14:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7009fdd7 not ready, will try again in 10000 ms (839956ms till timeout)
2022-03-31 06:14:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1774832ms till timeout)
2022-03-31 06:14:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (814876ms till timeout)
2022-03-31 06:14:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444040ms till timeout)
2022-03-31 06:14:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1773828ms till timeout)
2022-03-31 06:14:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (813873ms till timeout)
2022-03-31 06:14:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (442962ms till timeout)
2022-03-31 06:14:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1772825ms till timeout)
2022-03-31 06:14:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (812870ms till timeout)
2022-03-31 06:14:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (441889ms till timeout)
2022-03-31 06:14:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1771822ms till timeout)
2022-03-31 06:14:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (811868ms till timeout)
2022-03-31 06:14:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (440818ms till timeout)
2022-03-31 06:14:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1770819ms till timeout)
2022-03-31 06:14:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (810865ms till timeout)
2022-03-31 06:14:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (439742ms till timeout)
2022-03-31 06:14:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1769815ms till timeout)
2022-03-31 06:14:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (809862ms till timeout)
2022-03-31 06:14:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (438665ms till timeout)
2022-03-31 06:14:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1768812ms till timeout)
2022-03-31 06:14:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (808859ms till timeout)
2022-03-31 06:14:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (437595ms till timeout)
2022-03-31 06:14:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1767809ms till timeout)
2022-03-31 06:14:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (807857ms till timeout)
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequestsWithSecurityDisabled - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 3
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-FINISHED
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-STARTED
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequests
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 4
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequests test now can proceed its execution
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testCruiseControlBasicAPIRequests
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4
2022-03-31 06:14:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:14:23Z",
        "name": "namespace-4",
        "resourceVersion": "133831",
        "selfLink": "/api/v1/namespaces/namespace-4",
        "uid": "7f6b0086-b213-478d-aae2-4868bdea06dc"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-1], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f88869c9 in namespace namespace-4
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-f88869c9
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f88869c9 will have desired state: Ready
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-f88869c9 will have desired state: Ready
2022-03-31 06:14:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1319998ms till timeout)
2022-03-31 06:14:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1766805ms till timeout)
2022-03-31 06:14:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (806854ms till timeout)
2022-03-31 06:14:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1318994ms till timeout)
2022-03-31 06:14:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1765802ms till timeout)
2022-03-31 06:14:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (805850ms till timeout)
2022-03-31 06:14:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (449908ms till timeout)
2022-03-31 06:14:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:14:25 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testSendMessagesTlsScramSha
2022-03-31 06:14:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-31 06:14:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479921ms till timeout)
2022-03-31 06:14:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1317991ms till timeout)
2022-03-31 06:14:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1764799ms till timeout)
2022-03-31 06:14:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (804848ms till timeout)
2022-03-31 06:14:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1316987ms till timeout)
2022-03-31 06:14:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478844ms till timeout)
2022-03-31 06:14:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1763795ms till timeout)
2022-03-31 06:14:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (803845ms till timeout)
2022-03-31 06:14:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1315983ms till timeout)
2022-03-31 06:14:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (477768ms till timeout)
2022-03-31 06:14:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1762792ms till timeout)
2022-03-31 06:14:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (802840ms till timeout)
2022-03-31 06:14:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1314980ms till timeout)
2022-03-31 06:14:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476694ms till timeout)
2022-03-31 06:14:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1761789ms till timeout)
2022-03-31 06:14:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (801837ms till timeout)
2022-03-31 06:14:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1313975ms till timeout)
2022-03-31 06:14:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475612ms till timeout)
2022-03-31 06:14:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1760785ms till timeout)
2022-03-31 06:14:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (800834ms till timeout)
2022-03-31 06:14:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1312971ms till timeout)
2022-03-31 06:14:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1759782ms till timeout)
2022-03-31 06:14:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474519ms till timeout)
2022-03-31 06:14:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (799830ms till timeout)
2022-03-31 06:14:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1311960ms till timeout)
2022-03-31 06:14:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1758779ms till timeout)
2022-03-31 06:14:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (798824ms till timeout)
2022-03-31 06:14:32 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:32 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473423ms till timeout)
2022-03-31 06:14:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1310957ms till timeout)
2022-03-31 06:14:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1757774ms till timeout)
2022-03-31 06:14:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (797820ms till timeout)
2022-03-31 06:14:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (472302ms till timeout)
2022-03-31 06:14:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1309947ms till timeout)
2022-03-31 06:14:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1756769ms till timeout)
2022-03-31 06:14:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (796814ms till timeout)
2022-03-31 06:14:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (471222ms till timeout)
2022-03-31 06:14:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1308943ms till timeout)
2022-03-31 06:14:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1755765ms till timeout)
2022-03-31 06:14:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (795810ms till timeout)
2022-03-31 06:14:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-31 06:14:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-31 06:14:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470131ms till timeout)
2022-03-31 06:14:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (479968ms till timeout)
2022-03-31 06:14:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1307939ms till timeout)
2022-03-31 06:14:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1754760ms till timeout)
2022-03-31 06:14:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (794807ms till timeout)
2022-03-31 06:14:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (469037ms till timeout)
2022-03-31 06:14:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1306935ms till timeout)
2022-03-31 06:14:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1753754ms till timeout)
2022-03-31 06:14:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (793803ms till timeout)
2022-03-31 06:14:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (467945ms till timeout)
2022-03-31 06:14:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1305932ms till timeout)
2022-03-31 06:14:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1752749ms till timeout)
2022-03-31 06:14:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (792798ms till timeout)
2022-03-31 06:14:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1304918ms till timeout)
2022-03-31 06:14:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466827ms till timeout)
2022-03-31 06:14:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1751745ms till timeout)
2022-03-31 06:14:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (791794ms till timeout)
2022-03-31 06:14:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1303910ms till timeout)
2022-03-31 06:14:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (465691ms till timeout)
2022-03-31 06:14:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1750738ms till timeout)
2022-03-31 06:14:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (790786ms till timeout)
2022-03-31 06:14:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1302907ms till timeout)
2022-03-31 06:14:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (464567ms till timeout)
2022-03-31 06:14:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1749731ms till timeout)
2022-03-31 06:14:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (789777ms till timeout)
2022-03-31 06:14:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1301899ms till timeout)
2022-03-31 06:14:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1748725ms till timeout)
2022-03-31 06:14:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (463472ms till timeout)
2022-03-31 06:14:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (788770ms till timeout)
2022-03-31 06:14:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1300891ms till timeout)
2022-03-31 06:14:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1747707ms till timeout)
2022-03-31 06:14:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (787760ms till timeout)
2022-03-31 06:14:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (462370ms till timeout)
2022-03-31 06:14:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1299888ms till timeout)
2022-03-31 06:14:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1746704ms till timeout)
2022-03-31 06:14:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (786757ms till timeout)
2022-03-31 06:14:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (461295ms till timeout)
2022-03-31 06:14:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1298884ms till timeout)
2022-03-31 06:14:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1745700ms till timeout)
2022-03-31 06:14:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (785754ms till timeout)
2022-03-31 06:14:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (460217ms till timeout)
2022-03-31 06:14:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-182343961-967786628 in namespace http-bridge-tls-st
2022-03-31 06:14:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-182343961-967786628
2022-03-31 06:14:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-182343961-967786628 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-31 06:14:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1297881ms till timeout)
2022-03-31 06:14:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1744697ms till timeout)
2022-03-31 06:14:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (784752ms till timeout)
2022-03-31 06:14:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (459141ms till timeout)
2022-03-31 06:14:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1296877ms till timeout)
2022-03-31 06:14:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1743693ms till timeout)
2022-03-31 06:14:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (783749ms till timeout)
2022-03-31 06:14:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458068ms till timeout)
2022-03-31 06:14:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1295873ms till timeout)
2022-03-31 06:14:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1742689ms till timeout)
2022-03-31 06:14:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (782746ms till timeout)
2022-03-31 06:14:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (456995ms till timeout)
2022-03-31 06:14:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1294870ms till timeout)
2022-03-31 06:14:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1741686ms till timeout)
2022-03-31 06:14:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (781744ms till timeout)
2022-03-31 06:14:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (455917ms till timeout)
2022-03-31 06:14:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1293866ms till timeout)
2022-03-31 06:14:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1740682ms till timeout)
2022-03-31 06:14:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (780741ms till timeout)
2022-03-31 06:14:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (454845ms till timeout)
2022-03-31 06:14:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1292862ms till timeout)
2022-03-31 06:14:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1739678ms till timeout)
2022-03-31 06:14:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (779738ms till timeout)
2022-03-31 06:14:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1291859ms till timeout)
2022-03-31 06:14:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453770ms till timeout)
2022-03-31 06:14:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1738675ms till timeout)
2022-03-31 06:14:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (778735ms till timeout)
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1290856ms till timeout)
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesTlsScramSha - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests] to and randomly select one to start execution
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesTlsScramSha
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 3
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-FINISHED
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-STARTED
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 4
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesCustomListenerTlsScramSha test now can proceed its execution
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:14:53Z",
        "name": "namespace-5",
        "resourceVersion": "134101",
        "selfLink": "/api/v1/namespaces/namespace-5",
        "uid": "74c1f1f3-9cf1-47f5-aaf5-c44c64485099"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-5], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0a4e5233 in namespace namespace-5
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0a4e5233
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0a4e5233 will have desired state: Ready
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-0a4e5233 will have desired state: Ready
2022-03-31 06:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-31 06:14:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1737671ms till timeout)
2022-03-31 06:14:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (777732ms till timeout)
2022-03-31 06:14:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1289852ms till timeout)
2022-03-31 06:14:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (838988ms till timeout)
2022-03-31 06:14:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1736668ms till timeout)
2022-03-31 06:14:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (776730ms till timeout)
2022-03-31 06:14:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1288849ms till timeout)
2022-03-31 06:14:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (837985ms till timeout)
2022-03-31 06:14:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1735664ms till timeout)
2022-03-31 06:14:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (775726ms till timeout)
2022-03-31 06:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-31 06:14:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-31 06:14:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-31 06:14:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1287846ms till timeout)
2022-03-31 06:14:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (836982ms till timeout)
2022-03-31 06:14:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1734661ms till timeout)
2022-03-31 06:14:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (774724ms till timeout)
2022-03-31 06:14:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1286842ms till timeout)
2022-03-31 06:14:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:14:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:14:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (835979ms till timeout)
2022-03-31 06:14:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1733658ms till timeout)
2022-03-31 06:14:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (773721ms till timeout)
2022-03-31 06:14:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1285839ms till timeout)
2022-03-31 06:14:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (834976ms till timeout)
2022-03-31 06:14:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1732655ms till timeout)
2022-03-31 06:14:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (772717ms till timeout)
2022-03-31 06:14:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1284828ms till timeout)
2022-03-31 06:14:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (833972ms till timeout)
2022-03-31 06:14:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1731652ms till timeout)
2022-03-31 06:14:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (771710ms till timeout)
2022-03-31 06:15:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1283825ms till timeout)
2022-03-31 06:15:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (832969ms till timeout)
2022-03-31 06:15:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1730646ms till timeout)
2022-03-31 06:15:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (770706ms till timeout)
2022-03-31 06:15:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1282821ms till timeout)
2022-03-31 06:15:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (831965ms till timeout)
2022-03-31 06:15:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1729638ms till timeout)
2022-03-31 06:15:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (769704ms till timeout)
2022-03-31 06:15:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1281817ms till timeout)
2022-03-31 06:15:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (830959ms till timeout)
2022-03-31 06:15:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1728633ms till timeout)
2022-03-31 06:15:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (768687ms till timeout)
2022-03-31 06:15:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1280814ms till timeout)
2022-03-31 06:15:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (829955ms till timeout)
2022-03-31 06:15:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1727630ms till timeout)
2022-03-31 06:15:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (767683ms till timeout)
2022-03-31 06:15:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1279810ms till timeout)
2022-03-31 06:15:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (828952ms till timeout)
2022-03-31 06:15:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1726626ms till timeout)
2022-03-31 06:15:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (766681ms till timeout)
2022-03-31 06:15:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1278807ms till timeout)
2022-03-31 06:15:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (827948ms till timeout)
2022-03-31 06:15:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1725609ms till timeout)
2022-03-31 06:15:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (765678ms till timeout)
2022-03-31 06:15:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:15:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-31 06:15:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479911ms till timeout)
2022-03-31 06:15:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1277803ms till timeout)
2022-03-31 06:15:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (826944ms till timeout)
2022-03-31 06:15:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1724595ms till timeout)
2022-03-31 06:15:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (764672ms till timeout)
2022-03-31 06:15:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478829ms till timeout)
2022-03-31 06:15:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1276799ms till timeout)
2022-03-31 06:15:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (825938ms till timeout)
2022-03-31 06:15:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1723590ms till timeout)
2022-03-31 06:15:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (763669ms till timeout)
2022-03-31 06:15:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (477739ms till timeout)
2022-03-31 06:15:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1275794ms till timeout)
2022-03-31 06:15:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (824934ms till timeout)
2022-03-31 06:15:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1722586ms till timeout)
2022-03-31 06:15:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (762666ms till timeout)
2022-03-31 06:15:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1274785ms till timeout)
2022-03-31 06:15:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476629ms till timeout)
2022-03-31 06:15:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (823930ms till timeout)
2022-03-31 06:15:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1721582ms till timeout)
2022-03-31 06:15:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (761658ms till timeout)
2022-03-31 06:15:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1273780ms till timeout)
2022-03-31 06:15:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (475514ms till timeout)
2022-03-31 06:15:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (822926ms till timeout)
2022-03-31 06:15:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1720575ms till timeout)
2022-03-31 06:15:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (760653ms till timeout)
2022-03-31 06:15:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1272774ms till timeout)
2022-03-31 06:15:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (821920ms till timeout)
2022-03-31 06:15:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (474384ms till timeout)
2022-03-31 06:15:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1719571ms till timeout)
2022-03-31 06:15:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (759649ms till timeout)
2022-03-31 06:15:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1271770ms till timeout)
2022-03-31 06:15:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (820912ms till timeout)
2022-03-31 06:15:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1718567ms till timeout)
2022-03-31 06:15:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473276ms till timeout)
2022-03-31 06:15:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (758642ms till timeout)
2022-03-31 06:15:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1270765ms till timeout)
2022-03-31 06:15:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (819908ms till timeout)
2022-03-31 06:15:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1717563ms till timeout)
2022-03-31 06:15:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (757635ms till timeout)
2022-03-31 06:15:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (472169ms till timeout)
2022-03-31 06:15:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1269762ms till timeout)
2022-03-31 06:15:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (818904ms till timeout)
2022-03-31 06:15:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1716559ms till timeout)
2022-03-31 06:15:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (756627ms till timeout)
2022-03-31 06:15:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (471071ms till timeout)
2022-03-31 06:15:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1268758ms till timeout)
2022-03-31 06:15:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (817900ms till timeout)
2022-03-31 06:15:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1715554ms till timeout)
2022-03-31 06:15:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (755623ms till timeout)
2022-03-31 06:15:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (469960ms till timeout)
2022-03-31 06:15:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1267750ms till timeout)
2022-03-31 06:15:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (816893ms till timeout)
2022-03-31 06:15:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1714549ms till timeout)
2022-03-31 06:15:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (754618ms till timeout)
2022-03-31 06:15:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (468832ms till timeout)
2022-03-31 06:15:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1266746ms till timeout)
2022-03-31 06:15:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (815888ms till timeout)
2022-03-31 06:15:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1713543ms till timeout)
2022-03-31 06:15:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (753614ms till timeout)
2022-03-31 06:15:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (467682ms till timeout)
2022-03-31 06:15:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1265741ms till timeout)
2022-03-31 06:15:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (814884ms till timeout)
2022-03-31 06:15:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1712533ms till timeout)
2022-03-31 06:15:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (752610ms till timeout)
2022-03-31 06:15:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1264734ms till timeout)
2022-03-31 06:15:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (466551ms till timeout)
2022-03-31 06:15:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (813872ms till timeout)
2022-03-31 06:15:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1711527ms till timeout)
2022-03-31 06:15:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (751606ms till timeout)
2022-03-31 06:15:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1263730ms till timeout)
2022-03-31 06:15:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (465411ms till timeout)
2022-03-31 06:15:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (812868ms till timeout)
2022-03-31 06:15:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1710521ms till timeout)
2022-03-31 06:15:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (750603ms till timeout)
2022-03-31 06:15:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1262726ms till timeout)
2022-03-31 06:15:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (811862ms till timeout)
2022-03-31 06:15:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (464315ms till timeout)
2022-03-31 06:15:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1709517ms till timeout)
2022-03-31 06:15:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (749600ms till timeout)
2022-03-31 06:15:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1261722ms till timeout)
2022-03-31 06:15:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (810858ms till timeout)
2022-03-31 06:15:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1708511ms till timeout)
2022-03-31 06:15:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (748596ms till timeout)
2022-03-31 06:15:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (463172ms till timeout)
2022-03-31 06:15:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1260719ms till timeout)
2022-03-31 06:15:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (809854ms till timeout)
2022-03-31 06:15:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1707507ms till timeout)
2022-03-31 06:15:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (747593ms till timeout)
2022-03-31 06:15:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (462077ms till timeout)
2022-03-31 06:15:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1259715ms till timeout)
2022-03-31 06:15:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (808844ms till timeout)
2022-03-31 06:15:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1706503ms till timeout)
2022-03-31 06:15:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (746590ms till timeout)
2022-03-31 06:15:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (460986ms till timeout)
2022-03-31 06:15:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1258711ms till timeout)
2022-03-31 06:15:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (807839ms till timeout)
2022-03-31 06:15:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1705500ms till timeout)
2022-03-31 06:15:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (745586ms till timeout)
2022-03-31 06:15:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (459907ms till timeout)
2022-03-31 06:15:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1257707ms till timeout)
2022-03-31 06:15:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (806836ms till timeout)
2022-03-31 06:15:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1704496ms till timeout)
2022-03-31 06:15:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (744582ms till timeout)
2022-03-31 06:15:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (458812ms till timeout)
2022-03-31 06:15:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1256703ms till timeout)
2022-03-31 06:15:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (805833ms till timeout)
2022-03-31 06:15:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1703492ms till timeout)
2022-03-31 06:15:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (743578ms till timeout)
2022-03-31 06:15:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (457728ms till timeout)
2022-03-31 06:15:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1255700ms till timeout)
2022-03-31 06:15:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (804818ms till timeout)
2022-03-31 06:15:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1702483ms till timeout)
2022-03-31 06:15:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (742569ms till timeout)
2022-03-31 06:15:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (456650ms till timeout)
2022-03-31 06:15:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1254697ms till timeout)
2022-03-31 06:15:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (803814ms till timeout)
2022-03-31 06:15:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1701474ms till timeout)
2022-03-31 06:15:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (741562ms till timeout)
2022-03-31 06:15:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1253692ms till timeout)
2022-03-31 06:15:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (455549ms till timeout)
2022-03-31 06:15:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (802808ms till timeout)
2022-03-31 06:15:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1700470ms till timeout)
2022-03-31 06:15:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (740555ms till timeout)
2022-03-31 06:15:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1252685ms till timeout)
2022-03-31 06:15:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (454435ms till timeout)
2022-03-31 06:15:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (801801ms till timeout)
2022-03-31 06:15:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1699461ms till timeout)
2022-03-31 06:15:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (739545ms till timeout)
2022-03-31 06:15:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 6
2022-03-31 06:15:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1251680ms till timeout)
2022-03-31 06:15:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (800787ms till timeout)
2022-03-31 06:15:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:15:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (453305ms till timeout)
2022-03-31 06:15:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1698456ms till timeout)
2022-03-31 06:15:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (738541ms till timeout)
2022-03-31 06:15:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1250675ms till timeout)
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (799774ms till timeout)
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-5 get Namespace http-bridge-tls-st -o yaml
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-5], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST] to and randomly select one to start execution
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 312.17 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [connect.ConnectIsolatedST - Before All] - Setup test suite environment
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:15:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1697450ms till timeout)
2022-03-31 06:15:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (737536ms till timeout)
2022-03-31 06:15:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1249668ms till timeout)
2022-03-31 06:15:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (798762ms till timeout)
2022-03-31 06:15:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1696440ms till timeout)
2022-03-31 06:15:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (736519ms till timeout)
2022-03-31 06:15:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1248662ms till timeout)
2022-03-31 06:15:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (797754ms till timeout)
2022-03-31 06:15:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1695431ms till timeout)
2022-03-31 06:15:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (735512ms till timeout)
2022-03-31 06:15:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1247648ms till timeout)
2022-03-31 06:15:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (796747ms till timeout)
2022-03-31 06:15:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1694422ms till timeout)
2022-03-31 06:15:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (734508ms till timeout)
2022-03-31 06:15:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1246643ms till timeout)
2022-03-31 06:15:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (795741ms till timeout)
2022-03-31 06:15:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1693413ms till timeout)
2022-03-31 06:15:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (733499ms till timeout)
2022-03-31 06:15:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1245638ms till timeout)
2022-03-31 06:15:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (794736ms till timeout)
2022-03-31 06:15:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1692401ms till timeout)
2022-03-31 06:15:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (732481ms till timeout)
2022-03-31 06:15:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1244632ms till timeout)
2022-03-31 06:15:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (793732ms till timeout)
2022-03-31 06:15:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1691396ms till timeout)
2022-03-31 06:15:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (731478ms till timeout)
2022-03-31 06:15:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1243628ms till timeout)
2022-03-31 06:15:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (792727ms till timeout)
2022-03-31 06:15:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1690392ms till timeout)
2022-03-31 06:15:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (730474ms till timeout)
2022-03-31 06:15:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1242624ms till timeout)
2022-03-31 06:15:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (791722ms till timeout)
2022-03-31 06:15:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1689385ms till timeout)
2022-03-31 06:15:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (729471ms till timeout)
2022-03-31 06:15:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1241615ms till timeout)
2022-03-31 06:15:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (790716ms till timeout)
2022-03-31 06:15:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1688377ms till timeout)
2022-03-31 06:15:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (728457ms till timeout)
2022-03-31 06:15:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1240609ms till timeout)
2022-03-31 06:15:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (789711ms till timeout)
2022-03-31 06:15:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1687364ms till timeout)
2022-03-31 06:15:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (727452ms till timeout)
2022-03-31 06:15:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1239603ms till timeout)
2022-03-31 06:15:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (788706ms till timeout)
2022-03-31 06:15:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (726442ms till timeout)
2022-03-31 06:15:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1686352ms till timeout)
2022-03-31 06:15:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1238596ms till timeout)
2022-03-31 06:15:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (787701ms till timeout)
2022-03-31 06:15:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (725391ms till timeout)
2022-03-31 06:15:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1685302ms till timeout)
2022-03-31 06:15:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1237591ms till timeout)
2022-03-31 06:15:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (786695ms till timeout)
2022-03-31 06:15:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (724385ms till timeout)
2022-03-31 06:15:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1684294ms till timeout)
2022-03-31 06:15:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1236585ms till timeout)
2022-03-31 06:15:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (785691ms till timeout)
2022-03-31 06:15:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (723376ms till timeout)
2022-03-31 06:15:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1683282ms till timeout)
2022-03-31 06:15:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1235581ms till timeout)
2022-03-31 06:15:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (784687ms till timeout)
2022-03-31 06:15:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (722372ms till timeout)
2022-03-31 06:15:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1682276ms till timeout)
2022-03-31 06:15:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1234576ms till timeout)
2022-03-31 06:15:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (783673ms till timeout)
2022-03-31 06:15:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (721367ms till timeout)
2022-03-31 06:15:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1681267ms till timeout)
2022-03-31 06:15:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1233566ms till timeout)
2022-03-31 06:15:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (782669ms till timeout)
2022-03-31 06:15:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (720360ms till timeout)
2022-03-31 06:15:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1680263ms till timeout)
2022-03-31 06:15:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1232561ms till timeout)
2022-03-31 06:15:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (781664ms till timeout)
2022-03-31 06:15:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (719357ms till timeout)
2022-03-31 06:15:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1679260ms till timeout)
2022-03-31 06:15:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1231557ms till timeout)
2022-03-31 06:15:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (780660ms till timeout)
2022-03-31 06:15:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (718353ms till timeout)
2022-03-31 06:15:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1678257ms till timeout)
2022-03-31 06:15:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1230549ms till timeout)
2022-03-31 06:15:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (779655ms till timeout)
2022-03-31 06:15:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (717349ms till timeout)
2022-03-31 06:15:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1677251ms till timeout)
2022-03-31 06:15:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1229542ms till timeout)
2022-03-31 06:15:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (778650ms till timeout)
2022-03-31 06:15:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (716341ms till timeout)
2022-03-31 06:15:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1676241ms till timeout)
2022-03-31 06:15:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1228534ms till timeout)
2022-03-31 06:15:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (777638ms till timeout)
2022-03-31 06:15:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (715337ms till timeout)
2022-03-31 06:15:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1675237ms till timeout)
2022-03-31 06:15:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1227530ms till timeout)
2022-03-31 06:15:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (776634ms till timeout)
2022-03-31 06:15:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (714333ms till timeout)
2022-03-31 06:15:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1674234ms till timeout)
2022-03-31 06:15:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1226526ms till timeout)
2022-03-31 06:15:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (775627ms till timeout)
2022-03-31 06:15:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (713330ms till timeout)
2022-03-31 06:15:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1673227ms till timeout)
2022-03-31 06:15:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1225516ms till timeout)
2022-03-31 06:15:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:15:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:15:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (774621ms till timeout)
2022-03-31 06:15:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (712326ms till timeout)
2022-03-31 06:15:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1672224ms till timeout)
2022-03-31 06:15:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1224513ms till timeout)
2022-03-31 06:15:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (773618ms till timeout)
2022-03-31 06:15:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (711323ms till timeout)
2022-03-31 06:16:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1671220ms till timeout)
2022-03-31 06:16:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1223509ms till timeout)
2022-03-31 06:16:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (772614ms till timeout)
2022-03-31 06:16:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (710311ms till timeout)
2022-03-31 06:16:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1670217ms till timeout)
2022-03-31 06:16:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1222505ms till timeout)
2022-03-31 06:16:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (771608ms till timeout)
2022-03-31 06:16:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-012ef870 will have desired state: Ready not ready, will try again in 1000 ms (709306ms till timeout)
2022-03-31 06:16:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1669215ms till timeout)
2022-03-31 06:16:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1221502ms till timeout)
2022-03-31 06:16:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (770602ms till timeout)
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-012ef870 is in desired state: Ready
2022-03-31 06:16:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1668212ms till timeout)
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1993501230-801238233 in namespace namespace-3
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1993501230-801238233
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1993501230-801238233 will have desired state: Ready
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1993501230-801238233 will have desired state: Ready
2022-03-31 06:16:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1993501230-801238233 will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-31 06:16:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1220499ms till timeout)
2022-03-31 06:16:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (769597ms till timeout)
2022-03-31 06:16:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1667207ms till timeout)
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1993501230-801238233 is in desired state: Ready
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:489] Verifying docker image names
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get pod -l strimzi.io/name=my-cluster-012ef870-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get pod -l strimzi.io/name=my-cluster-012ef870-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:525] Docker images verified
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:292] Running kafkaScaleUpScaleDown my-cluster-012ef870
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-606370994-844170518 in namespace infra-namespace
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-606370994-844170518
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-606370994-844170518 will have desired state: Ready
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-606370994-844170518 will have desired state: Ready
2022-03-31 06:16:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518 will have desired state: Ready not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-31 06:16:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1219491ms till timeout)
2022-03-31 06:16:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (768593ms till timeout)
2022-03-31 06:16:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1666204ms till timeout)
2022-03-31 06:16:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-606370994-844170518 is in desired state: Ready
2022-03-31 06:16:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-012ef870-kafka-clients in namespace infra-namespace
2022-03-31 06:16:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-31 06:16:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-012ef870-kafka-clients is present.
2022-03-31 06:16:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-012ef870-kafka-clients is present. not ready, will try again in 10000 ms (299983ms till timeout)
2022-03-31 06:16:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1218487ms till timeout)
2022-03-31 06:16:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (767578ms till timeout)
2022-03-31 06:16:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1665200ms till timeout)
2022-03-31 06:16:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1217478ms till timeout)
2022-03-31 06:16:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (766575ms till timeout)
2022-03-31 06:16:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1664196ms till timeout)
2022-03-31 06:16:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1216474ms till timeout)
2022-03-31 06:16:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (765567ms till timeout)
2022-03-31 06:16:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1663192ms till timeout)
2022-03-31 06:16:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1215470ms till timeout)
2022-03-31 06:16:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0a4e5233 will have desired state: Ready not ready, will try again in 1000 ms (764564ms till timeout)
2022-03-31 06:16:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1662187ms till timeout)
2022-03-31 06:16:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1214466ms till timeout)
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0a4e5233 is in desired state: Ready
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-260837071-1349464842 in namespace infra-namespace
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-260837071-1349464842
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-260837071-1349464842 will have desired state: Ready
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-260837071-1349464842 will have desired state: Ready
2022-03-31 06:16:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-260837071-1349464842 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:16:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1661183ms till timeout)
2022-03-31 06:16:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1213463ms till timeout)
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-260837071-1349464842 is in desired state: Ready
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1898862452-1098575469 in namespace infra-namespace
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1898862452-1098575469
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1898862452-1098575469 will have desired state: Ready
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1898862452-1098575469 will have desired state: Ready
2022-03-31 06:16:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1898862452-1098575469 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-31 06:16:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1660178ms till timeout)
2022-03-31 06:16:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1212459ms till timeout)
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1898862452-1098575469 is in desired state: Ready
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0a4e5233-kafka-clients in namespace namespace-5
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0a4e5233-kafka-clients
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0a4e5233-kafka-clients will be ready
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0a4e5233-kafka-clients will be ready
2022-03-31 06:16:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-0a4e5233-kafka-clients will be ready not ready, will try again in 1000 ms (479993ms till timeout)
2022-03-31 06:16:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1659174ms till timeout)
2022-03-31 06:16:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1211455ms till timeout)
2022-03-31 06:16:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-0a4e5233-kafka-clients will be ready not ready, will try again in 1000 ms (478988ms till timeout)
2022-03-31 06:16:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1658170ms till timeout)
2022-03-31 06:16:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1210452ms till timeout)
2022-03-31 06:16:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0a4e5233-kafka-clients is ready
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ListenersST:442] Checking produced and consumed messages to pod:my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@a13b108, which are set.
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4d536d06, messages=[], arguments=[--bootstrap-server, my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122, --topic, my-topic-260837071-1349464842, --max-messages, 100, USER=my_user_1898862452_1098575469], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj', podNamespace='namespace-5', bootstrapServer='my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122', topicName='my-topic-260837071-1349464842', maxMessages=100, kafkaUsername='my-user-1898862452-1098575469', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@a13b108}
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122:my-topic-260837071-1349464842 from pod my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj -n namespace-5 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122 --topic my-topic-260837071-1349464842 --max-messages 100 USER=my_user_1898862452_1098575469
2022-03-31 06:16:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj -n namespace-5 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122 --topic my-topic-260837071-1349464842 --max-messages 100 USER=my_user_1898862452_1098575469
2022-03-31 06:16:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1657165ms till timeout)
2022-03-31 06:16:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1209447ms till timeout)
2022-03-31 06:16:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1656160ms till timeout)
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6d0a54a3, which are set.
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4be5c88f, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --topic, my-topic-606370994-844170518, --max-messages, 100, USER=my_user_1993501230_801238233], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6d0a54a3}
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518 from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233
2022-03-31 06:16:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233
2022-03-31 06:16:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1208443ms till timeout)
2022-03-31 06:16:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1655146ms till timeout)
2022-03-31 06:16:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1207439ms till timeout)
2022-03-31 06:16:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1654138ms till timeout)
2022-03-31 06:16:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1206435ms till timeout)
2022-03-31 06:16:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1653127ms till timeout)
2022-03-31 06:16:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1205430ms till timeout)
2022-03-31 06:16:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6e226f22, which are set.
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@59ae30ee, messages=[], arguments=[--bootstrap-server, my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122, --group-id, my-consumer-group-2081294634, --topic, my-topic-260837071-1349464842, --max-messages, 100, USER=my_user_1898862452_1098575469, --group-instance-id, instance1566630134], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj', podNamespace='namespace-5', bootstrapServer='my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122', topicName='my-topic-260837071-1349464842', maxMessages=100, kafkaUsername='my-user-1898862452-1098575469', consumerGroupName='my-consumer-group-2081294634', consumerInstanceId='instance1566630134', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6e226f22}
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122:my-topic-260837071-1349464842 from pod my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj -n namespace-5 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122 --group-id my-consumer-group-2081294634 --topic my-topic-260837071-1349464842 --max-messages 100 USER=my_user_1898862452_1098575469 --group-instance-id instance1566630134
2022-03-31 06:16:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-0a4e5233-kafka-clients-7c56776c45-bhhdj -n namespace-5 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-0a4e5233-kafka-bootstrap.namespace-5.svc:9122 --group-id my-consumer-group-2081294634 --topic my-topic-260837071-1349464842 --max-messages 100 USER=my_user_1898862452_1098575469 --group-instance-id instance1566630134
2022-03-31 06:16:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1652122ms till timeout)
2022-03-31 06:16:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1204424ms till timeout)
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@10cbe082, which are set.
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2fdb1174, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --group-id, my-consumer-group-132939965, --topic, my-topic-606370994-844170518, --max-messages, 100, USER=my_user_1993501230_801238233, --group-instance-id, instance1806959995], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='my-consumer-group-132939965', consumerInstanceId='instance1806959995', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@10cbe082}
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518 from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-132939965 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance1806959995
2022-03-31 06:16:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-132939965 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance1806959995
2022-03-31 06:16:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1651117ms till timeout)
2022-03-31 06:16:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1203420ms till timeout)
2022-03-31 06:16:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1650114ms till timeout)
2022-03-31 06:16:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1202417ms till timeout)
2022-03-31 06:16:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1649110ms till timeout)
2022-03-31 06:16:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1201412ms till timeout)
2022-03-31 06:16:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1648106ms till timeout)
2022-03-31 06:16:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1200409ms till timeout)
2022-03-31 06:16:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1647102ms till timeout)
2022-03-31 06:16:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1199405ms till timeout)
2022-03-31 06:16:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1646099ms till timeout)
2022-03-31 06:16:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1198401ms till timeout)
2022-03-31 06:16:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1645095ms till timeout)
2022-03-31 06:16:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1197397ms till timeout)
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesCustomListenerTlsScramSha
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1898862452-1098575469 in namespace namespace-5
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1898862452-1098575469
2022-03-31 06:16:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1898862452-1098575469 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-31 06:16:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1644092ms till timeout)
2022-03-31 06:16:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:317] Scale up Kafka to 7
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-012ef870-kafka rolling update
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-012ef870-kafka rolling update
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1799994ms till timeout)
2022-03-31 06:16:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1196393ms till timeout)
2022-03-31 06:16:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1643089ms till timeout)
2022-03-31 06:16:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1195389ms till timeout)
2022-03-31 06:16:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1642085ms till timeout)
2022-03-31 06:16:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1194385ms till timeout)
2022-03-31 06:16:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1641081ms till timeout)
2022-03-31 06:16:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1193381ms till timeout)
2022-03-31 06:16:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1640078ms till timeout)
2022-03-31 06:16:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1192375ms till timeout)
2022-03-31 06:16:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1639074ms till timeout)
2022-03-31 06:16:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1794988ms till timeout)
2022-03-31 06:16:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1191371ms till timeout)
2022-03-31 06:16:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1638068ms till timeout)
2022-03-31 06:16:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1190368ms till timeout)
2022-03-31 06:16:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1637065ms till timeout)
2022-03-31 06:16:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1189364ms till timeout)
2022-03-31 06:16:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1636061ms till timeout)
2022-03-31 06:16:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1188360ms till timeout)
2022-03-31 06:16:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1635056ms till timeout)
2022-03-31 06:16:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1187357ms till timeout)
2022-03-31 06:16:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0a4e5233-kafka-clients in namespace namespace-5
2022-03-31 06:16:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0a4e5233-kafka-clients
2022-03-31 06:16:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0a4e5233-kafka-clients not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-31 06:16:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1634053ms till timeout)
2022-03-31 06:16:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1789981ms till timeout)
2022-03-31 06:16:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1186354ms till timeout)
2022-03-31 06:16:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1633049ms till timeout)
2022-03-31 06:16:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1185350ms till timeout)
2022-03-31 06:16:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1632046ms till timeout)
2022-03-31 06:16:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1184346ms till timeout)
2022-03-31 06:16:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1631042ms till timeout)
2022-03-31 06:16:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1183342ms till timeout)
2022-03-31 06:16:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1630039ms till timeout)
2022-03-31 06:16:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1182338ms till timeout)
2022-03-31 06:16:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1629035ms till timeout)
2022-03-31 06:16:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1784972ms till timeout)
2022-03-31 06:16:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1181329ms till timeout)
2022-03-31 06:16:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1628032ms till timeout)
2022-03-31 06:16:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1180325ms till timeout)
2022-03-31 06:16:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1627028ms till timeout)
2022-03-31 06:16:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1179322ms till timeout)
2022-03-31 06:16:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1626025ms till timeout)
2022-03-31 06:16:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1178318ms till timeout)
2022-03-31 06:16:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1625020ms till timeout)
2022-03-31 06:16:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1177314ms till timeout)
2022-03-31 06:16:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0a4e5233-kafka-clients not ready, will try again in 10000 ms (469937ms till timeout)
2022-03-31 06:16:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1624016ms till timeout)
2022-03-31 06:16:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1779966ms till timeout)
2022-03-31 06:16:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1176311ms till timeout)
2022-03-31 06:16:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1623013ms till timeout)
2022-03-31 06:16:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f88869c9 will have desired state: Ready not ready, will try again in 1000 ms (1175307ms till timeout)
2022-03-31 06:16:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1622009ms till timeout)
2022-03-31 06:16:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f88869c9 is in desired state: Ready
2022-03-31 06:16:49 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:48] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-31 06:16:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:16:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1621005ms till timeout)
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:58] Verifying that Cruise Control REST API is available
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:66] ----> KAFKA REBALANCE <----
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:73] Waiting for CC will have for enough metrics to be recorded to make a proposal 
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for rebalance endpoint is ready
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707410879] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:16:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (599709ms till timeout)
2022-03-31 06:16:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1620002ms till timeout)
2022-03-31 06:16:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1618998ms till timeout)
2022-03-31 06:16:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1774960ms till timeout)
2022-03-31 06:16:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1617995ms till timeout)
2022-03-31 06:16:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1616992ms till timeout)
2022-03-31 06:16:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1615989ms till timeout)
2022-03-31 06:16:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:16:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:16:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:16:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707416145] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:16:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (594445ms till timeout)
2022-03-31 06:16:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1614985ms till timeout)
2022-03-31 06:16:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0a4e5233-kafka-clients not ready, will try again in 10000 ms (459927ms till timeout)
2022-03-31 06:16:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1613982ms till timeout)
2022-03-31 06:16:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:16:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:16:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1769954ms till timeout)
2022-03-31 06:16:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (1612979ms till timeout)
2022-03-31 06:16:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:16:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2def1ad9 is in desired state: Ready
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-332320981-1578568188 in namespace infra-namespace
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-332320981-1578568188
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-332320981-1578568188 will have desired state: Ready
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-332320981-1578568188 will have desired state: Ready
2022-03-31 06:16:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-332320981-1578568188 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-332320981-1578568188 is in desired state: Ready
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-606982444-1208823186 in namespace infra-namespace
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-606982444-1208823186
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-606982444-1208823186 will have desired state: Ready
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-606982444-1208823186 will have desired state: Ready
2022-03-31 06:17:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606982444-1208823186 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:17:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606982444-1208823186 will have desired state: Ready not ready, will try again in 1000 ms (178994ms till timeout)
2022-03-31 06:17:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707421413] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (589171ms till timeout)
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-606982444-1208823186 is in desired state: Ready
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2def1ad9-kafka-clients in namespace infra-namespace
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2def1ad9-kafka-clients will be ready
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-2def1ad9-kafka-clients will be ready
2022-03-31 06:17:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-kafka-clients will be ready not ready, will try again in 1000 ms (479987ms till timeout)
2022-03-31 06:17:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:17:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1764949ms till timeout)
2022-03-31 06:17:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-kafka-clients will be ready not ready, will try again in 1000 ms (478984ms till timeout)
2022-03-31 06:17:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2def1ad9-kafka-clients is ready
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:269] Checking produced and consumed messages to pod:my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2143e41b, which are set.
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@28c9b836, messages=[], arguments=[--bootstrap-server, my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092, --topic, my-topic-606982444-1208823186, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw', podNamespace='namespace-2', bootstrapServer='my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-606982444-1208823186', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2143e41b}
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092:my-topic-606982444-1208823186 from pod my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw -n namespace-2 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092 --topic my-topic-606982444-1208823186 --max-messages 100
2022-03-31 06:17:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw -n namespace-2 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092 --topic my-topic-606982444-1208823186 --max-messages 100
2022-03-31 06:17:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707426716] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (583871ms till timeout)
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@36655e05, which are set.
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3627492b, messages=[], arguments=[--bootstrap-server, my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092, --group-id, my-consumer-group-1826026528, --topic, my-topic-606982444-1208823186, --max-messages, 100, --group-instance-id, instance607045432], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw', podNamespace='namespace-2', bootstrapServer='my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-606982444-1208823186', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1826026528', consumerInstanceId='instance607045432', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@36655e05}
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092#my-topic-606982444-1208823186 from pod my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw -n namespace-2 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092 --group-id my-consumer-group-1826026528 --topic my-topic-606982444-1208823186 --max-messages 100 --group-instance-id instance607045432
2022-03-31 06:17:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw -n namespace-2 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092 --group-id my-consumer-group-1826026528 --topic my-topic-606982444-1208823186 --max-messages 100 --group-instance-id instance607045432
2022-03-31 06:17:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0a4e5233-kafka-clients not ready, will try again in 10000 ms (449915ms till timeout)
2022-03-31 06:17:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1759939ms till timeout)
2022-03-31 06:17:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707431979] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (578603ms till timeout)
2022-03-31 06:17:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1754933ms till timeout)
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:283] Triggering CA cert renewal by adding the annotation
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:295] Patching secret my-cluster-2def1ad9-cluster-ca-cert with strimzi.io/force-renew
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:295] Patching secret my-cluster-2def1ad9-clients-ca-cert with strimzi.io/force-renew
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:300] Wait for zk to rolling restart ...
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2def1ad9-zookeeper rolling update
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-2def1ad9-zookeeper rolling update
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1799995ms till timeout)
2022-03-31 06:17:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-260837071-1349464842 in namespace namespace-5
2022-03-31 06:17:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-260837071-1349464842
2022-03-31 06:17:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-260837071-1349464842 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-31 06:17:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707437255] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (573334ms till timeout)
2022-03-31 06:17:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1749927ms till timeout)
2022-03-31 06:17:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1794989ms till timeout)
2022-03-31 06:17:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1744922ms till timeout)
2022-03-31 06:17:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707442500] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (568085ms till timeout)
2022-03-31 06:17:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1789984ms till timeout)
2022-03-31 06:17:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0a4e5233 in namespace namespace-5
2022-03-31 06:17:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0a4e5233
2022-03-31 06:17:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0a4e5233 not ready, will try again in 10000 ms (839990ms till timeout)
2022-03-31 06:17:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1739916ms till timeout)
2022-03-31 06:17:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707447789] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (562801ms till timeout)
2022-03-31 06:17:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1784978ms till timeout)
2022-03-31 06:17:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1734911ms till timeout)
2022-03-31 06:17:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1779971ms till timeout)
2022-03-31 06:17:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707453121] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (557462ms till timeout)
2022-03-31 06:17:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479931ms till timeout)
2022-03-31 06:17:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:17:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (1729905ms till timeout)
2022-03-31 06:17:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1774966ms till timeout)
2022-03-31 06:17:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478846ms till timeout)
2022-03-31 06:17:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648707458370] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-31 06:17:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (552218ms till timeout)
2022-03-31 06:17:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (477777ms till timeout)
2022-03-31 06:17:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (476699ms till timeout)
2022-03-31 06:17:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (475624ms till timeout)
2022-03-31 06:17:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=5bc4b45f-9f24-42e1-803f-49369bed114c, my-cluster-012ef870-kafka-1=295a8ea3-6c3f-4bf5-a598-c0a889bb0deb, my-cluster-012ef870-kafka-2=8164e982-62d5-40a2-adb2-dd3daa8a1770}
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-012ef870-kafka has been successfully rolled
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-012ef870-kafka to be ready
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-31 06:17:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (474526ms till timeout)
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4199956ms till timeout)
2022-03-31 06:17:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1769960ms till timeout)
2022-03-31 06:17:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4198949ms till timeout)
2022-03-31 06:17:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (473408ms till timeout)
2022-03-31 06:17:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707463695] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:17:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (546881ms till timeout)
2022-03-31 06:17:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4197944ms till timeout)
2022-03-31 06:17:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (472324ms till timeout)
2022-03-31 06:17:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4196939ms till timeout)
2022-03-31 06:17:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (471231ms till timeout)
2022-03-31 06:17:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4195932ms till timeout)
2022-03-31 06:17:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (470151ms till timeout)
2022-03-31 06:17:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4194928ms till timeout)
2022-03-31 06:17:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (469062ms till timeout)
2022-03-31 06:17:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1764955ms till timeout)
2022-03-31 06:17:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4193923ms till timeout)
2022-03-31 06:17:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707468990] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:17:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (541540ms till timeout)
2022-03-31 06:17:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (467966ms till timeout)
2022-03-31 06:17:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4192918ms till timeout)
2022-03-31 06:17:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (466881ms till timeout)
2022-03-31 06:17:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4191914ms till timeout)
2022-03-31 06:17:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (465803ms till timeout)
2022-03-31 06:17:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4190909ms till timeout)
2022-03-31 06:17:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (464729ms till timeout)
2022-03-31 06:17:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4189904ms till timeout)
2022-03-31 06:17:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1759949ms till timeout)
2022-03-31 06:17:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (463628ms till timeout)
2022-03-31 06:17:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4188899ms till timeout)
2022-03-31 06:17:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707474309] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:17:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (536275ms till timeout)
2022-03-31 06:17:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (462545ms till timeout)
2022-03-31 06:17:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4187894ms till timeout)
2022-03-31 06:17:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (461465ms till timeout)
2022-03-31 06:17:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4186888ms till timeout)
2022-03-31 06:17:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4185880ms till timeout)
2022-03-31 06:17:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (460363ms till timeout)
2022-03-31 06:17:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4184875ms till timeout)
2022-03-31 06:17:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459283ms till timeout)
2022-03-31 06:17:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:17:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:17:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1754944ms till timeout)
2022-03-31 06:17:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4183870ms till timeout)
2022-03-31 06:17:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:17:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:17:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (458205ms till timeout)
2022-03-31 06:17:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:17:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707479568] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:17:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (531022ms till timeout)
2022-03-31 06:17:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:17:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4182865ms till timeout)
2022-03-31 06:17:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:17:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:17:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457132ms till timeout)
2022-03-31 06:18:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4181860ms till timeout)
2022-03-31 06:18:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (456058ms till timeout)
2022-03-31 06:18:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4180855ms till timeout)
2022-03-31 06:18:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (454980ms till timeout)
2022-03-31 06:18:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4179849ms till timeout)
2022-03-31 06:18:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:18:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1749938ms till timeout)
2022-03-31 06:18:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (453893ms till timeout)
2022-03-31 06:18:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4178845ms till timeout)
2022-03-31 06:18:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (452809ms till timeout)
2022-03-31 06:18:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4177840ms till timeout)
2022-03-31 06:18:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707485204] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (525374ms till timeout)
2022-03-31 06:18:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (451700ms till timeout)
2022-03-31 06:18:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4176835ms till timeout)
2022-03-31 06:18:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (450619ms till timeout)
2022-03-31 06:18:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4175830ms till timeout)
2022-03-31 06:18:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (449539ms till timeout)
2022-03-31 06:18:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4174824ms till timeout)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:18:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1744932ms till timeout)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (448460ms till timeout)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4173818ms till timeout)
2022-03-31 06:18:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (447363ms till timeout)
2022-03-31 06:18:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4172810ms till timeout)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707490523] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (520062ms till timeout)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4171784ms till timeout)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (446265ms till timeout)
2022-03-31 06:18:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4170777ms till timeout)
2022-03-31 06:18:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (445175ms till timeout)
2022-03-31 06:18:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4169770ms till timeout)
2022-03-31 06:18:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (444045ms till timeout)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:18:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1739925ms till timeout)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4168763ms till timeout)
2022-03-31 06:18:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (442933ms till timeout)
2022-03-31 06:18:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4167757ms till timeout)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (441831ms till timeout)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4166747ms till timeout)
2022-03-31 06:18:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707495909] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (514662ms till timeout)
2022-03-31 06:18:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (440711ms till timeout)
2022-03-31 06:18:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4165736ms till timeout)
2022-03-31 06:18:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (439597ms till timeout)
2022-03-31 06:18:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4164728ms till timeout)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:18:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1734919ms till timeout)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (438490ms till timeout)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4163720ms till timeout)
2022-03-31 06:18:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (437408ms till timeout)
2022-03-31 06:18:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4162714ms till timeout)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesCustomListenerTlsScramSha - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 3
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-FINISHED
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:690] [kafka.listeners.ListenersST - After All] - Clean up after test suite
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context ListenersST is everything deleted.
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st removal
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (479922ms till timeout)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4161708ms till timeout)
2022-03-31 06:18:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707501172] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (509418ms till timeout)
2022-03-31 06:18:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4160698ms till timeout)
2022-03-31 06:18:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (478826ms till timeout)
2022-03-31 06:18:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4159692ms till timeout)
2022-03-31 06:18:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (477751ms till timeout)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:18:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1729914ms till timeout)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4158685ms till timeout)
2022-03-31 06:18:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (476665ms till timeout)
2022-03-31 06:18:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4157679ms till timeout)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (475590ms till timeout)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4156673ms till timeout)
2022-03-31 06:18:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "listeners-st" not found
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:254] ListenersST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST] to and randomly select one to start execution
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:85] [kafka.listeners.ListenersST] - Removing parallel suite: ListenersST
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:89] [kafka.listeners.ListenersST] - Parallel suites count: 4
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 484.638 s - in io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMaker2IsolatedST - Before All] - Setup test suite environment
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:18:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707506415] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (504167ms till timeout)
2022-03-31 06:18:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4155667ms till timeout)
2022-03-31 06:18:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4154661ms till timeout)
2022-03-31 06:18:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-zookeeper-2 hasn't rolled
2022-03-31 06:18:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-zookeeper rolling update not ready, will try again in 5000 ms (1724908ms till timeout)
2022-03-31 06:18:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4153653ms till timeout)
2022-03-31 06:18:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4152646ms till timeout)
2022-03-31 06:18:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-3)
2022-03-31 06:18:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4151640ms till timeout)
2022-03-31 06:18:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707511701] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (498881ms till timeout)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4150631ms till timeout)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4149624ms till timeout)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-zookeeper-0=33eb38f6-8d43-4ebb-8efd-29c53635f829, my-cluster-2def1ad9-zookeeper-1=44fa10b9-fd97-4946-8e2e-d99d9dab3106, my-cluster-2def1ad9-zookeeper-2=0b02ff9e-8e49-4bbb-8924-d9412aed5f33}
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=52494377-3a60-4fe0-a037-f178b5fed978}
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-zookeeper-0=44fd05d1-9a48-4e96-8ca1-2f07b2dbbbc3, my-cluster-2def1ad9-zookeeper-1=d25f6540-ecbc-4458-938d-317a79b4543d, my-cluster-2def1ad9-zookeeper-2=52494377-3a60-4fe0-a037-f178b5fed978}
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2def1ad9-zookeeper has been successfully rolled
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-2def1ad9-zookeeper to be ready
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4148616ms till timeout)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4147609ms till timeout)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797976ms till timeout)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4146601ms till timeout)
2022-03-31 06:18:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796971ms till timeout)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4145591ms till timeout)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707516992] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (493591ms till timeout)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795966ms till timeout)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4144584ms till timeout)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794961ms till timeout)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4143578ms till timeout)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793956ms till timeout)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4142571ms till timeout)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792950ms till timeout)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4141565ms till timeout)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791945ms till timeout)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-3 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-4 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-5 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-6 not ready: kafka)
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2, my-cluster-012ef870-kafka-3, my-cluster-012ef870-kafka-4, my-cluster-012ef870-kafka-5, my-cluster-012ef870-kafka-6 are ready
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-012ef870 is in desired state: Ready
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-012ef870 is ready
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:327] Kafka scale up to 7 finished
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@61942e6e, which are set.
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@455b250c, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --group-id, my-consumer-group-2081267081, --topic, my-topic-606370994-844170518, --max-messages, 100, USER=my_user_1993501230_801238233, --group-instance-id, instance901271310], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='my-consumer-group-2081267081', consumerInstanceId='instance901271310', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@61942e6e}
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518 from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-2081267081 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance901271310
2022-03-31 06:18:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-2081267081 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance901271310
2022-03-31 06:18:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790940ms till timeout)
2022-03-31 06:18:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707522271] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (488314ms till timeout)
2022-03-31 06:18:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789934ms till timeout)
2022-03-31 06:18:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788929ms till timeout)
2022-03-31 06:18:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787924ms till timeout)
2022-03-31 06:18:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786919ms till timeout)
2022-03-31 06:18:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785913ms till timeout)
2022-03-31 06:18:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707527525] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (483049ms till timeout)
2022-03-31 06:18:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784908ms till timeout)
2022-03-31 06:18:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:339] Scale up Zookeeper to 5
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 5 Pod(s) of my-cluster-012ef870-zookeeper to be ready
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2999994ms till timeout)
2022-03-31 06:18:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783903ms till timeout)
2022-03-31 06:18:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2998988ms till timeout)
2022-03-31 06:18:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782898ms till timeout)
2022-03-31 06:18:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2997983ms till timeout)
2022-03-31 06:18:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-zookeeper-2)
2022-03-31 06:18:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781892ms till timeout)
2022-03-31 06:18:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2996978ms till timeout)
2022-03-31 06:18:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780887ms till timeout)
2022-03-31 06:18:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707532806] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (477781ms till timeout)
2022-03-31 06:18:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2995973ms till timeout)
2022-03-31 06:18:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779881ms till timeout)
2022-03-31 06:18:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2994968ms till timeout)
2022-03-31 06:18:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778877ms till timeout)
2022-03-31 06:18:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2993962ms till timeout)
2022-03-31 06:18:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777871ms till timeout)
2022-03-31 06:18:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2992957ms till timeout)
2022-03-31 06:18:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776866ms till timeout)
2022-03-31 06:18:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:18:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2991952ms till timeout)
2022-03-31 06:18:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775861ms till timeout)
2022-03-31 06:18:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:18:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:18:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707538054] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:18:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (472535ms till timeout)
2022-03-31 06:18:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2990947ms till timeout)
2022-03-31 06:18:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774856ms till timeout)
2022-03-31 06:18:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:18:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:18:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:18:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2989941ms till timeout)
2022-03-31 06:18:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:18:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:18:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:18:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:18:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773850ms till timeout)
2022-03-31 06:19:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2988936ms till timeout)
2022-03-31 06:19:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:19:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772845ms till timeout)
2022-03-31 06:19:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2987931ms till timeout)
2022-03-31 06:19:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:19:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-zookeeper, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771840ms till timeout)
2022-03-31 06:19:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2986925ms till timeout)
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-zookeeper-0, my-cluster-2def1ad9-zookeeper-1, my-cluster-2def1ad9-zookeeper-2 are ready
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:304] Wait for kafka to rolling restart ...
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2def1ad9-kafka rolling update
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-2def1ad9-kafka rolling update
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-0 hasn't rolled
2022-03-31 06:19:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1799996ms till timeout)
2022-03-31 06:19:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2985906ms till timeout)
2022-03-31 06:19:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707543299] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (467289ms till timeout)
2022-03-31 06:19:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2984901ms till timeout)
2022-03-31 06:19:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2983896ms till timeout)
2022-03-31 06:19:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2982891ms till timeout)
2022-03-31 06:19:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2981885ms till timeout)
2022-03-31 06:19:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-0 hasn't rolled
2022-03-31 06:19:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1794991ms till timeout)
2022-03-31 06:19:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2980880ms till timeout)
2022-03-31 06:19:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707548549] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (462041ms till timeout)
2022-03-31 06:19:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2979875ms till timeout)
2022-03-31 06:19:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2978869ms till timeout)
2022-03-31 06:19:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2977864ms till timeout)
2022-03-31 06:19:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2976859ms till timeout)
2022-03-31 06:19:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1789985ms till timeout)
2022-03-31 06:19:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2975855ms till timeout)
2022-03-31 06:19:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707553783] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (456805ms till timeout)
2022-03-31 06:19:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-31 06:19:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2974850ms till timeout)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2973843ms till timeout)
2022-03-31 06:19:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2972837ms till timeout)
2022-03-31 06:19:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2971829ms till timeout)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1784980ms till timeout)
2022-03-31 06:19:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2970824ms till timeout)
2022-03-31 06:19:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2969814ms till timeout)
2022-03-31 06:19:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707559260] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (451323ms till timeout)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2968808ms till timeout)
2022-03-31 06:19:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2967803ms till timeout)
2022-03-31 06:19:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2966797ms till timeout)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1779975ms till timeout)
2022-03-31 06:19:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2965792ms till timeout)
2022-03-31 06:19:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2964786ms till timeout)
2022-03-31 06:19:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707564520] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (446067ms till timeout)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2963781ms till timeout)
2022-03-31 06:19:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2962775ms till timeout)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2961770ms till timeout)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1774971ms till timeout)
2022-03-31 06:19:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2960765ms till timeout)
2022-03-31 06:19:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2959760ms till timeout)
2022-03-31 06:19:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707569773] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (440815ms till timeout)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2958754ms till timeout)
2022-03-31 06:19:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2957749ms till timeout)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1769965ms till timeout)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2956745ms till timeout)
2022-03-31 06:19:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2955739ms till timeout)
2022-03-31 06:19:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2954734ms till timeout)
2022-03-31 06:19:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707575034] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (435556ms till timeout)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2953728ms till timeout)
2022-03-31 06:19:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2952722ms till timeout)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1764959ms till timeout)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2951717ms till timeout)
2022-03-31 06:19:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2950712ms till timeout)
2022-03-31 06:19:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2949705ms till timeout)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707580293] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (430292ms till timeout)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2948700ms till timeout)
2022-03-31 06:19:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2947694ms till timeout)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1759954ms till timeout)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-zookeeper-4)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2946687ms till timeout)
2022-03-31 06:19:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2945681ms till timeout)
2022-03-31 06:19:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2944675ms till timeout)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2943670ms till timeout)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707585593] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (424994ms till timeout)
2022-03-31 06:19:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2942663ms till timeout)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1754946ms till timeout)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2941655ms till timeout)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2940649ms till timeout)
2022-03-31 06:19:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2939643ms till timeout)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2938637ms till timeout)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707590837] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (419751ms till timeout)
2022-03-31 06:19:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2937631ms till timeout)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1749941ms till timeout)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-zookeeper, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2936625ms till timeout)
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-0 not ready: zookeeper)
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-1 not ready: zookeeper)
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-2 not ready: zookeeper)
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-3 not ready: zookeeper)
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-zookeeper-4 not ready: zookeeper)
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-zookeeper-0, my-cluster-012ef870-zookeeper-1, my-cluster-012ef870-zookeeper-2, my-cluster-012ef870-zookeeper-3, my-cluster-012ef870-zookeeper-4 are ready
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-012ef870 is in desired state: Ready
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-012ef870 is ready
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:342] Kafka scale up to 5 finished
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f314bba, which are set.
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@34aebffb, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --group-id, my-consumer-group-2137373141, --topic, my-topic-606370994-844170518, --max-messages, 100, USER=my_user_1993501230_801238233, --group-instance-id, instance1330926207], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='my-consumer-group-2137373141', consumerInstanceId='instance1330926207', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f314bba}
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518 from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-2137373141 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance1330926207
2022-03-31 06:19:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-2137373141 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance1330926207
2022-03-31 06:19:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:19:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:19:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707596099] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:19:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (414490ms till timeout)
2022-03-31 06:19:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:19:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:19:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:19:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:19:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1744936ms till timeout)
2022-03-31 06:19:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:19:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:19:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:351] Scale down Kafka to 3
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-012ef870-kafka rolling update
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-012ef870-kafka rolling update
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-3 hasn't rolled
2022-03-31 06:20:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4199995ms till timeout)
2022-03-31 06:20:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707601355] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (409232ms till timeout)
2022-03-31 06:20:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:20:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:20:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:20:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:20:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1739930ms till timeout)
2022-03-31 06:20:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-3 hasn't rolled
2022-03-31 06:20:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4194988ms till timeout)
2022-03-31 06:20:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707606603] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (403986ms till timeout)
2022-03-31 06:20:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:20:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:20:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:20:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2def1ad9-kafka-1 hasn't rolled
2022-03-31 06:20:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-2def1ad9-kafka rolling update not ready, will try again in 5000 ms (1734925ms till timeout)
2022-03-31 06:20:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-3 hasn't rolled
2022-03-31 06:20:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4189982ms till timeout)
2022-03-31 06:20:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707611861] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (398724ms till timeout)
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2def1ad9-kafka-0=517bba58-0724-454b-a4cf-1552d402bef0, my-cluster-2def1ad9-kafka-1=08e69ced-c3e9-4f30-a13c-881ffc602267, my-cluster-2def1ad9-kafka-2=04385f92-8a13-4d00-aadd-aaa6bae85f7f}
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=9688d0e5-29e4-4151-8fd6-88522117f2c0, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2def1ad9-kafka-0=91bb06e7-a769-43f3-9889-48b746e68ea0, my-cluster-2def1ad9-kafka-1=9688d0e5-29e4-4151-8fd6-88522117f2c0, my-cluster-2def1ad9-kafka-2=ec65f7fc-cc71-47c0-bbff-b1faff7ea40b}
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2def1ad9-kafka has been successfully rolled
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-2def1ad9-kafka to be ready
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-31 06:20:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-31 06:20:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-31 06:20:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796980ms till timeout)
2022-03-31 06:20:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0}
2022-03-31 06:20:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0}
2022-03-31 06:20:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-3 hasn't rolled
2022-03-31 06:20:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4184976ms till timeout)
2022-03-31 06:20:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795974ms till timeout)
2022-03-31 06:20:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707617116] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (393472ms till timeout)
2022-03-31 06:20:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794969ms till timeout)
2022-03-31 06:20:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793964ms till timeout)
2022-03-31 06:20:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792954ms till timeout)
2022-03-31 06:20:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791948ms till timeout)
2022-03-31 06:20:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29}
2022-03-31 06:20:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29}
2022-03-31 06:20:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-3 hasn't rolled
2022-03-31 06:20:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4179969ms till timeout)
2022-03-31 06:20:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790942ms till timeout)
2022-03-31 06:20:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707622364] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (388222ms till timeout)
2022-03-31 06:20:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789937ms till timeout)
2022-03-31 06:20:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788932ms till timeout)
2022-03-31 06:20:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787927ms till timeout)
2022-03-31 06:20:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786921ms till timeout)
2022-03-31 06:20:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:20:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4174964ms till timeout)
2022-03-31 06:20:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785916ms till timeout)
2022-03-31 06:20:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784912ms till timeout)
2022-03-31 06:20:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707627783] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (382807ms till timeout)
2022-03-31 06:20:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783907ms till timeout)
2022-03-31 06:20:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2def1ad9-kafka-1)
2022-03-31 06:20:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782902ms till timeout)
2022-03-31 06:20:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781897ms till timeout)
2022-03-31 06:20:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-1 hasn't rolled
2022-03-31 06:20:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4169958ms till timeout)
2022-03-31 06:20:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780891ms till timeout)
2022-03-31 06:20:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779886ms till timeout)
2022-03-31 06:20:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707633036] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (377550ms till timeout)
2022-03-31 06:20:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778880ms till timeout)
2022-03-31 06:20:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777875ms till timeout)
2022-03-31 06:20:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776870ms till timeout)
2022-03-31 06:20:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:20:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4164951ms till timeout)
2022-03-31 06:20:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775863ms till timeout)
2022-03-31 06:20:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774857ms till timeout)
2022-03-31 06:20:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707638335] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (372251ms till timeout)
2022-03-31 06:20:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773852ms till timeout)
2022-03-31 06:20:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2def1ad9-kafka, strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772847ms till timeout)
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-0 not ready: kafka)
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-1 not ready: kafka)
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-2 not ready: kafka)
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-0, my-cluster-2def1ad9-kafka-1, my-cluster-2def1ad9-kafka-2 are ready
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:308] Wait for EO to rolling restart ...
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-2def1ad9-entity-operator rolling update
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-2def1ad9-entity-operator rolling update in namespace:namespace-2
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-entity-operator-56f5bc7f69-mqn2r=39c9bdc3-fa44-4994-ad96-11ca8bc4479a}
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk=be5d3abc-c278-4aff-ba72-15a62455e8ff}
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready
2022-03-31 06:20:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-31 06:20:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:20:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4159945ms till timeout)
2022-03-31 06:20:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-31 06:20:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-31 06:20:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-31 06:20:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707643594] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (366996ms till timeout)
2022-03-31 06:20:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-31 06:20:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (474981ms till timeout)
2022-03-31 06:20:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:20:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4154940ms till timeout)
2022-03-31 06:20:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-31 06:20:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (472973ms till timeout)
2022-03-31 06:20:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-31 06:20:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707648836] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (361754ms till timeout)
2022-03-31 06:20:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (470966ms till timeout)
2022-03-31 06:20:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (469963ms till timeout)
2022-03-31 06:20:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:20:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4149935ms till timeout)
2022-03-31 06:20:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (468959ms till timeout)
2022-03-31 06:20:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (467956ms till timeout)
2022-03-31 06:20:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (466952ms till timeout)
2022-03-31 06:20:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707654081] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (356504ms till timeout)
2022-03-31 06:20:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (465949ms till timeout)
2022-03-31 06:20:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (464946ms till timeout)
2022-03-31 06:20:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:20:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:20:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:20:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4144929ms till timeout)
2022-03-31 06:20:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:20:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (463942ms till timeout)
2022-03-31 06:20:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (462938ms till timeout)
2022-03-31 06:20:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (461935ms till timeout)
2022-03-31 06:20:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:20:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:20:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:20:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:20:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707659327] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:20:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (351265ms till timeout)
2022-03-31 06:20:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (460931ms till timeout)
2022-03-31 06:21:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (459927ms till timeout)
2022-03-31 06:21:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4139925ms till timeout)
2022-03-31 06:21:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (458923ms till timeout)
2022-03-31 06:21:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (457920ms till timeout)
2022-03-31 06:21:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (456916ms till timeout)
2022-03-31 06:21:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (455913ms till timeout)
2022-03-31 06:21:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707664569] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (346022ms till timeout)
2022-03-31 06:21:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (454909ms till timeout)
2022-03-31 06:21:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4134920ms till timeout)
2022-03-31 06:21:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (453906ms till timeout)
2022-03-31 06:21:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (452902ms till timeout)
2022-03-31 06:21:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (451899ms till timeout)
2022-03-31 06:21:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (450896ms till timeout)
2022-03-31 06:21:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707669838] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (340752ms till timeout)
2022-03-31 06:21:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (449892ms till timeout)
2022-03-31 06:21:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4129912ms till timeout)
2022-03-31 06:21:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (448888ms till timeout)
2022-03-31 06:21:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (447885ms till timeout)
2022-03-31 06:21:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (446881ms till timeout)
2022-03-31 06:21:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (445877ms till timeout)
2022-03-31 06:21:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707675076] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (335507ms till timeout)
2022-03-31 06:21:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (444874ms till timeout)
2022-03-31 06:21:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4124907ms till timeout)
2022-03-31 06:21:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (443869ms till timeout)
2022-03-31 06:21:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (442865ms till timeout)
2022-03-31 06:21:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (441862ms till timeout)
2022-03-31 06:21:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (440858ms till timeout)
2022-03-31 06:21:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707680339] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (330242ms till timeout)
2022-03-31 06:21:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4119902ms till timeout)
2022-03-31 06:21:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (439853ms till timeout)
2022-03-31 06:21:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (438849ms till timeout)
2022-03-31 06:21:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (437845ms till timeout)
2022-03-31 06:21:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (436841ms till timeout)
2022-03-31 06:21:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (435838ms till timeout)
2022-03-31 06:21:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707685592] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (324995ms till timeout)
2022-03-31 06:21:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4114897ms till timeout)
2022-03-31 06:21:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (434835ms till timeout)
2022-03-31 06:21:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (433831ms till timeout)
2022-03-31 06:21:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (432828ms till timeout)
2022-03-31 06:21:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (431824ms till timeout)
2022-03-31 06:21:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (430821ms till timeout)
2022-03-31 06:21:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4}
2022-03-31 06:21:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-2 hasn't rolled
2022-03-31 06:21:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4109892ms till timeout)
2022-03-31 06:21:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (429817ms till timeout)
2022-03-31 06:21:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707690854] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (319736ms till timeout)
2022-03-31 06:21:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (428814ms till timeout)
2022-03-31 06:21:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (427811ms till timeout)
2022-03-31 06:21:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (426808ms till timeout)
2022-03-31 06:21:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (425805ms till timeout)
2022-03-31 06:21:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-0 hasn't rolled
2022-03-31 06:21:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4104887ms till timeout)
2022-03-31 06:21:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (424802ms till timeout)
2022-03-31 06:21:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707696098] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (314489ms till timeout)
2022-03-31 06:21:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (423796ms till timeout)
2022-03-31 06:21:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (422793ms till timeout)
2022-03-31 06:21:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (421789ms till timeout)
2022-03-31 06:21:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (420786ms till timeout)
2022-03-31 06:21:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-0 hasn't rolled
2022-03-31 06:21:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4099882ms till timeout)
2022-03-31 06:21:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (419782ms till timeout)
2022-03-31 06:21:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648707701339] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-31 06:21:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (309247ms till timeout)
2022-03-31 06:21:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (418779ms till timeout)
2022-03-31 06:21:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (417775ms till timeout)
2022-03-31 06:21:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (416772ms till timeout)
2022-03-31 06:21:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (415769ms till timeout)
2022-03-31 06:21:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-0 hasn't rolled
2022-03-31 06:21:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4094876ms till timeout)
2022-03-31 06:21:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (414766ms till timeout)
2022-03-31 06:21:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response 

Optimization has 19 inter-broker replica(0 MB) moves, 0 intra-broker replica(0 MB) moves and 18 leadership moves with a cluster model of 1 recent windows and 100.000% of the partitions covered.
Excluded Topics: [].
Excluded Brokers For Leadership: [].
Excluded Brokers For Replica Move: [].
Counts: 3 brokers 287 replicas 6 topics.
On-demand Balancedness Score Before (87.919) After(87.919).
Provision Status: RIGHT_SIZED.

[     2 ms] Stats for RackAwareGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     0 ms] Stats for MinTopicLeadersPerBrokerGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for ReplicaCapacityGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for DiskCapacityGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkInboundCapacityGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkOutboundCapacityGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for CpuCapacityGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     0 ms] Stats for ReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     0 ms] Stats for PotentialNwOutGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     0 ms] Stats for DiskUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkInboundUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.702 networkInbound:       0.661 networkOutbound:       1.320 disk:       0.026 potentialNwOut:       1.507 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.413 networkInbound:       0.622 networkOutbound:       0.139 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.550 networkInbound:       0.016 networkOutbound:       0.553 disk:       0.001 potentialNwOut:       0.016 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     8 ms] Stats for NetworkOutboundUsageDistributionGoal(VIOLATED):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.321 networkInbound:       0.663 networkOutbound:       0.686 disk:       0.026 potentialNwOut:       1.509 replicas:96 leaderReplicas:58 topicReplicas:50}
MIN:{cpu:       0.948 networkInbound:       0.622 networkOutbound:       0.438 disk:       0.025 potentialNwOut:       1.468 replicas:95 leaderReplicas:12 topicReplicas:1}
STD:{cpu:       0.588 networkInbound:       0.017 networkOutbound:       0.107 disk:       0.001 potentialNwOut:       0.017 replicas:0.4714045207910317 leaderReplicas:19.612920911140865 topicReplicas:0.15713484026367722

[     7 ms] Stats for CpuUsageDistributionGoal(VIOLATED):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.169 networkInbound:       0.676 networkOutbound:       0.686 disk:       0.027 potentialNwOut:       1.522 replicas:99 leaderReplicas:58 topicReplicas:50}
MIN:{cpu:       1.040 networkInbound:       0.587 networkOutbound:       0.438 disk:       0.024 potentialNwOut:       1.433 replicas:92 leaderReplicas:12 topicReplicas:1}
STD:{cpu:       0.480 networkInbound:       0.040 networkOutbound:       0.107 disk:       0.002 potentialNwOut:       0.040 replicas:2.8674417556808756 leaderReplicas:19.612920911140865 topicReplicas:1.0919528647889454

[     0 ms] Stats for TopicReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.169 networkInbound:       0.676 networkOutbound:       0.686 disk:       0.027 potentialNwOut:       1.522 replicas:99 leaderReplicas:58 topicReplicas:50}
MIN:{cpu:       1.040 networkInbound:       0.587 networkOutbound:       0.438 disk:       0.024 potentialNwOut:       1.433 replicas:92 leaderReplicas:12 topicReplicas:1}
STD:{cpu:       0.480 networkInbound:       0.040 networkOutbound:       0.107 disk:       0.002 potentialNwOut:       0.040 replicas:2.8674417556808756 leaderReplicas:19.612920911140865 topicReplicas:1.0919528647889454

[     4 ms] Stats for LeaderReplicaDistributionGoal(VIOLATED):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.169 networkInbound:       0.676 networkOutbound:       0.686 disk:       0.027 potentialNwOut:       1.522 replicas:99 leaderReplicas:58 topicReplicas:50}
MIN:{cpu:       1.040 networkInbound:       0.587 networkOutbound:       0.438 disk:       0.024 potentialNwOut:       1.433 replicas:92 leaderReplicas:12 topicReplicas:1}
STD:{cpu:       0.480 networkInbound:       0.040 networkOutbound:       0.107 disk:       0.002 potentialNwOut:       0.040 replicas:2.8674417556808756 leaderReplicas:19.612920911140865 topicReplicas:1.0919528647889454

[     2 ms] Stats for LeaderBytesInDistributionGoal(VIOLATED):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.169 networkInbound:       0.676 networkOutbound:       0.686 disk:       0.027 potentialNwOut:       1.522 replicas:99 leaderReplicas:58 topicReplicas:50}
MIN:{cpu:       1.040 networkInbound:       0.587 networkOutbound:       0.438 disk:       0.024 potentialNwOut:       1.433 replicas:92 leaderReplicas:12 topicReplicas:1}
STD:{cpu:       0.480 networkInbound:       0.040 networkOutbound:       0.107 disk:       0.002 potentialNwOut:       0.040 replicas:2.8674417556808756 leaderReplicas:19.612920911140865 topicReplicas:1.0919528647889454

[     0 ms] Stats for PreferredLeaderElectionGoal(VIOLATED):
AVG:{cpu:       1.509 networkInbound:       0.643 networkOutbound:       0.538 disk:       0.026 potentialNwOut:       1.489 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.169 networkInbound:       0.676 networkOutbound:       0.686 disk:       0.027 potentialNwOut:       1.522 replicas:99 leaderReplicas:58 topicReplicas:50}
MIN:{cpu:       1.040 networkInbound:       0.587 networkOutbound:       0.438 disk:       0.024 potentialNwOut:       1.433 replicas:92 leaderReplicas:12 topicReplicas:1}
STD:{cpu:       0.480 networkInbound:       0.040 networkOutbound:       0.107 disk:       0.002 potentialNwOut:       0.040 replicas:2.8674417556808756 leaderReplicas:19.612920911140865 topicReplicas:1.0919528647889454

Cluster load after rebalance:


                                                                         HOST         BROKER                                                                         RACK         DISK_CAP(MB)            DISK(MB)/_(%)_            CORE_NUM         CPU(%)          NW_IN_CAP(KB/s)       LEADER_NW_IN(KB/s)     FOLLOWER_NW_IN(KB/s)         NW_OUT_CAP(KB/s)        NW_OUT(KB/s)       PNW_OUT(KB/s)    LEADERS/REPLICAS
my-cluster-f88869c9-kafka-0.my-cluster-f88869c9-kafka-brokers.namespace-4.svc,             0,my-cluster-f88869c9-kafka-0.my-cluster-f88869c9-kafka-brokers.namespace-4.svc,          100000.000,              0.026/00.00,                  1,         1.320,               10000.000,                   0.333,                   0.342,               10000.000,              0.489,              1.522,            58/99
my-cluster-f88869c9-kafka-1.my-cluster-f88869c9-kafka-brokers.namespace-4.svc,             1,my-cluster-f88869c9-kafka-1.my-cluster-f88869c9-kafka-brokers.namespace-4.svc,          100000.000,              0.027/00.00,                  1,         1.040,               10000.000,                   0.281,                   0.385,               10000.000,              0.438,              1.512,            47/96
my-cluster-f88869c9-kafka-2.my-cluster-f88869c9-kafka-brokers.namespace-4.svc,             2,my-cluster-f88869c9-kafka-2.my-cluster-f88869c9-kafka-brokers.namespace-4.svc,          100000.000,              0.024/00.00,                  1,         2.169,               10000.000,                   0.153,                   0.434,               10000.000,              0.686,              1.433,            12/92

2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (413757ms till timeout)
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:97] ----> EXECUTION OF STOP PROPOSAL <----
2022-03-31 06:21:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-31 06:21:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:108] ----> USER TASKS <----
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-31 06:21:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (412754ms till timeout)
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:126] Verifying that Cruise Control REST API doesn't allow HTTP requests
2022-03-31 06:21:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:132] Verifying that Cruise Control REST API doesn't allow unauthenticated requests
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 exec my-cluster-f88869c9-cruise-control-85786ccc6c-rfmqd -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequests
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f88869c9 in namespace namespace-4
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-f88869c9
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f88869c9
2022-03-31 06:21:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f88869c9 not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-31 06:21:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (411739ms till timeout)
2022-03-31 06:21:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (410734ms till timeout)
2022-03-31 06:21:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-0 hasn't rolled
2022-03-31 06:21:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4089870ms till timeout)
2022-03-31 06:21:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (409730ms till timeout)
2022-03-31 06:21:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (408726ms till timeout)
2022-03-31 06:21:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (407721ms till timeout)
2022-03-31 06:21:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (406718ms till timeout)
2022-03-31 06:21:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (405715ms till timeout)
2022-03-31 06:21:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:21:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:21:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-0 hasn't rolled
2022-03-31 06:21:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4084865ms till timeout)
2022-03-31 06:21:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (404709ms till timeout)
2022-03-31 06:21:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:21:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (403706ms till timeout)
2022-03-31 06:21:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (402702ms till timeout)
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testCruiseControlBasicAPIRequests
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (479915ms till timeout)
2022-03-31 06:21:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (401693ms till timeout)
2022-03-31 06:21:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:21:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:21:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:21:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:21:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:21:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (478834ms till timeout)
2022-03-31 06:21:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (400690ms till timeout)
2022-03-31 06:22:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (477754ms till timeout)
2022-03-31 06:22:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:22:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:22:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:22:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-012ef870-kafka-0 hasn't rolled
2022-03-31 06:22:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] component with name my-cluster-012ef870-kafka rolling update not ready, will try again in 5000 ms (4079860ms till timeout)
2022-03-31 06:22:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (399686ms till timeout)
2022-03-31 06:22:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (476659ms till timeout)
2022-03-31 06:22:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-entity-operator will be ready not ready, will try again in 1000 ms (398683ms till timeout)
2022-03-31 06:22:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2def1ad9-entity-operator is ready
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599996ms till timeout)
2022-03-31 06:22:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (475573ms till timeout)
2022-03-31 06:22:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-31 06:22:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474499ms till timeout)
2022-03-31 06:22:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597987ms till timeout)
2022-03-31 06:22:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (473411ms till timeout)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-012ef870-kafka-0=0f5750fa-9c08-400b-9fe7-b058ca3f70b4, my-cluster-012ef870-kafka-1=d9ee2709-27f8-42ad-a5bb-8955aab0d969, my-cluster-012ef870-kafka-2=d138ebd0-d2f9-4e74-b5c4-400c00dca0a4, my-cluster-012ef870-kafka-3=5ed1d7f5-7b7c-4d68-9c2c-177a1a750c29, my-cluster-012ef870-kafka-4=0d618229-25e8-43b4-9ff7-d795f8c609b0, my-cluster-012ef870-kafka-5=1b6ccde2-5802-4f53-98d6-37eb9fef74e9, my-cluster-012ef870-kafka-6=c6c3cbc3-6b40-4d61-bca9-2cb756010adc}
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-012ef870-kafka-0=4d794983-e27d-498c-9993-8bf13a1f1c32, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-012ef870-kafka-0=4d794983-e27d-498c-9993-8bf13a1f1c32, my-cluster-012ef870-kafka-1=93717e3a-a2a9-4111-adbb-0628b147647c, my-cluster-012ef870-kafka-2=ad3a2984-f7ec-4eb2-b626-461d36e18154}
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-012ef870-kafka has been successfully rolled
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-012ef870-kafka to be ready
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596981ms till timeout)
2022-03-31 06:22:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472337ms till timeout)
2022-03-31 06:22:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-31 06:22:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595976ms till timeout)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471237ms till timeout)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594971ms till timeout)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (470156ms till timeout)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796981ms till timeout)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593959ms till timeout)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (469079ms till timeout)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795976ms till timeout)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592954ms till timeout)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (467989ms till timeout)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794971ms till timeout)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591949ms till timeout)
2022-03-31 06:22:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (466907ms till timeout)
2022-03-31 06:22:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793966ms till timeout)
2022-03-31 06:22:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590944ms till timeout)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (465818ms till timeout)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792960ms till timeout)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: tls-sidecar)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: topic-operator)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk not ready: user-operator)
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-entity-operator-6fdcb4f4f6-5nrhk are ready
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-2def1ad9-entity-operator rolling update finished
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:312] Wait for CC and KE to rolling restart ...
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-2def1ad9-kafka-exporter rolling update
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (599994ms till timeout)
2022-03-31 06:22:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (464726ms till timeout)
2022-03-31 06:22:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791953ms till timeout)
2022-03-31 06:22:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790939ms till timeout)
2022-03-31 06:22:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (463645ms till timeout)
2022-03-31 06:22:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789934ms till timeout)
2022-03-31 06:22:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (462550ms till timeout)
2022-03-31 06:22:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788930ms till timeout)
2022-03-31 06:22:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (461473ms till timeout)
2022-03-31 06:22:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787925ms till timeout)
2022-03-31 06:22:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (594987ms till timeout)
2022-03-31 06:22:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (460396ms till timeout)
2022-03-31 06:22:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786921ms till timeout)
2022-03-31 06:22:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (459306ms till timeout)
2022-03-31 06:22:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785916ms till timeout)
2022-03-31 06:22:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (458225ms till timeout)
2022-03-31 06:22:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784912ms till timeout)
2022-03-31 06:22:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (457147ms till timeout)
2022-03-31 06:22:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783908ms till timeout)
2022-03-31 06:22:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (456068ms till timeout)
2022-03-31 06:22:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782903ms till timeout)
2022-03-31 06:22:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (589980ms till timeout)
2022-03-31 06:22:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (454990ms till timeout)
2022-03-31 06:22:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781898ms till timeout)
2022-03-31 06:22:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (453918ms till timeout)
2022-03-31 06:22:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780894ms till timeout)
2022-03-31 06:22:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (452849ms till timeout)
2022-03-31 06:22:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-012ef870-kafka-0)
2022-03-31 06:22:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779889ms till timeout)
2022-03-31 06:22:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (451767ms till timeout)
2022-03-31 06:22:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778881ms till timeout)
2022-03-31 06:22:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (450675ms till timeout)
2022-03-31 06:22:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777876ms till timeout)
2022-03-31 06:22:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (584974ms till timeout)
2022-03-31 06:22:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776871ms till timeout)
2022-03-31 06:22:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (449600ms till timeout)
2022-03-31 06:22:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775866ms till timeout)
2022-03-31 06:22:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (448530ms till timeout)
2022-03-31 06:22:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774861ms till timeout)
2022-03-31 06:22:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (447453ms till timeout)
2022-03-31 06:22:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773854ms till timeout)
2022-03-31 06:22:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (446364ms till timeout)
2022-03-31 06:22:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772849ms till timeout)
2022-03-31 06:22:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (579967ms till timeout)
2022-03-31 06:22:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (445285ms till timeout)
2022-03-31 06:22:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771845ms till timeout)
2022-03-31 06:22:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (444217ms till timeout)
2022-03-31 06:22:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770840ms till timeout)
2022-03-31 06:22:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (443130ms till timeout)
2022-03-31 06:22:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-012ef870-kafka, strimzi.io/cluster=my-cluster-012ef870, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769834ms till timeout)
2022-03-31 06:22:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (442049ms till timeout)
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-0 not ready: kafka)
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-1 not ready: kafka)
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-012ef870-kafka-2 not ready: kafka)
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods my-cluster-012ef870-kafka-0, my-cluster-012ef870-kafka-1, my-cluster-012ef870-kafka-2 are ready
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-012ef870 will have desired state: Ready
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-012ef870 is in desired state: Ready
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-012ef870 is ready
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateST:356] Kafka scale down to 3 finished
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@13fa2da3, which are set.
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@519146fd, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --group-id, my-consumer-group-1072662256, --topic, my-topic-606370994-844170518, --max-messages, 100, USER=my_user_1993501230_801238233, --group-instance-id, instance413032876], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='my-consumer-group-1072662256', consumerInstanceId='instance413032876', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@13fa2da3}
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518 from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-1072662256 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance413032876
2022-03-31 06:22:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-1072662256 --topic my-topic-606370994-844170518 --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance413032876
2022-03-31 06:22:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (440952ms till timeout)
2022-03-31 06:22:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (574960ms till timeout)
2022-03-31 06:22:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (439872ms till timeout)
2022-03-31 06:22:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (438793ms till timeout)
2022-03-31 06:22:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (437720ms till timeout)
2022-03-31 06:22:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequests - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequests
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 2
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-FINISHED
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlApiST - After All] - Clean up after test suite
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context CruiseControlApiST is everything deleted.
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st removal
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (479928ms till timeout)
2022-03-31 06:22:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (478860ms till timeout)
2022-03-31 06:22:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (569954ms till timeout)
2022-03-31 06:22:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-606370994-844170518-new in namespace infra-namespace
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-606370994-844170518-new
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready
2022-03-31 06:22:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:22:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (477773ms till timeout)
2022-03-31 06:22:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (178995ms till timeout)
2022-03-31 06:22:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (476699ms till timeout)
2022-03-31 06:22:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (177992ms till timeout)
2022-03-31 06:22:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-31 06:22:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:22:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (475616ms till timeout)
2022-03-31 06:22:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-31 06:22:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (176990ms till timeout)
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-api-st" not found
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-3], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:254] CruiseControlApiST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST] to and randomly select one to start execution
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlApiST] - Removing parallel suite: CruiseControlApiST
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 3
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 745.654 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMakerIsolatedST - Before All] - Setup test suite environment
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:22:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (175986ms till timeout)
2022-03-31 06:22:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (564947ms till timeout)
2022-03-31 06:22:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (174983ms till timeout)
2022-03-31 06:22:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (173980ms till timeout)
2022-03-31 06:22:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (172976ms till timeout)
2022-03-31 06:22:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:22:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (171973ms till timeout)
2022-03-31 06:22:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (170970ms till timeout)
2022-03-31 06:22:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (559940ms till timeout)
2022-03-31 06:22:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (169967ms till timeout)
2022-03-31 06:22:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (168964ms till timeout)
2022-03-31 06:22:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (167961ms till timeout)
2022-03-31 06:22:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:22:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (166958ms till timeout)
2022-03-31 06:22:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (165954ms till timeout)
2022-03-31 06:22:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:22:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (554933ms till timeout)
2022-03-31 06:22:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:22:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:22:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (164951ms till timeout)
2022-03-31 06:22:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (163948ms till timeout)
2022-03-31 06:23:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (162945ms till timeout)
2022-03-31 06:23:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (161942ms till timeout)
2022-03-31 06:23:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (160939ms till timeout)
2022-03-31 06:23:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (549927ms till timeout)
2022-03-31 06:23:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (159936ms till timeout)
2022-03-31 06:23:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (158934ms till timeout)
2022-03-31 06:23:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (157930ms till timeout)
2022-03-31 06:23:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (156927ms till timeout)
2022-03-31 06:23:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (155925ms till timeout)
2022-03-31 06:23:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2, my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-2def1ad9-kafka-exporter rolling update in namespace:namespace-2 not ready, will try again in 5000 ms (544918ms till timeout)
2022-03-31 06:23:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (154921ms till timeout)
2022-03-31 06:23:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (153918ms till timeout)
2022-03-31 06:23:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (152915ms till timeout)
2022-03-31 06:23:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (151912ms till timeout)
2022-03-31 06:23:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (150909ms till timeout)
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-2def1ad9-kafka-exporter-7fdfd7d56f-x2q2b=2a93a804-ff47-4109-a7dc-1e73eec699a9}
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m=5b39eca7-ec71-40b8-8f8d-8c3eaa162bf2}
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2def1ad9-kafka-exporter will be ready
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-2def1ad9-kafka-exporter will be ready
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2def1ad9-kafka-exporter is ready
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 06:23:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (149906ms till timeout)
2022-03-31 06:23:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:23:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (148903ms till timeout)
2022-03-31 06:23:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-31 06:23:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (147900ms till timeout)
2022-03-31 06:23:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-31 06:23:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (146897ms till timeout)
2022-03-31 06:23:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595981ms till timeout)
2022-03-31 06:23:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (145893ms till timeout)
2022-03-31 06:23:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594977ms till timeout)
2022-03-31 06:23:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (144890ms till timeout)
2022-03-31 06:23:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593973ms till timeout)
2022-03-31 06:23:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (143887ms till timeout)
2022-03-31 06:23:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592969ms till timeout)
2022-03-31 06:23:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (142884ms till timeout)
2022-03-31 06:23:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591965ms till timeout)
2022-03-31 06:23:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (141881ms till timeout)
2022-03-31 06:23:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-2def1ad9, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-2def1ad9-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590960ms till timeout)
2022-03-31 06:23:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (140878ms till timeout)
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m not ready: my-cluster-2def1ad9-kafka-exporter)
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2def1ad9-kafka-exporter-7b67496b6-mck4m are ready
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-2def1ad9-kafka-exporter rolling update finished
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:317] Checking the certificates have been replaced
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:331] Checking consumed messages to pod:my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3cadd5d1, which are set.
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4861220c, messages=[], arguments=[--bootstrap-server, my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092, --group-id, my-consumer-group-949479313, --topic, my-topic-606982444-1208823186, --max-messages, 100, --group-instance-id, instance1194770627], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw', podNamespace='namespace-2', bootstrapServer='my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-606982444-1208823186', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-949479313', consumerInstanceId='instance1194770627', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3cadd5d1}
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092#my-topic-606982444-1208823186 from pod my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw -n namespace-2 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092 --group-id my-consumer-group-949479313 --topic my-topic-606982444-1208823186 --max-messages 100 --group-instance-id instance1194770627
2022-03-31 06:23:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2def1ad9-kafka-clients-569db6876b-x84cw -n namespace-2 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9092 --group-id my-consumer-group-949479313 --topic my-topic-606982444-1208823186 --max-messages 100 --group-instance-id instance1194770627
2022-03-31 06:23:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (139875ms till timeout)
2022-03-31 06:23:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (138872ms till timeout)
2022-03-31 06:23:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (137868ms till timeout)
2022-03-31 06:23:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (136865ms till timeout)
2022-03-31 06:23:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (135862ms till timeout)
2022-03-31 06:23:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-2def1ad9 in namespace infra-namespace
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:bob-my-cluster-2def1ad9
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-2def1ad9 will have desired state: Ready
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: bob-my-cluster-2def1ad9 will have desired state: Ready
2022-03-31 06:23:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: bob-my-cluster-2def1ad9 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:23:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (134859ms till timeout)
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-2def1ad9 is in desired state: Ready
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2def1ad9-kafka-clients-tls in namespace namespace-2
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients-tls
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2def1ad9-kafka-clients-tls will be ready
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-2def1ad9-kafka-clients-tls will be ready
2022-03-31 06:23:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-kafka-clients-tls will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-31 06:23:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (133855ms till timeout)
2022-03-31 06:23:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-2def1ad9-kafka-clients-tls will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-31 06:23:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (132852ms till timeout)
2022-03-31 06:23:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2def1ad9-kafka-clients-tls is ready
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:355] Checking consumed messages to pod:my-cluster-2def1ad9-kafka-clients-tls-7c8bd6b5cc-7drmb
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@30c8742d, which are set.
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@680b04bb, messages=[], arguments=[--bootstrap-server, my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9093, --group-id, my-consumer-group-495975392, --topic, my-topic-606982444-1208823186, --max-messages, 100, USER=bob_my_cluster_2def1ad9, --group-instance-id, instance995658884], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2def1ad9-kafka-clients-tls-7c8bd6b5cc-7drmb', podNamespace='namespace-2', bootstrapServer='my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9093', topicName='my-topic-606982444-1208823186', maxMessages=100, kafkaUsername='bob-my-cluster-2def1ad9', consumerGroupName='my-consumer-group-495975392', consumerInstanceId='instance995658884', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@30c8742d}
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9093#my-topic-606982444-1208823186 from pod my-cluster-2def1ad9-kafka-clients-tls-7c8bd6b5cc-7drmb
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2def1ad9-kafka-clients-tls-7c8bd6b5cc-7drmb -n namespace-2 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9093 --group-id my-consumer-group-495975392 --topic my-topic-606982444-1208823186 --max-messages 100 USER=bob_my_cluster_2def1ad9 --group-instance-id instance995658884
2022-03-31 06:23:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2def1ad9-kafka-clients-tls-7c8bd6b5cc-7drmb -n namespace-2 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-2def1ad9-kafka-bootstrap.namespace-2.svc:9093 --group-id my-consumer-group-495975392 --topic my-topic-606982444-1208823186 --max-messages 100 USER=bob_my_cluster_2def1ad9 --group-instance-id instance995658884
2022-03-31 06:23:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (131849ms till timeout)
2022-03-31 06:23:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (130846ms till timeout)
2022-03-31 06:23:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (129842ms till timeout)
2022-03-31 06:23:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (128839ms till timeout)
2022-03-31 06:23:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (127836ms till timeout)
2022-03-31 06:23:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (126832ms till timeout)
2022-03-31 06:23:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (125829ms till timeout)
2022-03-31 06:23:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:675] [security.SecurityST - After Each] - Clean up after test
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2def1ad9-kafka-clients in namespace namespace-2
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients
2022-03-31 06:23:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-31 06:23:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (124826ms till timeout)
2022-03-31 06:23:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (123823ms till timeout)
2022-03-31 06:23:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (122820ms till timeout)
2022-03-31 06:23:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (121817ms till timeout)
2022-03-31 06:23:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (120814ms till timeout)
2022-03-31 06:23:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (119811ms till timeout)
2022-03-31 06:23:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (118808ms till timeout)
2022-03-31 06:23:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (117805ms till timeout)
2022-03-31 06:23:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (116802ms till timeout)
2022-03-31 06:23:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (115798ms till timeout)
2022-03-31 06:23:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (469982ms till timeout)
2022-03-31 06:23:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (114796ms till timeout)
2022-03-31 06:23:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (113793ms till timeout)
2022-03-31 06:23:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (112790ms till timeout)
2022-03-31 06:23:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (111787ms till timeout)
2022-03-31 06:23:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (110784ms till timeout)
2022-03-31 06:23:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (109780ms till timeout)
2022-03-31 06:23:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (108778ms till timeout)
2022-03-31 06:23:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (107775ms till timeout)
2022-03-31 06:23:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:23:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (106772ms till timeout)
2022-03-31 06:23:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (105769ms till timeout)
2022-03-31 06:23:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:23:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:23:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (459973ms till timeout)
2022-03-31 06:23:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (104766ms till timeout)
2022-03-31 06:24:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (103763ms till timeout)
2022-03-31 06:24:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (102759ms till timeout)
2022-03-31 06:24:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (101757ms till timeout)
2022-03-31 06:24:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (100754ms till timeout)
2022-03-31 06:24:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (99751ms till timeout)
2022-03-31 06:24:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (98748ms till timeout)
2022-03-31 06:24:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (97745ms till timeout)
2022-03-31 06:24:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (96742ms till timeout)
2022-03-31 06:24:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (95739ms till timeout)
2022-03-31 06:24:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (449963ms till timeout)
2022-03-31 06:24:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (94736ms till timeout)
2022-03-31 06:24:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (93732ms till timeout)
2022-03-31 06:24:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (92729ms till timeout)
2022-03-31 06:24:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (91726ms till timeout)
2022-03-31 06:24:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (90723ms till timeout)
2022-03-31 06:24:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (89720ms till timeout)
2022-03-31 06:24:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-606370994-844170518-new will have desired state: Ready not ready, will try again in 1000 ms (88717ms till timeout)
2022-03-31 06:24:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-606370994-844170518-new is in desired state: Ready
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@40ebec99, which are set.
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@81752b2, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --topic, my-topic-606370994-844170518-new, --max-messages, 100, USER=my_user_1993501230_801238233], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518-new', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@40ebec99}
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518-new from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --topic my-topic-606370994-844170518-new --max-messages 100 USER=my_user_1993501230_801238233
2022-03-31 06:24:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --topic my-topic-606370994-844170518-new --max-messages 100 USER=my_user_1993501230_801238233
2022-03-31 06:24:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (439952ms till timeout)
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@523c062, which are set.
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@28178b3f, messages=[], arguments=[--bootstrap-server, my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093, --group-id, my-consumer-group-616220558, --topic, my-topic-606370994-844170518-new, --max-messages, 100, USER=my_user_1993501230_801238233, --group-instance-id, instance1160409880], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-012ef870-kafka-clients-5467765f67-hgpd8', podNamespace='namespace-3', bootstrapServer='my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-606370994-844170518-new', maxMessages=100, kafkaUsername='my-user-1993501230-801238233', consumerGroupName='my-consumer-group-616220558', consumerInstanceId='instance1160409880', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@523c062}
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093:my-topic-606370994-844170518-new from pod my-cluster-012ef870-kafka-clients-5467765f67-hgpd8
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-616220558 --topic my-topic-606370994-844170518-new --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance1160409880
2022-03-31 06:24:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-012ef870-kafka-clients-5467765f67-hgpd8 -n namespace-3 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-012ef870-kafka-bootstrap.namespace-3.svc:9093 --group-id my-consumer-group-616220558 --topic my-topic-606370994-844170518-new --max-messages 100 USER=my_user_1993501230_801238233 --group-instance-id instance1160409880
2022-03-31 06:24:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [rollingupdate.RollingUpdateST - After Each] - Clean up after test
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndZookeeperScaleUpScaleDown
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-606370994-844170518 in namespace namespace-3
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-606370994-844170518
2022-03-31 06:24:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-606370994-844170518 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:24:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (429943ms till timeout)
2022-03-31 06:24:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-606370994-844170518-new in namespace namespace-3
2022-03-31 06:24:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-606370994-844170518-new
2022-03-31 06:24:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-606370994-844170518-new not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:24:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (419933ms till timeout)
2022-03-31 06:24:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-012ef870-kafka-clients in namespace namespace-3
2022-03-31 06:24:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-012ef870-kafka-clients
2022-03-31 06:24:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-012ef870-kafka-clients not ready, will try again in 10000 ms (479989ms till timeout)
2022-03-31 06:24:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (409923ms till timeout)
2022-03-31 06:24:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:24:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-012ef870-kafka-clients not ready, will try again in 10000 ms (469980ms till timeout)
2022-03-31 06:24:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:24:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:24:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (399914ms till timeout)
2022-03-31 06:25:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-012ef870-kafka-clients not ready, will try again in 10000 ms (459971ms till timeout)
2022-03-31 06:25:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients not ready, will try again in 10000 ms (389903ms till timeout)
2022-03-31 06:25:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-012ef870-kafka-clients not ready, will try again in 10000 ms (449961ms till timeout)
2022-03-31 06:25:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2def1ad9-kafka-clients-tls in namespace namespace-2
2022-03-31 06:25:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2def1ad9-kafka-clients-tls
2022-03-31 06:25:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-2def1ad9 in namespace namespace-2
2022-03-31 06:25:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-2def1ad9
2022-03-31 06:25:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-2def1ad9 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-31 06:25:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1993501230-801238233 in namespace namespace-3
2022-03-31 06:25:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1993501230-801238233
2022-03-31 06:25:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1993501230-801238233 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-31 06:25:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-332320981-1578568188 in namespace namespace-2
2022-03-31 06:25:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-332320981-1578568188
2022-03-31 06:25:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-332320981-1578568188 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-31 06:25:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-012ef870 in namespace namespace-3
2022-03-31 06:25:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-012ef870
2022-03-31 06:25:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-012ef870 not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-31 06:25:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-606982444-1208823186 in namespace namespace-2
2022-03-31 06:25:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-606982444-1208823186
2022-03-31 06:25:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-606982444-1208823186 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-31 06:25:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479924ms till timeout)
2022-03-31 06:25:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478851ms till timeout)
2022-03-31 06:25:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (477774ms till timeout)
2022-03-31 06:25:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2def1ad9 in namespace namespace-2
2022-03-31 06:25:49 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-2def1ad9
2022-03-31 06:25:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-2def1ad9
2022-03-31 06:25:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-2def1ad9 not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-31 06:25:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476699ms till timeout)
2022-03-31 06:25:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475612ms till timeout)
2022-03-31 06:25:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (474534ms till timeout)
2022-03-31 06:25:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473456ms till timeout)
2022-03-31 06:25:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472380ms till timeout)
2022-03-31 06:25:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (471301ms till timeout)
2022-03-31 06:25:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:25:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470233ms till timeout)
2022-03-31 06:25:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469160ms till timeout)
2022-03-31 06:25:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (468079ms till timeout)
2022-03-31 06:25:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:25:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479927ms till timeout)
2022-03-31 06:25:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:25:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:25:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (466997ms till timeout)
2022-03-31 06:26:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478854ms till timeout)
2022-03-31 06:26:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (465917ms till timeout)
2022-03-31 06:26:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:26:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (477776ms till timeout)
2022-03-31 06:26:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464841ms till timeout)
2022-03-31 06:26:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476706ms till timeout)
2022-03-31 06:26:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463763ms till timeout)
2022-03-31 06:26:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475627ms till timeout)
2022-03-31 06:26:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (462690ms till timeout)
2022-03-31 06:26:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (474544ms till timeout)
2022-03-31 06:26:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461618ms till timeout)
2022-03-31 06:26:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473465ms till timeout)
2022-03-31 06:26:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460544ms till timeout)
2022-03-31 06:26:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:26:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472391ms till timeout)
2022-03-31 06:26:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (459477ms till timeout)
2022-03-31 06:26:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (471315ms till timeout)
2022-03-31 06:26:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (458409ms till timeout)
2022-03-31 06:26:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470238ms till timeout)
2022-03-31 06:26:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457333ms till timeout)
2022-03-31 06:26:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469160ms till timeout)
2022-03-31 06:26:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (456261ms till timeout)
2022-03-31 06:26:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (468088ms till timeout)
2022-03-31 06:26:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:26:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455185ms till timeout)
2022-03-31 06:26:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467011ms till timeout)
2022-03-31 06:26:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454119ms till timeout)
2022-03-31 06:26:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (465943ms till timeout)
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaAndZookeeperScaleUpScaleDown - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [rollingupdate.RollingUpdateST] - Removing parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [rollingupdate.RollingUpdateST] - Parallel test count: 1
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-FINISHED
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:690] [rollingupdate.RollingUpdateST - After All] - Clean up after test suite
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context RollingUpdateST is everything deleted.
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st removal
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (479925ms till timeout)
2022-03-31 06:26:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (464870ms till timeout)
2022-03-31 06:26:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (478854ms till timeout)
2022-03-31 06:26:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463789ms till timeout)
2022-03-31 06:26:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (477770ms till timeout)
2022-03-31 06:26:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-31 06:26:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (462712ms till timeout)
2022-03-31 06:26:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (476698ms till timeout)
2022-03-31 06:26:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (461640ms till timeout)
2022-03-31 06:26:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (475623ms till timeout)
2022-03-31 06:26:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (460564ms till timeout)
2022-03-31 06:26:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "rolling-update-st" not found
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-2], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:254] RollingUpdateST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST] to and randomly select one to start execution
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:85] [rollingupdate.RollingUpdateST] - Removing parallel suite: RollingUpdateST
2022-03-31 06:26:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:89] [rollingupdate.RollingUpdateST] - Parallel suites count: 2
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 923.137 s - in io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-31 06:26:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (459487ms till timeout)
2022-03-31 06:26:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (458413ms till timeout)
2022-03-31 06:26:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-31 06:26:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457337ms till timeout)
2022-03-31 06:26:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (456259ms till timeout)
2022-03-31 06:26:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (455183ms till timeout)
2022-03-31 06:26:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454108ms till timeout)
2022-03-31 06:26:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (453036ms till timeout)
2022-03-31 06:26:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-31 06:26:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (451961ms till timeout)
2022-03-31 06:26:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (450884ms till timeout)
2022-03-31 06:26:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (449810ms till timeout)
2022-03-31 06:26:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (448739ms till timeout)
2022-03-31 06:26:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-31 06:26:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (447665ms till timeout)
2022-03-31 06:26:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-2 -o yaml
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:267] testAutoRenewAllCaCertsTriggeredByAnno - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:93] [security.SecurityST] - Removing parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:97] [security.SecurityST] - Parallel test count: 0
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:690] [security.SecurityST - After All] - Clean up after test suite
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:346] In context SecurityST is everything deleted.
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-STARTED
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlST - Before Each] - Setup test case environment
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st removal
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-92a05c6e in namespace cruise-control-st
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-92a05c6e
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-92a05c6e will have desired state: Ready
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-92a05c6e will have desired state: Ready
2022-03-31 06:26:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1319998ms till timeout)
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (479924ms till timeout)
2022-03-31 06:26:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1318995ms till timeout)
2022-03-31 06:26:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (478850ms till timeout)
2022-03-31 06:26:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1317991ms till timeout)
2022-03-31 06:26:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (477778ms till timeout)
2022-03-31 06:26:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1316988ms till timeout)
2022-03-31 06:26:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (476706ms till timeout)
2022-03-31 06:26:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-31 06:26:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1315983ms till timeout)
2022-03-31 06:26:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:26:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (475590ms till timeout)
2022-03-31 06:26:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-31 06:26:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1314980ms till timeout)
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "security-st" not found
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:254] SecurityST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST] to and randomly select one to start execution
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:85] [security.SecurityST] - Removing parallel suite: SecurityST
2022-03-31 06:26:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:89] [security.SecurityST] - Parallel suites count: 1
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 942.146 s - in io.strimzi.systemtest.security.SecurityST
2022-03-31 06:26:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1313977ms till timeout)
2022-03-31 06:26:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1312967ms till timeout)
2022-03-31 06:26:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1311964ms till timeout)
2022-03-31 06:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:26:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1310960ms till timeout)
2022-03-31 06:26:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1309957ms till timeout)
2022-03-31 06:26:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1308954ms till timeout)
2022-03-31 06:26:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1307951ms till timeout)
2022-03-31 06:26:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1306948ms till timeout)
2022-03-31 06:26:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:26:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1305945ms till timeout)
2022-03-31 06:26:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1304941ms till timeout)
2022-03-31 06:26:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1303938ms till timeout)
2022-03-31 06:26:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1302935ms till timeout)
2022-03-31 06:26:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1301932ms till timeout)
2022-03-31 06:26:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:26:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1300929ms till timeout)
2022-03-31 06:26:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1299925ms till timeout)
2022-03-31 06:26:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1298922ms till timeout)
2022-03-31 06:26:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1297919ms till timeout)
2022-03-31 06:26:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1296915ms till timeout)
2022-03-31 06:26:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:26:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1295912ms till timeout)
2022-03-31 06:26:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1294909ms till timeout)
2022-03-31 06:26:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1293906ms till timeout)
2022-03-31 06:26:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:26:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:26:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1292902ms till timeout)
2022-03-31 06:27:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1291899ms till timeout)
2022-03-31 06:27:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1290896ms till timeout)
2022-03-31 06:27:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1289892ms till timeout)
2022-03-31 06:27:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1288887ms till timeout)
2022-03-31 06:27:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1287884ms till timeout)
2022-03-31 06:27:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1286880ms till timeout)
2022-03-31 06:27:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1285877ms till timeout)
2022-03-31 06:27:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1284873ms till timeout)
2022-03-31 06:27:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1283869ms till timeout)
2022-03-31 06:27:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1282866ms till timeout)
2022-03-31 06:27:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1281861ms till timeout)
2022-03-31 06:27:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1280858ms till timeout)
2022-03-31 06:27:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1279854ms till timeout)
2022-03-31 06:27:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1278851ms till timeout)
2022-03-31 06:27:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1277847ms till timeout)
2022-03-31 06:27:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1276844ms till timeout)
2022-03-31 06:27:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1275840ms till timeout)
2022-03-31 06:27:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1274837ms till timeout)
2022-03-31 06:27:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1273833ms till timeout)
2022-03-31 06:27:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1272830ms till timeout)
2022-03-31 06:27:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1271827ms till timeout)
2022-03-31 06:27:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1270823ms till timeout)
2022-03-31 06:27:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1269820ms till timeout)
2022-03-31 06:27:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1268816ms till timeout)
2022-03-31 06:27:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1267813ms till timeout)
2022-03-31 06:27:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1266810ms till timeout)
2022-03-31 06:27:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1265806ms till timeout)
2022-03-31 06:27:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1264803ms till timeout)
2022-03-31 06:27:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1263800ms till timeout)
2022-03-31 06:27:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1262797ms till timeout)
2022-03-31 06:27:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1261792ms till timeout)
2022-03-31 06:27:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1260788ms till timeout)
2022-03-31 06:27:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1259785ms till timeout)
2022-03-31 06:27:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1258781ms till timeout)
2022-03-31 06:27:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1257778ms till timeout)
2022-03-31 06:27:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1256774ms till timeout)
2022-03-31 06:27:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1255771ms till timeout)
2022-03-31 06:27:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1254767ms till timeout)
2022-03-31 06:27:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1253764ms till timeout)
2022-03-31 06:27:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1252761ms till timeout)
2022-03-31 06:27:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1251757ms till timeout)
2022-03-31 06:27:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1250754ms till timeout)
2022-03-31 06:27:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1249750ms till timeout)
2022-03-31 06:27:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1248747ms till timeout)
2022-03-31 06:27:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1247744ms till timeout)
2022-03-31 06:27:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1246740ms till timeout)
2022-03-31 06:27:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1245737ms till timeout)
2022-03-31 06:27:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1244734ms till timeout)
2022-03-31 06:27:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1243730ms till timeout)
2022-03-31 06:27:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1242727ms till timeout)
2022-03-31 06:27:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1241724ms till timeout)
2022-03-31 06:27:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1240720ms till timeout)
2022-03-31 06:27:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1239717ms till timeout)
2022-03-31 06:27:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1238713ms till timeout)
2022-03-31 06:27:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1237710ms till timeout)
2022-03-31 06:27:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1236706ms till timeout)
2022-03-31 06:27:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:27:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1235703ms till timeout)
2022-03-31 06:27:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1234700ms till timeout)
2022-03-31 06:27:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:27:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:27:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1233696ms till timeout)
2022-03-31 06:27:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1232693ms till timeout)
2022-03-31 06:28:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1231690ms till timeout)
2022-03-31 06:28:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1230687ms till timeout)
2022-03-31 06:28:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1229683ms till timeout)
2022-03-31 06:28:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1228680ms till timeout)
2022-03-31 06:28:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1227676ms till timeout)
2022-03-31 06:28:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1226673ms till timeout)
2022-03-31 06:28:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1225669ms till timeout)
2022-03-31 06:28:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1224666ms till timeout)
2022-03-31 06:28:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1223662ms till timeout)
2022-03-31 06:28:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1222659ms till timeout)
2022-03-31 06:28:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1221656ms till timeout)
2022-03-31 06:28:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1220652ms till timeout)
2022-03-31 06:28:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1219649ms till timeout)
2022-03-31 06:28:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1218645ms till timeout)
2022-03-31 06:28:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1217642ms till timeout)
2022-03-31 06:28:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1216639ms till timeout)
2022-03-31 06:28:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1215636ms till timeout)
2022-03-31 06:28:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1214632ms till timeout)
2022-03-31 06:28:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (1213629ms till timeout)
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-92a05c6e is in desired state: Ready
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-92a05c6e in namespace cruise-control-st
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-92a05c6e
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: PendingProposal
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: PendingProposal
2022-03-31 06:28:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: PendingProposal not ready, will try again in 1000 ms (359998ms till timeout)
2022-03-31 06:28:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: PendingProposal
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): PendingProposal
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:28:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Verifying that KafkaRebalance resource is in PendingProposal state
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: PendingProposal
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: PendingProposal
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: PendingProposal
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:85] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Verifying that KafkaRebalance resource is in ProposalReady state
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady
2022-03-31 06:28:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:28:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 06:28:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-31 06:28:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-31 06:28:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-31 06:28:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-31 06:28:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-31 06:28:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-31 06:28:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-31 06:28:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-31 06:28:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (589964ms till timeout)
2022-03-31 06:28:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (588961ms till timeout)
2022-03-31 06:28:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (587958ms till timeout)
2022-03-31 06:28:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (586955ms till timeout)
2022-03-31 06:28:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (585952ms till timeout)
2022-03-31 06:28:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (584948ms till timeout)
2022-03-31 06:28:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (583945ms till timeout)
2022-03-31 06:28:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (582942ms till timeout)
2022-03-31 06:28:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (581939ms till timeout)
2022-03-31 06:28:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (580936ms till timeout)
2022-03-31 06:28:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (579933ms till timeout)
2022-03-31 06:28:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (578930ms till timeout)
2022-03-31 06:28:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (577927ms till timeout)
2022-03-31 06:28:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (576924ms till timeout)
2022-03-31 06:28:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (575920ms till timeout)
2022-03-31 06:28:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (574918ms till timeout)
2022-03-31 06:28:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (573914ms till timeout)
2022-03-31 06:28:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (572911ms till timeout)
2022-03-31 06:28:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (571908ms till timeout)
2022-03-31 06:28:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (570904ms till timeout)
2022-03-31 06:28:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (569901ms till timeout)
2022-03-31 06:28:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (568898ms till timeout)
2022-03-31 06:28:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (567895ms till timeout)
2022-03-31 06:28:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (566892ms till timeout)
2022-03-31 06:28:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (565889ms till timeout)
2022-03-31 06:28:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (564886ms till timeout)
2022-03-31 06:28:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:28:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (563883ms till timeout)
2022-03-31 06:28:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (562880ms till timeout)
2022-03-31 06:28:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:28:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:28:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (561876ms till timeout)
2022-03-31 06:29:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (560873ms till timeout)
2022-03-31 06:29:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (559870ms till timeout)
2022-03-31 06:29:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (558866ms till timeout)
2022-03-31 06:29:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (557864ms till timeout)
2022-03-31 06:29:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (556861ms till timeout)
2022-03-31 06:29:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (555858ms till timeout)
2022-03-31 06:29:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (554855ms till timeout)
2022-03-31 06:29:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (553852ms till timeout)
2022-03-31 06:29:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (552849ms till timeout)
2022-03-31 06:29:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (551846ms till timeout)
2022-03-31 06:29:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (550843ms till timeout)
2022-03-31 06:29:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (549840ms till timeout)
2022-03-31 06:29:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (548837ms till timeout)
2022-03-31 06:29:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (547833ms till timeout)
2022-03-31 06:29:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (546830ms till timeout)
2022-03-31 06:29:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (545827ms till timeout)
2022-03-31 06:29:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (544824ms till timeout)
2022-03-31 06:29:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (543821ms till timeout)
2022-03-31 06:29:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (542818ms till timeout)
2022-03-31 06:29:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (541814ms till timeout)
2022-03-31 06:29:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (540812ms till timeout)
2022-03-31 06:29:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (539809ms till timeout)
2022-03-31 06:29:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (538806ms till timeout)
2022-03-31 06:29:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (537802ms till timeout)
2022-03-31 06:29:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (536799ms till timeout)
2022-03-31 06:29:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (535797ms till timeout)
2022-03-31 06:29:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (534794ms till timeout)
2022-03-31 06:29:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (533791ms till timeout)
2022-03-31 06:29:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (532787ms till timeout)
2022-03-31 06:29:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (531784ms till timeout)
2022-03-31 06:29:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (530781ms till timeout)
2022-03-31 06:29:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (529778ms till timeout)
2022-03-31 06:29:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (528775ms till timeout)
2022-03-31 06:29:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (527772ms till timeout)
2022-03-31 06:29:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (526769ms till timeout)
2022-03-31 06:29:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (525765ms till timeout)
2022-03-31 06:29:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (524762ms till timeout)
2022-03-31 06:29:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (523759ms till timeout)
2022-03-31 06:29:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (522756ms till timeout)
2022-03-31 06:29:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (521753ms till timeout)
2022-03-31 06:29:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (520750ms till timeout)
2022-03-31 06:29:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (519747ms till timeout)
2022-03-31 06:29:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (518744ms till timeout)
2022-03-31 06:29:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (517741ms till timeout)
2022-03-31 06:29:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (516737ms till timeout)
2022-03-31 06:29:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (515734ms till timeout)
2022-03-31 06:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (514731ms till timeout)
2022-03-31 06:29:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (513728ms till timeout)
2022-03-31 06:29:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (512725ms till timeout)
2022-03-31 06:29:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (511722ms till timeout)
2022-03-31 06:29:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (510719ms till timeout)
2022-03-31 06:29:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (509716ms till timeout)
2022-03-31 06:29:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (508713ms till timeout)
2022-03-31 06:29:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (507710ms till timeout)
2022-03-31 06:29:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (506707ms till timeout)
2022-03-31 06:29:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (505703ms till timeout)
2022-03-31 06:29:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:29:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (504700ms till timeout)
2022-03-31 06:29:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (503697ms till timeout)
2022-03-31 06:29:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (502694ms till timeout)
2022-03-31 06:29:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:29:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:29:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (501691ms till timeout)
2022-03-31 06:30:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (500687ms till timeout)
2022-03-31 06:30:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (499684ms till timeout)
2022-03-31 06:30:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (498681ms till timeout)
2022-03-31 06:30:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (497678ms till timeout)
2022-03-31 06:30:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (496675ms till timeout)
2022-03-31 06:30:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (495672ms till timeout)
2022-03-31 06:30:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (494669ms till timeout)
2022-03-31 06:30:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (493666ms till timeout)
2022-03-31 06:30:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (492663ms till timeout)
2022-03-31 06:30:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (491660ms till timeout)
2022-03-31 06:30:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (490657ms till timeout)
2022-03-31 06:30:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (489654ms till timeout)
2022-03-31 06:30:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (488651ms till timeout)
2022-03-31 06:30:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (487648ms till timeout)
2022-03-31 06:30:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (486644ms till timeout)
2022-03-31 06:30:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (485642ms till timeout)
2022-03-31 06:30:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (484638ms till timeout)
2022-03-31 06:30:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (483635ms till timeout)
2022-03-31 06:30:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (482632ms till timeout)
2022-03-31 06:30:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (481628ms till timeout)
2022-03-31 06:30:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (480625ms till timeout)
2022-03-31 06:30:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (479622ms till timeout)
2022-03-31 06:30:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (478619ms till timeout)
2022-03-31 06:30:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (477616ms till timeout)
2022-03-31 06:30:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (476613ms till timeout)
2022-03-31 06:30:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (475610ms till timeout)
2022-03-31 06:30:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (474606ms till timeout)
2022-03-31 06:30:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (473603ms till timeout)
2022-03-31 06:30:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (472600ms till timeout)
2022-03-31 06:30:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (471597ms till timeout)
2022-03-31 06:30:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (470594ms till timeout)
2022-03-31 06:30:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (469591ms till timeout)
2022-03-31 06:30:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (468587ms till timeout)
2022-03-31 06:30:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (467584ms till timeout)
2022-03-31 06:30:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (466581ms till timeout)
2022-03-31 06:30:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (465578ms till timeout)
2022-03-31 06:30:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (464575ms till timeout)
2022-03-31 06:30:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (463572ms till timeout)
2022-03-31 06:30:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (462569ms till timeout)
2022-03-31 06:30:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (461566ms till timeout)
2022-03-31 06:30:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (460563ms till timeout)
2022-03-31 06:30:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (459560ms till timeout)
2022-03-31 06:30:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (458557ms till timeout)
2022-03-31 06:30:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (457553ms till timeout)
2022-03-31 06:30:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (456550ms till timeout)
2022-03-31 06:30:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (455547ms till timeout)
2022-03-31 06:30:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (454544ms till timeout)
2022-03-31 06:30:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (453541ms till timeout)
2022-03-31 06:30:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (452538ms till timeout)
2022-03-31 06:30:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (451534ms till timeout)
2022-03-31 06:30:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (450531ms till timeout)
2022-03-31 06:30:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (449528ms till timeout)
2022-03-31 06:30:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (448525ms till timeout)
2022-03-31 06:30:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (447522ms till timeout)
2022-03-31 06:30:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (446519ms till timeout)
2022-03-31 06:30:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (445515ms till timeout)
2022-03-31 06:30:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:30:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (444512ms till timeout)
2022-03-31 06:30:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (443509ms till timeout)
2022-03-31 06:30:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (442506ms till timeout)
2022-03-31 06:30:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:30:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:30:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (441502ms till timeout)
2022-03-31 06:31:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (440499ms till timeout)
2022-03-31 06:31:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (439496ms till timeout)
2022-03-31 06:31:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (438493ms till timeout)
2022-03-31 06:31:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (437490ms till timeout)
2022-03-31 06:31:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (436486ms till timeout)
2022-03-31 06:31:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (435483ms till timeout)
2022-03-31 06:31:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (434480ms till timeout)
2022-03-31 06:31:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (433477ms till timeout)
2022-03-31 06:31:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (432474ms till timeout)
2022-03-31 06:31:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (431471ms till timeout)
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: ProposalReady
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ProposalReady
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Annotating KafkaRebalance:my-cluster-92a05c6e with annotation approve
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-92a05c6e strimzi.io/rebalance=approve
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-92a05c6e strimzi.io/rebalance=approve
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-92a05c6e annotated
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Verifying that annotation triggers the Rebalancing state
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: Rebalancing
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: Rebalancing
2022-03-31 06:31:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Rebalancing not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:31:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: Rebalancing
2022-03-31 06:31:11 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Verifying that KafkaRebalance is in the Ready state
2022-03-31 06:31:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready
2022-03-31 06:31:11 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready
2022-03-31 06:31:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:31:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 06:31:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-31 06:31:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-31 06:31:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-31 06:31:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (594981ms till timeout)
2022-03-31 06:31:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-31 06:31:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-31 06:31:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-31 06:31:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-31 06:31:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (589964ms till timeout)
2022-03-31 06:31:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (588961ms till timeout)
2022-03-31 06:31:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (587957ms till timeout)
2022-03-31 06:31:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (586952ms till timeout)
2022-03-31 06:31:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (585949ms till timeout)
2022-03-31 06:31:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (584945ms till timeout)
2022-03-31 06:31:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (583942ms till timeout)
2022-03-31 06:31:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (582939ms till timeout)
2022-03-31 06:31:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (581935ms till timeout)
2022-03-31 06:31:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (580932ms till timeout)
2022-03-31 06:31:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (579929ms till timeout)
2022-03-31 06:31:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (578926ms till timeout)
2022-03-31 06:31:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (577922ms till timeout)
2022-03-31 06:31:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (576919ms till timeout)
2022-03-31 06:31:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (575916ms till timeout)
2022-03-31 06:31:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (574913ms till timeout)
2022-03-31 06:31:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (573909ms till timeout)
2022-03-31 06:31:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (572906ms till timeout)
2022-03-31 06:31:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (571903ms till timeout)
2022-03-31 06:31:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (570899ms till timeout)
2022-03-31 06:31:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (569896ms till timeout)
2022-03-31 06:31:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (568893ms till timeout)
2022-03-31 06:31:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (567889ms till timeout)
2022-03-31 06:31:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (566886ms till timeout)
2022-03-31 06:31:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (565883ms till timeout)
2022-03-31 06:31:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (564880ms till timeout)
2022-03-31 06:31:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (563876ms till timeout)
2022-03-31 06:31:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (562873ms till timeout)
2022-03-31 06:31:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (561869ms till timeout)
2022-03-31 06:31:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (560866ms till timeout)
2022-03-31 06:31:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (559863ms till timeout)
2022-03-31 06:31:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (558859ms till timeout)
2022-03-31 06:31:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (557856ms till timeout)
2022-03-31 06:31:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (556853ms till timeout)
2022-03-31 06:31:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (555850ms till timeout)
2022-03-31 06:31:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:31:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (554846ms till timeout)
2022-03-31 06:31:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (553843ms till timeout)
2022-03-31 06:31:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:31:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:31:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (552840ms till timeout)
2022-03-31 06:32:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (551837ms till timeout)
2022-03-31 06:32:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (550833ms till timeout)
2022-03-31 06:32:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (549831ms till timeout)
2022-03-31 06:32:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (548827ms till timeout)
2022-03-31 06:32:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (547824ms till timeout)
2022-03-31 06:32:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (546821ms till timeout)
2022-03-31 06:32:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (545817ms till timeout)
2022-03-31 06:32:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: Ready
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlST:152] Annotating KafkaRebalance: my-cluster-92a05c6e with 'refresh' anno
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #2(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Annotating KafkaRebalance:my-cluster-92a05c6e with annotation refresh
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-92a05c6e strimzi.io/rebalance=refresh
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-92a05c6e strimzi.io/rebalance=refresh
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady
2022-03-31 06:32:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:32:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: ProposalReady
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlST:156] Trying rebalancing process again
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ProposalReady
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ProposalReady
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): ============================================================================
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Annotating KafkaRebalance:my-cluster-92a05c6e with annotation approve
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-92a05c6e strimzi.io/rebalance=approve
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-92a05c6e strimzi.io/rebalance=approve
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-92a05c6e annotated
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Verifying that annotation triggers the Rebalancing state
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: Rebalancing
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: Rebalancing
2022-03-31 06:32:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Rebalancing not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:32:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: Rebalancing
2022-03-31 06:32:09 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-92a05c6e): Verifying that KafkaRebalance is in the Ready state
2022-03-31 06:32:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready
2022-03-31 06:32:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready
2022-03-31 06:32:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:32:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 06:32:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-31 06:32:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-31 06:32:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-92a05c6e will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-31 06:32:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-92a05c6e is in desired state: Ready
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlST - After Each] - Clean up after test
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithRebalanceResourceAndRefreshAnnotation
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-92a05c6e in namespace cruise-control-st
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-92a05c6e
2022-03-31 06:32:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-92a05c6e not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:32:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-92a05c6e in namespace cruise-control-st
2022-03-31 06:32:24 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-92a05c6e
2022-03-31 06:32:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-92a05c6e
2022-03-31 06:32:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-92a05c6e not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-31 06:32:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-FINISHED
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlST - After All] - Clean up after test suite
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:346] In context CruiseControlST is everything deleted.
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st removal
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (479922ms till timeout)
2022-03-31 06:32:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:35 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:35 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (478843ms till timeout)
2022-03-31 06:32:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (477768ms till timeout)
2022-03-31 06:32:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:37 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:37 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (476697ms till timeout)
2022-03-31 06:32:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (475621ms till timeout)
2022-03-31 06:32:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (474547ms till timeout)
2022-03-31 06:32:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:40 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:40 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (473469ms till timeout)
2022-03-31 06:32:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (472387ms till timeout)
2022-03-31 06:32:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:43 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:43 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (471309ms till timeout)
2022-03-31 06:32:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:44 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:44 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (470233ms till timeout)
2022-03-31 06:32:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:45 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:45 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (469155ms till timeout)
2022-03-31 06:32:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (468082ms till timeout)
2022-03-31 06:32:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:47 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:47 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (467014ms till timeout)
2022-03-31 06:32:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:48 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:48 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (465936ms till timeout)
2022-03-31 06:32:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (464860ms till timeout)
2022-03-31 06:32:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:50 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:50 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (463787ms till timeout)
2022-03-31 06:32:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (462715ms till timeout)
2022-03-31 06:32:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:52 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:52 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (461635ms till timeout)
2022-03-31 06:32:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (460556ms till timeout)
2022-03-31 06:32:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:54 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:54 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (459483ms till timeout)
2022-03-31 06:32:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (458401ms till timeout)
2022-03-31 06:32:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:32:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:57 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:57 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (457326ms till timeout)
2022-03-31 06:32:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:58 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:58 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (456255ms till timeout)
2022-03-31 06:32:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:32:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:32:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:59 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:32:59 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:32:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (455182ms till timeout)
2022-03-31 06:33:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:00 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:00 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (454110ms till timeout)
2022-03-31 06:33:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:01 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:01 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (453039ms till timeout)
2022-03-31 06:33:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:33:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:02 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:02 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (451966ms till timeout)
2022-03-31 06:33:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (450890ms till timeout)
2022-03-31 06:33:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:04 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:04 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (449819ms till timeout)
2022-03-31 06:33:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:05 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:05 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (448750ms till timeout)
2022-03-31 06:33:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:33:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:06 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:06 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (447671ms till timeout)
2022-03-31 06:33:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (446604ms till timeout)
2022-03-31 06:33:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (445536ms till timeout)
2022-03-31 06:33:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (444464ms till timeout)
2022-03-31 06:33:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (443388ms till timeout)
2022-03-31 06:33:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:33:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:12 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:12 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (442315ms till timeout)
2022-03-31 06:33:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:13 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:13 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (441243ms till timeout)
2022-03-31 06:33:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (440170ms till timeout)
2022-03-31 06:33:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (439090ms till timeout)
2022-03-31 06:33:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-31 06:33:16 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:16 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:33:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (438019ms till timeout)
2022-03-31 06:33:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-31 06:33:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-st" not found
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:254] CruiseControlST - Notifies waiting test suites:[CruiseControlApiST, CruiseControlST, UserST, ListenersST, HttpBridgeTlsST, RollingUpdateST] to and randomly select one to start execution
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlST] - Removing parallel suite: CruiseControlST
2022-03-31 06:33:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,375.802 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-31 06:33:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-31 06:33:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-31 06:33:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-31 06:33:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-31 06:33:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-31 06:33:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-31 06:33:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-31 06:33:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-31 06:33:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-31 06:33:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-31 06:33:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-31 06:33:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-31 06:33:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-31 06:33:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-31 06:33:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-31 06:33:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-31 06:33:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-31 06:33:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-31 06:33:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-31 06:33:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-31 06:33:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-31 06:33:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-31 06:33:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-31 06:33:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-31 06:33:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-31 06:33:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-31 06:33:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-31 06:33:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-31 06:33:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:136] Suite watcher.AllNamespaceIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [AllNamespaceIsolatedST:190] Creating resources before the test class
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:33:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:33:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-31 06:33:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:33:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-31 06:33:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-31 06:33:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-31 06:33:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-31 06:33:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:33:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:33:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:33:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179915ms till timeout)
2022-03-31 06:33:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:33:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:33:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-31 06:33:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:34:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179946ms till timeout)
2022-03-31 06:34:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:34:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:34:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:34:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:34:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179822ms till timeout)
2022-03-31 06:34:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:34:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-31 06:34:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v138858
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v138858
2022-03-31 06:34:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=138858&allowWatchBookmarks=true&watch=true...
2022-03-31 06:34:48 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:34:48 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 138859
2022-03-31 06:34:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 138874
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 138875
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v138874 in namespace default
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@784ec8be
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@611abcdb
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@611abcdb
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@611abcdb
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:34:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:34:53Z",
        "name": "infra-namespace",
        "resourceVersion": "138876",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "48725bd4-93fc-4230-9e17-c9b99386e24b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-namespace-test
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace second-namespace-test -o json
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace second-namespace-test -o json
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:34:53Z",
        "name": "second-namespace-test",
        "resourceVersion": "138880",
        "selfLink": "/api/v1/namespaces/second-namespace-test",
        "uid": "f417fa9b-941f-44c5-af00-49d6bf30f096"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test]}
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: third-namespace-test
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace third-namespace-test
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace third-namespace-test -o json
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace third-namespace-test -o json
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:34:53Z",
        "name": "third-namespace-test",
        "resourceVersion": "138884",
        "selfLink": "/api/v1/namespaces/third-namespace-test",
        "uid": "43d2465a-1cf2-4200-a5d6-33fc8b288061"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test, third-namespace-test]}
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:34:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=third-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: third-namespace-test
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:34:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479980ms till timeout)
2022-03-31 06:34:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478977ms till timeout)
2022-03-31 06:34:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477973ms till timeout)
2022-03-31 06:34:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476970ms till timeout)
2022-03-31 06:34:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475966ms till timeout)
2022-03-31 06:34:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:34:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474963ms till timeout)
2022-03-31 06:35:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473960ms till timeout)
2022-03-31 06:35:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472956ms till timeout)
2022-03-31 06:35:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471953ms till timeout)
2022-03-31 06:35:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470950ms till timeout)
2022-03-31 06:35:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469946ms till timeout)
2022-03-31 06:35:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468943ms till timeout)
2022-03-31 06:35:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467940ms till timeout)
2022-03-31 06:35:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466936ms till timeout)
2022-03-31 06:35:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465933ms till timeout)
2022-03-31 06:35:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464930ms till timeout)
2022-03-31 06:35:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463927ms till timeout)
2022-03-31 06:35:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462923ms till timeout)
2022-03-31 06:35:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461920ms till timeout)
2022-03-31 06:35:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460917ms till timeout)
2022-03-31 06:35:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459914ms till timeout)
2022-03-31 06:35:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458910ms till timeout)
2022-03-31 06:35:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457907ms till timeout)
2022-03-31 06:35:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456904ms till timeout)
2022-03-31 06:35:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455901ms till timeout)
2022-03-31 06:35:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454897ms till timeout)
2022-03-31 06:35:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453894ms till timeout)
2022-03-31 06:35:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452891ms till timeout)
2022-03-31 06:35:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451887ms till timeout)
2022-03-31 06:35:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450884ms till timeout)
2022-03-31 06:35:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449881ms till timeout)
2022-03-31 06:35:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448877ms till timeout)
2022-03-31 06:35:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447874ms till timeout)
2022-03-31 06:35:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446871ms till timeout)
2022-03-31 06:35:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445867ms till timeout)
2022-03-31 06:35:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444864ms till timeout)
2022-03-31 06:35:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443861ms till timeout)
2022-03-31 06:35:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (442857ms till timeout)
2022-03-31 06:35:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (441854ms till timeout)
2022-03-31 06:35:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (440851ms till timeout)
2022-03-31 06:35:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (439847ms till timeout)
2022-03-31 06:35:35 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:35:35 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:35:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:35:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 06:35:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:35:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-31 06:35:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-31 06:35:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-31 06:35:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-31 06:35:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-31 06:35:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-31 06:35:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-31 06:35:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590963ms till timeout)
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5qqmv not ready: strimzi-cluster-operator)
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5qqmv are ready
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace third-namespace-test
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster will have desired state: Ready
2022-03-31 06:35:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-31 06:35:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-31 06:35:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-31 06:35:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:35:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-31 06:35:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (834980ms till timeout)
2022-03-31 06:35:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (833977ms till timeout)
2022-03-31 06:35:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-31 06:35:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (831970ms till timeout)
2022-03-31 06:35:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (830967ms till timeout)
2022-03-31 06:35:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (829964ms till timeout)
2022-03-31 06:35:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-31 06:35:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (827957ms till timeout)
2022-03-31 06:35:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (826954ms till timeout)
2022-03-31 06:35:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:35:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-31 06:36:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-31 06:36:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-31 06:36:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (822942ms till timeout)
2022-03-31 06:36:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (821939ms till timeout)
2022-03-31 06:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (820936ms till timeout)
2022-03-31 06:36:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (819933ms till timeout)
2022-03-31 06:36:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (818930ms till timeout)
2022-03-31 06:36:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (817927ms till timeout)
2022-03-31 06:36:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (816924ms till timeout)
2022-03-31 06:36:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (815920ms till timeout)
2022-03-31 06:36:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (814917ms till timeout)
2022-03-31 06:36:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (813913ms till timeout)
2022-03-31 06:36:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (812910ms till timeout)
2022-03-31 06:36:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (811906ms till timeout)
2022-03-31 06:36:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (810903ms till timeout)
2022-03-31 06:36:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (809899ms till timeout)
2022-03-31 06:36:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (808896ms till timeout)
2022-03-31 06:36:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (807893ms till timeout)
2022-03-31 06:36:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (806890ms till timeout)
2022-03-31 06:36:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (805887ms till timeout)
2022-03-31 06:36:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (804883ms till timeout)
2022-03-31 06:36:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (803880ms till timeout)
2022-03-31 06:36:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (802876ms till timeout)
2022-03-31 06:36:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (801873ms till timeout)
2022-03-31 06:36:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (800869ms till timeout)
2022-03-31 06:36:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (799865ms till timeout)
2022-03-31 06:36:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (798862ms till timeout)
2022-03-31 06:36:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (797859ms till timeout)
2022-03-31 06:36:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (796856ms till timeout)
2022-03-31 06:36:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (795853ms till timeout)
2022-03-31 06:36:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (794850ms till timeout)
2022-03-31 06:36:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (793847ms till timeout)
2022-03-31 06:36:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (792843ms till timeout)
2022-03-31 06:36:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (791840ms till timeout)
2022-03-31 06:36:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (790837ms till timeout)
2022-03-31 06:36:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (789833ms till timeout)
2022-03-31 06:36:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (788830ms till timeout)
2022-03-31 06:36:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (787826ms till timeout)
2022-03-31 06:36:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (786823ms till timeout)
2022-03-31 06:36:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (785819ms till timeout)
2022-03-31 06:36:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (784816ms till timeout)
2022-03-31 06:36:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (783812ms till timeout)
2022-03-31 06:36:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (782809ms till timeout)
2022-03-31 06:36:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (781805ms till timeout)
2022-03-31 06:36:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (780802ms till timeout)
2022-03-31 06:36:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (779798ms till timeout)
2022-03-31 06:36:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (778795ms till timeout)
2022-03-31 06:36:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (777792ms till timeout)
2022-03-31 06:36:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (776788ms till timeout)
2022-03-31 06:36:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (775785ms till timeout)
2022-03-31 06:36:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (774782ms till timeout)
2022-03-31 06:36:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (773779ms till timeout)
2022-03-31 06:36:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (772775ms till timeout)
2022-03-31 06:36:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (771771ms till timeout)
2022-03-31 06:36:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (770769ms till timeout)
2022-03-31 06:36:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (769765ms till timeout)
2022-03-31 06:36:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (768762ms till timeout)
2022-03-31 06:36:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (767759ms till timeout)
2022-03-31 06:36:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-second in namespace second-namespace-test
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-second
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-second will have desired state: Ready
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-second will have desired state: Ready
2022-03-31 06:36:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:37:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-31 06:37:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-31 06:37:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:37:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-31 06:37:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-31 06:37:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (833974ms till timeout)
2022-03-31 06:37:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (832971ms till timeout)
2022-03-31 06:37:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (831967ms till timeout)
2022-03-31 06:37:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (830962ms till timeout)
2022-03-31 06:37:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (829959ms till timeout)
2022-03-31 06:37:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (828955ms till timeout)
2022-03-31 06:37:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (827952ms till timeout)
2022-03-31 06:37:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (826949ms till timeout)
2022-03-31 06:37:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (825945ms till timeout)
2022-03-31 06:37:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (824942ms till timeout)
2022-03-31 06:37:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (823939ms till timeout)
2022-03-31 06:37:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (822935ms till timeout)
2022-03-31 06:37:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (821932ms till timeout)
2022-03-31 06:37:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (820929ms till timeout)
2022-03-31 06:37:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (819925ms till timeout)
2022-03-31 06:37:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (818922ms till timeout)
2022-03-31 06:37:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (817918ms till timeout)
2022-03-31 06:37:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (816915ms till timeout)
2022-03-31 06:37:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (815912ms till timeout)
2022-03-31 06:37:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (814908ms till timeout)
2022-03-31 06:37:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (813905ms till timeout)
2022-03-31 06:37:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (812901ms till timeout)
2022-03-31 06:37:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (811898ms till timeout)
2022-03-31 06:37:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (810895ms till timeout)
2022-03-31 06:37:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (809891ms till timeout)
2022-03-31 06:37:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (808888ms till timeout)
2022-03-31 06:37:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (807884ms till timeout)
2022-03-31 06:37:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (806880ms till timeout)
2022-03-31 06:37:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (805876ms till timeout)
2022-03-31 06:37:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (804873ms till timeout)
2022-03-31 06:37:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (803869ms till timeout)
2022-03-31 06:37:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (802865ms till timeout)
2022-03-31 06:37:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (801861ms till timeout)
2022-03-31 06:37:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (800857ms till timeout)
2022-03-31 06:37:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (799853ms till timeout)
2022-03-31 06:37:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (798849ms till timeout)
2022-03-31 06:37:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (797847ms till timeout)
2022-03-31 06:37:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (796843ms till timeout)
2022-03-31 06:37:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (795839ms till timeout)
2022-03-31 06:37:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (794836ms till timeout)
2022-03-31 06:37:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (793833ms till timeout)
2022-03-31 06:37:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (792830ms till timeout)
2022-03-31 06:37:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (791827ms till timeout)
2022-03-31 06:37:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (790824ms till timeout)
2022-03-31 06:37:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (789821ms till timeout)
2022-03-31 06:37:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (788817ms till timeout)
2022-03-31 06:37:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (787814ms till timeout)
2022-03-31 06:37:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (786811ms till timeout)
2022-03-31 06:37:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (785808ms till timeout)
2022-03-31 06:37:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (784804ms till timeout)
2022-03-31 06:37:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (783801ms till timeout)
2022-03-31 06:37:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (782798ms till timeout)
2022-03-31 06:37:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (781794ms till timeout)
2022-03-31 06:37:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (780791ms till timeout)
2022-03-31 06:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:37:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (779787ms till timeout)
2022-03-31 06:38:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (778784ms till timeout)
2022-03-31 06:38:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (777779ms till timeout)
2022-03-31 06:38:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (776776ms till timeout)
2022-03-31 06:38:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (775772ms till timeout)
2022-03-31 06:38:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:38:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (774769ms till timeout)
2022-03-31 06:38:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (773766ms till timeout)
2022-03-31 06:38:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (772762ms till timeout)
2022-03-31 06:38:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (771759ms till timeout)
2022-03-31 06:38:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (770756ms till timeout)
2022-03-31 06:38:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:38:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (769753ms till timeout)
2022-03-31 06:38:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (768749ms till timeout)
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-second is in desired state: Ready
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [watcher.AllNamespaceIsolatedST - Before Each] - Setup test case environment
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AllNamespaceIsolatedST:82] Deploying Kafka cluster in different namespace than CO when CO watches all namespaces
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster-second in namespace second-namespace-test
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka Cluster status is not in desired state: Ready
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [watcher.AllNamespaceIsolatedST - After Each] - Clean up after test
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:690] [watcher.AllNamespaceIsolatedST - After All] - Clean up after test suite
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for AllNamespaceIsolatedST
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-second in namespace second-namespace-test
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second
2022-03-31 06:38:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second not ready, will try again in 10000 ms (839995ms till timeout)
2022-03-31 06:38:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:38:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:38:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace third-namespace-test
2022-03-31 06:38:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster
2022-03-31 06:38:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster not ready, will try again in 10000 ms (839956ms till timeout)
2022-03-31 06:38:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:38:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-31 06:38:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,484.066 s - in io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:136] Suite connect.ConnectIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-31 06:38:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:38:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:38:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479950ms till timeout)
2022-03-31 06:38:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:38:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:38:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:38:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:38:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:38:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179953ms till timeout)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:38:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-31 06:38:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:38:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:38:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:38:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:38:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179893ms till timeout)
2022-03-31 06:38:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179834ms till timeout)
2022-03-31 06:38:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-namespace-test
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v139789
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v139789
2022-03-31 06:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-namespace-test&resourceVersion=139789&allowWatchBookmarks=true&watch=true...
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v139789
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v139789
2022-03-31 06:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=139789&allowWatchBookmarks=true&watch=true...
2022-03-31 06:39:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:39:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:39:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 139790
2022-03-31 06:39:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 139791
2022-03-31 06:39:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 139855
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 139858
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v139855 in namespace default
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@612a8bdb
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2f8abfcd
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2f8abfcd
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2f8abfcd
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 139871
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 139872
2022-03-31 06:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: third-namespace-test
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v139871 in namespace default
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@48f186eb
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4cce1b2d
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4cce1b2d
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4cce1b2d
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v139873
2022-03-31 06:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v139873
2022-03-31 06:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dthird-namespace-test&resourceVersion=139873&allowWatchBookmarks=true&watch=true...
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:39:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 139874
2022-03-31 06:39:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 139911
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 139912
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v139911 in namespace default
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@7c8db12c
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@380fa688
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@380fa688
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@380fa688
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:39:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:39:14Z",
        "name": "infra-namespace",
        "resourceVersion": "139913",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "b68063a2-a1e0-4a62-b86e-e2ecf6054a44"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:39:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-31 06:39:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-31 06:39:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-31 06:39:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-31 06:39:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-31 06:39:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-31 06:39:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-31 06:39:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472973ms till timeout)
2022-03-31 06:39:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-31 06:39:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-31 06:39:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469964ms till timeout)
2022-03-31 06:39:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468960ms till timeout)
2022-03-31 06:39:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467957ms till timeout)
2022-03-31 06:39:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466954ms till timeout)
2022-03-31 06:39:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465951ms till timeout)
2022-03-31 06:39:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464947ms till timeout)
2022-03-31 06:39:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463944ms till timeout)
2022-03-31 06:39:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462941ms till timeout)
2022-03-31 06:39:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461938ms till timeout)
2022-03-31 06:39:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:34 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:39:34 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 06:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-31 06:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-31 06:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-31 06:39:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-31 06:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-31 06:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-31 06:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591966ms till timeout)
2022-03-31 06:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590963ms till timeout)
2022-03-31 06:39:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2n5j9 not ready: strimzi-cluster-operator)
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2n5j9 are ready
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-STARTED
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [connect.ConnectIsolatedST - Before Each] - Setup test case environment
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [connect.ConnectIsolatedST] - Adding parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [connect.ConnectIsolatedST] - Parallel test count: 1
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testMultiNodeKafkaConnectWithConnectorCreation test now can proceed its execution
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendSimpleMessageTls=my-cluster-82f3beb1, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testUpdateUser=my-cluster-a375259c, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437}
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendSimpleMessageTls=my-user-237170398-1980436763, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testUpdateUser=my-user-134946258-722731532, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227}
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendSimpleMessageTls=my-topic-431455278-78371477, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testUpdateUser=my-topic-686947015-279679095, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180}
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients}
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:39:44Z",
        "name": "namespace-6",
        "resourceVersion": "140009",
        "selfLink": "/api/v1/namespaces/namespace-6",
        "uid": "7bc3e9b4-b408-4577-bea4-890edaefd89b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[namespace-6]}
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0988b907 in namespace namespace-6
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0988b907
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0988b907 will have desired state: Ready
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-0988b907 will have desired state: Ready
2022-03-31 06:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:39:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-31 06:39:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-31 06:39:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:39:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-31 06:39:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-31 06:39:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-31 06:39:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-31 06:39:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-31 06:39:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-31 06:39:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-31 06:39:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-31 06:39:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (827958ms till timeout)
2022-03-31 06:39:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (826955ms till timeout)
2022-03-31 06:39:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-31 06:39:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:39:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-31 06:40:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-31 06:40:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-31 06:40:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (821938ms till timeout)
2022-03-31 06:40:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (820934ms till timeout)
2022-03-31 06:40:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (819931ms till timeout)
2022-03-31 06:40:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (818927ms till timeout)
2022-03-31 06:40:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (817924ms till timeout)
2022-03-31 06:40:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (816920ms till timeout)
2022-03-31 06:40:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (815917ms till timeout)
2022-03-31 06:40:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (814913ms till timeout)
2022-03-31 06:40:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (813910ms till timeout)
2022-03-31 06:40:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (812907ms till timeout)
2022-03-31 06:40:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (811903ms till timeout)
2022-03-31 06:40:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (810900ms till timeout)
2022-03-31 06:40:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (809897ms till timeout)
2022-03-31 06:40:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (808893ms till timeout)
2022-03-31 06:40:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (807890ms till timeout)
2022-03-31 06:40:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (806885ms till timeout)
2022-03-31 06:40:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (805882ms till timeout)
2022-03-31 06:40:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (804878ms till timeout)
2022-03-31 06:40:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (803874ms till timeout)
2022-03-31 06:40:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (802870ms till timeout)
2022-03-31 06:40:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (801865ms till timeout)
2022-03-31 06:40:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (800862ms till timeout)
2022-03-31 06:40:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (799858ms till timeout)
2022-03-31 06:40:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (798855ms till timeout)
2022-03-31 06:40:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (797852ms till timeout)
2022-03-31 06:40:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (796849ms till timeout)
2022-03-31 06:40:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (795845ms till timeout)
2022-03-31 06:40:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (794842ms till timeout)
2022-03-31 06:40:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (793838ms till timeout)
2022-03-31 06:40:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (792835ms till timeout)
2022-03-31 06:40:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (791832ms till timeout)
2022-03-31 06:40:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (790828ms till timeout)
2022-03-31 06:40:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (789825ms till timeout)
2022-03-31 06:40:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (788821ms till timeout)
2022-03-31 06:40:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (787818ms till timeout)
2022-03-31 06:40:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (786815ms till timeout)
2022-03-31 06:40:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (785812ms till timeout)
2022-03-31 06:40:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (784808ms till timeout)
2022-03-31 06:40:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (783805ms till timeout)
2022-03-31 06:40:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (782801ms till timeout)
2022-03-31 06:40:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (781798ms till timeout)
2022-03-31 06:40:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (780795ms till timeout)
2022-03-31 06:40:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (779791ms till timeout)
2022-03-31 06:40:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (778788ms till timeout)
2022-03-31 06:40:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (777785ms till timeout)
2022-03-31 06:40:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (776781ms till timeout)
2022-03-31 06:40:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (775778ms till timeout)
2022-03-31 06:40:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (774774ms till timeout)
2022-03-31 06:40:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (773771ms till timeout)
2022-03-31 06:40:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (772767ms till timeout)
2022-03-31 06:40:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (771763ms till timeout)
2022-03-31 06:40:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (770759ms till timeout)
2022-03-31 06:40:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (769755ms till timeout)
2022-03-31 06:40:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (768752ms till timeout)
2022-03-31 06:40:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (767749ms till timeout)
2022-03-31 06:40:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (766746ms till timeout)
2022-03-31 06:40:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:40:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (765742ms till timeout)
2022-03-31 06:41:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (764739ms till timeout)
2022-03-31 06:41:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (763736ms till timeout)
2022-03-31 06:41:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (762732ms till timeout)
2022-03-31 06:41:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (761729ms till timeout)
2022-03-31 06:41:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (760725ms till timeout)
2022-03-31 06:41:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (759721ms till timeout)
2022-03-31 06:41:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (758718ms till timeout)
2022-03-31 06:41:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (757715ms till timeout)
2022-03-31 06:41:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (756711ms till timeout)
2022-03-31 06:41:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0988b907 is in desired state: Ready
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0988b907-scraper in namespace namespace-6
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0988b907-scraper
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0988b907-scraper will be ready
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0988b907-scraper will be ready
2022-03-31 06:41:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-0988b907-scraper will be ready not ready, will try again in 1000 ms (479991ms till timeout)
2022-03-31 06:41:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-0988b907-scraper will be ready not ready, will try again in 1000 ms (478987ms till timeout)
2022-03-31 06:41:11 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0988b907-scraper is ready
2022-03-31 06:41:11 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-0988b907-scraper to be ready
2022-03-31 06:41:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-31 06:41:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599996ms till timeout)
2022-03-31 06:41:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-31 06:41:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597987ms till timeout)
2022-03-31 06:41:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596983ms till timeout)
2022-03-31 06:41:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595978ms till timeout)
2022-03-31 06:41:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594974ms till timeout)
2022-03-31 06:41:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593970ms till timeout)
2022-03-31 06:41:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592965ms till timeout)
2022-03-31 06:41:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591961ms till timeout)
2022-03-31 06:41:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0988b907-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590956ms till timeout)
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0988b907-scraper-785dfb987b-cjb5h not ready: my-cluster-0988b907-scraper)
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-0988b907-scraper-785dfb987b-cjb5h are ready
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-0988b907-scraper is ready
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-0988b907-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-0988b907-allow, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0988b907, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0988b907-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-0988b907-allow in namespace namespace-6
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-0988b907-allow
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-0988b907 in namespace namespace-6
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-0988b907
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-0988b907 will have desired state: Ready
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-0988b907 will have desired state: Ready
2022-03-31 06:41:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:41:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:41:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (597987ms till timeout)
2022-03-31 06:41:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-31 06:41:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-31 06:41:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-31 06:41:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (593969ms till timeout)
2022-03-31 06:41:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (592964ms till timeout)
2022-03-31 06:41:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (591960ms till timeout)
2022-03-31 06:41:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (590955ms till timeout)
2022-03-31 06:41:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (589945ms till timeout)
2022-03-31 06:41:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (588941ms till timeout)
2022-03-31 06:41:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (587937ms till timeout)
2022-03-31 06:41:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (586934ms till timeout)
2022-03-31 06:41:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (585930ms till timeout)
2022-03-31 06:41:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (584927ms till timeout)
2022-03-31 06:41:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (583924ms till timeout)
2022-03-31 06:41:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (582920ms till timeout)
2022-03-31 06:41:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (581917ms till timeout)
2022-03-31 06:41:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (580913ms till timeout)
2022-03-31 06:41:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (579910ms till timeout)
2022-03-31 06:41:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (578906ms till timeout)
2022-03-31 06:41:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (577903ms till timeout)
2022-03-31 06:41:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (576899ms till timeout)
2022-03-31 06:41:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (575896ms till timeout)
2022-03-31 06:41:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (574893ms till timeout)
2022-03-31 06:41:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (573889ms till timeout)
2022-03-31 06:41:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (572886ms till timeout)
2022-03-31 06:41:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (571882ms till timeout)
2022-03-31 06:41:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (570879ms till timeout)
2022-03-31 06:41:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (569876ms till timeout)
2022-03-31 06:41:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (568872ms till timeout)
2022-03-31 06:41:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (567869ms till timeout)
2022-03-31 06:41:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (566866ms till timeout)
2022-03-31 06:41:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (565862ms till timeout)
2022-03-31 06:41:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (564859ms till timeout)
2022-03-31 06:41:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (563855ms till timeout)
2022-03-31 06:41:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (562852ms till timeout)
2022-03-31 06:41:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:41:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (561849ms till timeout)
2022-03-31 06:42:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (560845ms till timeout)
2022-03-31 06:42:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (559841ms till timeout)
2022-03-31 06:42:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (558838ms till timeout)
2022-03-31 06:42:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (557835ms till timeout)
2022-03-31 06:42:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (556831ms till timeout)
2022-03-31 06:42:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (555828ms till timeout)
2022-03-31 06:42:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (554825ms till timeout)
2022-03-31 06:42:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (553822ms till timeout)
2022-03-31 06:42:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (552818ms till timeout)
2022-03-31 06:42:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (551815ms till timeout)
2022-03-31 06:42:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (550811ms till timeout)
2022-03-31 06:42:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (549808ms till timeout)
2022-03-31 06:42:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (548805ms till timeout)
2022-03-31 06:42:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (547801ms till timeout)
2022-03-31 06:42:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (546798ms till timeout)
2022-03-31 06:42:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (545795ms till timeout)
2022-03-31 06:42:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (544792ms till timeout)
2022-03-31 06:42:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (543788ms till timeout)
2022-03-31 06:42:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (542785ms till timeout)
2022-03-31 06:42:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (541782ms till timeout)
2022-03-31 06:42:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (540778ms till timeout)
2022-03-31 06:42:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (539775ms till timeout)
2022-03-31 06:42:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (538772ms till timeout)
2022-03-31 06:42:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (537768ms till timeout)
2022-03-31 06:42:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (536765ms till timeout)
2022-03-31 06:42:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (535762ms till timeout)
2022-03-31 06:42:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (534759ms till timeout)
2022-03-31 06:42:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (533755ms till timeout)
2022-03-31 06:42:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (532752ms till timeout)
2022-03-31 06:42:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (531749ms till timeout)
2022-03-31 06:42:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (530746ms till timeout)
2022-03-31 06:42:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (529742ms till timeout)
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-0988b907 is in desired state: Ready
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-0988b907 in namespace namespace-6
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-0988b907
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-0988b907 will have desired state: Ready
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnector: my-cluster-0988b907 will have desired state: Ready
2022-03-31 06:42:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaConnector: my-cluster-0988b907 will have desired state: Ready not ready, will try again in 1000 ms (239998ms till timeout)
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-0988b907 is in desired state: Ready
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 exec my-cluster-0988b907-connect-789d5945bd-jjrw4 -- curl -X GET http://localhost:8083/connectors/my-cluster-0988b907/status
2022-03-31 06:42:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-0988b907-connect-789d5945bd-jjrw4 -- curl -X GET http://localhost:8083/connectors/my-cluster-0988b907/status
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-0988b907-hello-world-producer in namespace namespace-6
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-0988b907-hello-world-consumer in namespace namespace-6
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-0988b907-hello-world-producer
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-0988b907-hello-world-producer will be in active state
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-0988b907-hello-world-consumer
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-0988b907-hello-world-consumer will be in active state
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-0988b907-hello-world-producer and consumer my-cluster-0988b907-hello-world-consumer finish
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-31 06:42:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219996ms till timeout)
2022-03-31 06:42:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218993ms till timeout)
2022-03-31 06:42:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217990ms till timeout)
2022-03-31 06:42:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216987ms till timeout)
2022-03-31 06:42:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215984ms till timeout)
2022-03-31 06:42:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214978ms till timeout)
2022-03-31 06:42:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (213973ms till timeout)
2022-03-31 06:42:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-0988b907-hello-world-producer deletion
2022-03-31 06:42:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-0988b907-hello-world-producer to be deleted
2022-03-31 06:42:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-0988b907-hello-world-producer to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-31 06:42:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job my-cluster-0988b907-hello-world-producer was deleted
2022-03-31 06:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-0988b907-hello-world-consumer deletion
2022-03-31 06:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-0988b907-hello-world-consumer to be deleted
2022-03-31 06:42:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-0988b907-hello-world-consumer to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-31 06:42:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job my-cluster-0988b907-hello-world-consumer was deleted
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-0988b907-connect-789d5945bd-jjrw4
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 exec my-cluster-0988b907-connect-789d5945bd-jjrw4 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:421] Command: kubectl --namespace namespace-6 exec my-cluster-0988b907-connect-789d5945bd-jjrw4 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:421] Return code: 0
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-0988b907-connect-789d5945bd-jjrw4
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [connect.ConnectIsolatedST - After Each] - Clean up after test
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testMultiNodeKafkaConnectWithConnectorCreation
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-0988b907 in namespace namespace-6
2022-03-31 06:42:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0988b907-scraper in namespace namespace-6
2022-03-31 06:42:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0988b907-scraper
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-0988b907
2022-03-31 06:42:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0988b907-scraper not ready, will try again in 10000 ms (479993ms till timeout)
2022-03-31 06:42:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-0988b907 not ready, will try again in 10000 ms (239992ms till timeout)
2022-03-31 06:42:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:42:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-0988b907 in namespace namespace-6
2022-03-31 06:43:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0988b907-scraper not ready, will try again in 10000 ms (469985ms till timeout)
2022-03-31 06:43:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-0988b907
2022-03-31 06:43:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-0988b907 not ready, will try again in 10000 ms (599994ms till timeout)
2022-03-31 06:43:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0988b907-scraper not ready, will try again in 10000 ms (459979ms till timeout)
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-0988b907-hello-world-consumer in namespace namespace-6
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-0988b907-hello-world-consumer
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-0988b907-hello-world-producer in namespace namespace-6
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-0988b907-hello-world-producer
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0988b907 in namespace namespace-6
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0988b907
2022-03-31 06:43:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0988b907 not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-31 06:43:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0988b907-scraper not ready, will try again in 10000 ms (449974ms till timeout)
2022-03-31 06:43:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-0988b907-allow in namespace namespace-6
2022-03-31 06:43:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-0988b907-allow
2022-03-31 06:43:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479921ms till timeout)
2022-03-31 06:43:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (478841ms till timeout)
2022-03-31 06:43:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (477768ms till timeout)
2022-03-31 06:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476700ms till timeout)
2022-03-31 06:43:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475624ms till timeout)
2022-03-31 06:43:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474552ms till timeout)
2022-03-31 06:43:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473475ms till timeout)
2022-03-31 06:43:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472404ms till timeout)
2022-03-31 06:43:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-31 06:43:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (471328ms till timeout)
2022-03-31 06:43:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:43:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470259ms till timeout)
2022-03-31 06:43:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[]}
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testMultiNodeKafkaConnectWithConnectorCreation - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation] to and randomly select one to start execution
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [connect.ConnectIsolatedST] - Removing parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [connect.ConnectIsolatedST] - Parallel test count: 0
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-FINISHED
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [connect.ConnectIsolatedST - After All] - Clean up after test suite
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-03-31 06:43:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,688.305 s - in io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMakerIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-31 06:43:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:43:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179984ms till timeout)
2022-03-31 06:43:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:43:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:43:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179896ms till timeout)
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:43:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-31 06:43:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:43:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:43:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:43:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:43:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:43:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179979ms till timeout)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179946ms till timeout)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:43:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:43:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:04 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:44:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:44:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179964ms till timeout)
2022-03-31 06:44:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179833ms till timeout)
2022-03-31 06:44:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v140878
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v140878
2022-03-31 06:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=140878&allowWatchBookmarks=true&watch=true...
2022-03-31 06:44:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:44:14 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 140879
2022-03-31 06:44:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 140905
2022-03-31 06:44:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 140912
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v140905 in namespace default
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@45249bfa
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@411b1786
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=120000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@411b1786
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@411b1786
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:44:24 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace infra-namespace -o json
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace infra-namespace -o json
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:44:24Z",
        "name": "infra-namespace",
        "resourceVersion": "140913",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "64b8832f-9631-4aaf-a25b-ee4f732266e1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:44:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-31 06:44:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-31 06:44:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477989ms till timeout)
2022-03-31 06:44:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-31 06:44:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-31 06:44:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474979ms till timeout)
2022-03-31 06:44:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473976ms till timeout)
2022-03-31 06:44:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472972ms till timeout)
2022-03-31 06:44:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471969ms till timeout)
2022-03-31 06:44:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470966ms till timeout)
2022-03-31 06:44:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469963ms till timeout)
2022-03-31 06:44:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468960ms till timeout)
2022-03-31 06:44:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467956ms till timeout)
2022-03-31 06:44:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466953ms till timeout)
2022-03-31 06:44:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465950ms till timeout)
2022-03-31 06:44:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464946ms till timeout)
2022-03-31 06:44:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463943ms till timeout)
2022-03-31 06:44:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462939ms till timeout)
2022-03-31 06:44:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461936ms till timeout)
2022-03-31 06:44:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460933ms till timeout)
2022-03-31 06:44:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459930ms till timeout)
2022-03-31 06:44:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458927ms till timeout)
2022-03-31 06:44:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457923ms till timeout)
2022-03-31 06:44:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456920ms till timeout)
2022-03-31 06:44:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455917ms till timeout)
2022-03-31 06:44:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454913ms till timeout)
2022-03-31 06:44:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453910ms till timeout)
2022-03-31 06:44:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452907ms till timeout)
2022-03-31 06:44:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451903ms till timeout)
2022-03-31 06:44:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450900ms till timeout)
2022-03-31 06:44:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449897ms till timeout)
2022-03-31 06:44:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448893ms till timeout)
2022-03-31 06:44:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447890ms till timeout)
2022-03-31 06:44:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446886ms till timeout)
2022-03-31 06:44:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:44:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445883ms till timeout)
2022-03-31 06:45:00 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:45:00 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:45:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:45:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:45:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:45:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597987ms till timeout)
2022-03-31 06:45:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-31 06:45:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-31 06:45:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-31 06:45:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593972ms till timeout)
2022-03-31 06:45:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592968ms till timeout)
2022-03-31 06:45:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591964ms till timeout)
2022-03-31 06:45:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590961ms till timeout)
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-pp8hm not ready: strimzi-cluster-operator)
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-pp8hm are ready
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-STARTED
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMakerIsolatedST - Before Each] - Setup test case environment
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMakerIsolatedST] - Adding parallel test: testMirrorMakerTlsAuthenticated
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 1
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testMirrorMakerTlsAuthenticated test now can proceed its execution
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testMirrorMakerTlsAuthenticated
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:45:10Z",
        "name": "namespace-7",
        "resourceVersion": "141021",
        "selfLink": "/api/v1/namespaces/namespace-7",
        "uid": "2d3152aa-8ec7-4831-901e-0bf3574f9fec"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[namespace-7]}
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7dd975ed-source in namespace namespace-7
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7dd975ed-source
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7dd975ed-source will have desired state: Ready
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7dd975ed-source will have desired state: Ready
2022-03-31 06:45:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:45:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-31 06:45:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-31 06:45:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:45:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-31 06:45:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-31 06:45:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-31 06:45:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-31 06:45:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (831971ms till timeout)
2022-03-31 06:45:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-31 06:45:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-31 06:45:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (828962ms till timeout)
2022-03-31 06:45:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (827957ms till timeout)
2022-03-31 06:45:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (826954ms till timeout)
2022-03-31 06:45:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (825951ms till timeout)
2022-03-31 06:45:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-31 06:45:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-31 06:45:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-31 06:45:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (821938ms till timeout)
2022-03-31 06:45:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (820935ms till timeout)
2022-03-31 06:45:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (819932ms till timeout)
2022-03-31 06:45:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (818929ms till timeout)
2022-03-31 06:45:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (817926ms till timeout)
2022-03-31 06:45:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (816923ms till timeout)
2022-03-31 06:45:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (815920ms till timeout)
2022-03-31 06:45:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (814917ms till timeout)
2022-03-31 06:45:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (813914ms till timeout)
2022-03-31 06:45:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (812910ms till timeout)
2022-03-31 06:45:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (811908ms till timeout)
2022-03-31 06:45:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (810905ms till timeout)
2022-03-31 06:45:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (809901ms till timeout)
2022-03-31 06:45:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (808898ms till timeout)
2022-03-31 06:45:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (807895ms till timeout)
2022-03-31 06:45:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (806891ms till timeout)
2022-03-31 06:45:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (805888ms till timeout)
2022-03-31 06:45:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (804885ms till timeout)
2022-03-31 06:45:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (803882ms till timeout)
2022-03-31 06:45:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (802878ms till timeout)
2022-03-31 06:45:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (801875ms till timeout)
2022-03-31 06:45:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (800872ms till timeout)
2022-03-31 06:45:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (799869ms till timeout)
2022-03-31 06:45:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (798866ms till timeout)
2022-03-31 06:45:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (797862ms till timeout)
2022-03-31 06:45:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (796859ms till timeout)
2022-03-31 06:45:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (795856ms till timeout)
2022-03-31 06:45:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (794852ms till timeout)
2022-03-31 06:45:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (793849ms till timeout)
2022-03-31 06:45:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (792846ms till timeout)
2022-03-31 06:45:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (791843ms till timeout)
2022-03-31 06:45:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:45:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (790839ms till timeout)
2022-03-31 06:46:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (789836ms till timeout)
2022-03-31 06:46:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (788833ms till timeout)
2022-03-31 06:46:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (787830ms till timeout)
2022-03-31 06:46:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (786827ms till timeout)
2022-03-31 06:46:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (785824ms till timeout)
2022-03-31 06:46:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (784821ms till timeout)
2022-03-31 06:46:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (783817ms till timeout)
2022-03-31 06:46:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (782814ms till timeout)
2022-03-31 06:46:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (781811ms till timeout)
2022-03-31 06:46:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (780808ms till timeout)
2022-03-31 06:46:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (779804ms till timeout)
2022-03-31 06:46:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (778801ms till timeout)
2022-03-31 06:46:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (777797ms till timeout)
2022-03-31 06:46:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (776794ms till timeout)
2022-03-31 06:46:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (775791ms till timeout)
2022-03-31 06:46:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (774787ms till timeout)
2022-03-31 06:46:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (773784ms till timeout)
2022-03-31 06:46:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (772781ms till timeout)
2022-03-31 06:46:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (771778ms till timeout)
2022-03-31 06:46:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (770775ms till timeout)
2022-03-31 06:46:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (769771ms till timeout)
2022-03-31 06:46:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (768768ms till timeout)
2022-03-31 06:46:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (767765ms till timeout)
2022-03-31 06:46:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (766762ms till timeout)
2022-03-31 06:46:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (765759ms till timeout)
2022-03-31 06:46:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (764755ms till timeout)
2022-03-31 06:46:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (763752ms till timeout)
2022-03-31 06:46:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (762749ms till timeout)
2022-03-31 06:46:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (761746ms till timeout)
2022-03-31 06:46:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (760742ms till timeout)
2022-03-31 06:46:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (759739ms till timeout)
2022-03-31 06:46:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-source will have desired state: Ready not ready, will try again in 1000 ms (758736ms till timeout)
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7dd975ed-source is in desired state: Ready
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7dd975ed-target in namespace namespace-7
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7dd975ed-target
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7dd975ed-target will have desired state: Ready
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7dd975ed-target will have desired state: Ready
2022-03-31 06:46:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:46:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-31 06:46:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-31 06:46:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:46:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-31 06:46:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-31 06:46:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-31 06:46:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-31 06:46:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-31 06:46:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-31 06:46:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-31 06:46:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (828962ms till timeout)
2022-03-31 06:46:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (827959ms till timeout)
2022-03-31 06:46:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (826955ms till timeout)
2022-03-31 06:46:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-31 06:46:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (824949ms till timeout)
2022-03-31 06:46:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (823946ms till timeout)
2022-03-31 06:46:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (822943ms till timeout)
2022-03-31 06:46:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (821940ms till timeout)
2022-03-31 06:46:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (820936ms till timeout)
2022-03-31 06:46:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (819933ms till timeout)
2022-03-31 06:46:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (818930ms till timeout)
2022-03-31 06:46:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (817927ms till timeout)
2022-03-31 06:46:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (816923ms till timeout)
2022-03-31 06:46:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (815920ms till timeout)
2022-03-31 06:46:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (814917ms till timeout)
2022-03-31 06:46:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (813913ms till timeout)
2022-03-31 06:46:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:46:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (812909ms till timeout)
2022-03-31 06:47:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (811906ms till timeout)
2022-03-31 06:47:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (810903ms till timeout)
2022-03-31 06:47:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (809899ms till timeout)
2022-03-31 06:47:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (808896ms till timeout)
2022-03-31 06:47:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (807892ms till timeout)
2022-03-31 06:47:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (806889ms till timeout)
2022-03-31 06:47:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (805885ms till timeout)
2022-03-31 06:47:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (804882ms till timeout)
2022-03-31 06:47:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (803878ms till timeout)
2022-03-31 06:47:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (802875ms till timeout)
2022-03-31 06:47:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (801871ms till timeout)
2022-03-31 06:47:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (800868ms till timeout)
2022-03-31 06:47:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (799865ms till timeout)
2022-03-31 06:47:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (798862ms till timeout)
2022-03-31 06:47:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (797859ms till timeout)
2022-03-31 06:47:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (796855ms till timeout)
2022-03-31 06:47:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (795852ms till timeout)
2022-03-31 06:47:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (794848ms till timeout)
2022-03-31 06:47:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (793845ms till timeout)
2022-03-31 06:47:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (792842ms till timeout)
2022-03-31 06:47:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (791838ms till timeout)
2022-03-31 06:47:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (790834ms till timeout)
2022-03-31 06:47:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (789830ms till timeout)
2022-03-31 06:47:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (788827ms till timeout)
2022-03-31 06:47:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (787824ms till timeout)
2022-03-31 06:47:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (786821ms till timeout)
2022-03-31 06:47:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (785817ms till timeout)
2022-03-31 06:47:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (784814ms till timeout)
2022-03-31 06:47:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (783811ms till timeout)
2022-03-31 06:47:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (782807ms till timeout)
2022-03-31 06:47:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (781804ms till timeout)
2022-03-31 06:47:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (780801ms till timeout)
2022-03-31 06:47:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (779797ms till timeout)
2022-03-31 06:47:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (778794ms till timeout)
2022-03-31 06:47:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7dd975ed-target will have desired state: Ready not ready, will try again in 1000 ms (777791ms till timeout)
2022-03-31 06:47:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7dd975ed-target is in desired state: Ready
2022-03-31 06:47:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1116101568-1197334626-source-1868787774 in namespace namespace-7
2022-03-31 06:47:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:47:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1116101568-1197334626-source-1868787774
2022-03-31 06:47:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1116101568-1197334626-source-1868787774 will have desired state: Ready
2022-03-31 06:47:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1116101568-1197334626-source-1868787774 will have desired state: Ready
2022-03-31 06:47:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1116101568-1197334626-source-1868787774 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1116101568-1197334626-source-1868787774 is in desired state: Ready
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7dd975ed-my-user-source in namespace namespace-7
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-7dd975ed-my-user-source
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7dd975ed-my-user-source will have desired state: Ready
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-7dd975ed-my-user-source will have desired state: Ready
2022-03-31 06:47:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-7dd975ed-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7dd975ed-my-user-source is in desired state: Ready
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7dd975ed-my-user-target in namespace namespace-7
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-7dd975ed-my-user-target
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7dd975ed-my-user-target will have desired state: Ready
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-7dd975ed-my-user-target will have desired state: Ready
2022-03-31 06:47:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-7dd975ed-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:47:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7dd975ed-my-user-target is in desired state: Ready
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7dd975ed-kafka-clients in namespace namespace-7
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-7dd975ed-kafka-clients
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7dd975ed-kafka-clients will be ready
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-7dd975ed-kafka-clients will be ready
2022-03-31 06:47:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7dd975ed-kafka-clients will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-31 06:47:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7dd975ed-kafka-clients will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7dd975ed-kafka-clients is ready
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-7dd975ed-kafka-clients is present.
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-425825936-2110554116-test-1 in namespace namespace-7
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-425825936-2110554116-test-1
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-425825936-2110554116-test-1 will have desired state: Ready
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-425825936-2110554116-test-1 will have desired state: Ready
2022-03-31 06:47:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-425825936-2110554116-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-425825936-2110554116-test-1 is in desired state: Ready
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-425825936-2110554116-test-2 in namespace namespace-7
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-425825936-2110554116-test-2
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-425825936-2110554116-test-2 will have desired state: Ready
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-425825936-2110554116-test-2 will have desired state: Ready
2022-03-31 06:47:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-425825936-2110554116-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-425825936-2110554116-test-2 is in desired state: Ready
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@56556778, which are set.
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7bab5f8, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093, --topic, my-topic-425825936-2110554116-test-1, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_source], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-425825936-2110554116-test-1', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@56556778}
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093:my-topic-425825936-2110554116-test-1 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --topic my-topic-425825936-2110554116-test-1 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source
2022-03-31 06:47:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --topic my-topic-425825936-2110554116-test-1 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source
2022-03-31 06:47:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ef2d801, which are set.
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@73610000, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093, --group-id, my-consumer-group-1669203422, --topic, my-topic-425825936-2110554116-test-1, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_source, --group-instance-id, instance709919010], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-425825936-2110554116-test-1', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-source', consumerGroupName='my-consumer-group-1669203422', consumerInstanceId='instance709919010', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ef2d801}
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093:my-topic-425825936-2110554116-test-1 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-1669203422 --topic my-topic-425825936-2110554116-test-1 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source --group-instance-id instance709919010
2022-03-31 06:47:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-1669203422 --topic my-topic-425825936-2110554116-test-1 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source --group-instance-id instance709919010
2022-03-31 06:47:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@21266ee1, which are set.
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2eb0e9cf, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093, --topic, my-topic-425825936-2110554116-test-2, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_target], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-425825936-2110554116-test-2', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@21266ee1}
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-425825936-2110554116-test-2 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093 --topic my-topic-425825936-2110554116-test-2 --max-messages 200 USER=my_cluster_7dd975ed_my_user_target
2022-03-31 06:47:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093 --topic my-topic-425825936-2110554116-test-2 --max-messages 200 USER=my_cluster_7dd975ed_my_user_target
2022-03-31 06:47:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@369a1ec8, which are set.
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4d80691b, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093, --group-id, my-consumer-group-678956172, --topic, my-topic-425825936-2110554116-test-2, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_target, --group-instance-id, instance1130111203], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-425825936-2110554116-test-2', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-target', consumerGroupName='my-consumer-group-678956172', consumerInstanceId='instance1130111203', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@369a1ec8}
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-425825936-2110554116-test-2 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-678956172 --topic my-topic-425825936-2110554116-test-2 --max-messages 200 USER=my_cluster_7dd975ed_my_user_target --group-instance-id instance1130111203
2022-03-31 06:47:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-678956172 --topic my-topic-425825936-2110554116-test-2 --max-messages 200 USER=my_cluster_7dd975ed_my_user_target --group-instance-id instance1130111203
2022-03-31 06:47:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-7dd975ed in namespace namespace-7
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker:my-cluster-7dd975ed
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready
2022-03-31 06:48:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-31 06:48:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-31 06:48:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-31 06:48:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-31 06:48:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-31 06:48:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (474968ms till timeout)
2022-03-31 06:48:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (473964ms till timeout)
2022-03-31 06:48:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (472961ms till timeout)
2022-03-31 06:48:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (471957ms till timeout)
2022-03-31 06:48:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (470954ms till timeout)
2022-03-31 06:48:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (469950ms till timeout)
2022-03-31 06:48:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (468945ms till timeout)
2022-03-31 06:48:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (467941ms till timeout)
2022-03-31 06:48:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (466937ms till timeout)
2022-03-31 06:48:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (465934ms till timeout)
2022-03-31 06:48:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (464930ms till timeout)
2022-03-31 06:48:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (463927ms till timeout)
2022-03-31 06:48:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (462924ms till timeout)
2022-03-31 06:48:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (461921ms till timeout)
2022-03-31 06:48:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (460917ms till timeout)
2022-03-31 06:48:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (459914ms till timeout)
2022-03-31 06:48:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (458911ms till timeout)
2022-03-31 06:48:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (457907ms till timeout)
2022-03-31 06:48:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (456904ms till timeout)
2022-03-31 06:48:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (455900ms till timeout)
2022-03-31 06:48:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (454897ms till timeout)
2022-03-31 06:48:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (453893ms till timeout)
2022-03-31 06:48:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (452890ms till timeout)
2022-03-31 06:48:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (451886ms till timeout)
2022-03-31 06:48:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (450883ms till timeout)
2022-03-31 06:48:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (449880ms till timeout)
2022-03-31 06:48:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (448877ms till timeout)
2022-03-31 06:48:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (447873ms till timeout)
2022-03-31 06:48:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (446870ms till timeout)
2022-03-31 06:48:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (445867ms till timeout)
2022-03-31 06:48:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (444863ms till timeout)
2022-03-31 06:48:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (443860ms till timeout)
2022-03-31 06:48:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (442856ms till timeout)
2022-03-31 06:48:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (441853ms till timeout)
2022-03-31 06:48:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (440850ms till timeout)
2022-03-31 06:48:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (439846ms till timeout)
2022-03-31 06:48:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (438843ms till timeout)
2022-03-31 06:48:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (437840ms till timeout)
2022-03-31 06:48:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (436836ms till timeout)
2022-03-31 06:48:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (435833ms till timeout)
2022-03-31 06:48:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (434829ms till timeout)
2022-03-31 06:48:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (433826ms till timeout)
2022-03-31 06:48:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (432823ms till timeout)
2022-03-31 06:48:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (431819ms till timeout)
2022-03-31 06:48:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (430816ms till timeout)
2022-03-31 06:48:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (429812ms till timeout)
2022-03-31 06:48:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (428809ms till timeout)
2022-03-31 06:48:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (427805ms till timeout)
2022-03-31 06:48:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (426802ms till timeout)
2022-03-31 06:48:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (425798ms till timeout)
2022-03-31 06:48:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:48:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (424795ms till timeout)
2022-03-31 06:49:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (423792ms till timeout)
2022-03-31 06:49:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (422788ms till timeout)
2022-03-31 06:49:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (421785ms till timeout)
2022-03-31 06:49:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (420781ms till timeout)
2022-03-31 06:49:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (419778ms till timeout)
2022-03-31 06:49:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7dd975ed will have desired state: Ready not ready, will try again in 1000 ms (418775ms till timeout)
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-7dd975ed is in desired state: Ready
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68f8c4b4, which are set.
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5c0bbb38, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093, --topic, my-topic-1116101568-1197334626-source-1868787774, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_source], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1116101568-1197334626-source-1868787774', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68f8c4b4}
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093:my-topic-1116101568-1197334626-source-1868787774 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --topic my-topic-1116101568-1197334626-source-1868787774 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source
2022-03-31 06:49:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --topic my-topic-1116101568-1197334626-source-1868787774 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source
2022-03-31 06:49:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2fd5d064, which are set.
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1a67c4e, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093, --group-id, my-consumer-group-615692985, --topic, my-topic-1116101568-1197334626-source-1868787774, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_source, --group-instance-id, instance1840765480], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1116101568-1197334626-source-1868787774', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-source', consumerGroupName='my-consumer-group-615692985', consumerInstanceId='instance1840765480', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2fd5d064}
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093:my-topic-1116101568-1197334626-source-1868787774 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-615692985 --topic my-topic-1116101568-1197334626-source-1868787774 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source --group-instance-id instance1840765480
2022-03-31 06:49:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-source-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-615692985 --topic my-topic-1116101568-1197334626-source-1868787774 --max-messages 200 USER=my_cluster_7dd975ed_my_user_source --group-instance-id instance1840765480
2022-03-31 06:49:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@75d539, which are set.
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@466aa197, messages=[], arguments=[--bootstrap-server, my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093, --group-id, my-consumer-group-83378592, --topic, my-topic-1116101568-1197334626-source-1868787774, --max-messages, 200, USER=my_cluster_7dd975ed_my_user_target, --group-instance-id, instance1702677506], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls', podNamespace='namespace-7', bootstrapServer='my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1116101568-1197334626-source-1868787774', maxMessages=200, kafkaUsername='my-cluster-7dd975ed-my-user-target', consumerGroupName='my-consumer-group-83378592', consumerInstanceId='instance1702677506', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@75d539}
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-1116101568-1197334626-source-1868787774 from pod my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-83378592 --topic my-topic-1116101568-1197334626-source-1868787774 --max-messages 200 USER=my_cluster_7dd975ed_my_user_target --group-instance-id instance1702677506
2022-03-31 06:49:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7dd975ed-kafka-clients-596979fd98-xbpls -n namespace-7 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7dd975ed-target-kafka-bootstrap.namespace-7.svc:9093 --group-id my-consumer-group-83378592 --topic my-topic-1116101568-1197334626-source-1868787774 --max-messages 200 USER=my_cluster_7dd975ed_my_user_target --group-instance-id instance1702677506
2022-03-31 06:49:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMakerIsolatedST - After Each] - Clean up after test
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerTlsAuthenticated
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7dd975ed-kafka-clients in namespace namespace-7
2022-03-31 06:49:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-425825936-2110554116-test-2 in namespace namespace-7
2022-03-31 06:49:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1116101568-1197334626-source-1868787774 in namespace namespace-7
2022-03-31 06:49:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1116101568-1197334626-source-1868787774
2022-03-31 06:49:23 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-425825936-2110554116-test-2
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7dd975ed-kafka-clients
2022-03-31 06:49:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-425825936-2110554116-test-2 not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-31 06:49:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1116101568-1197334626-source-1868787774 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-31 06:49:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7dd975ed-kafka-clients not ready, will try again in 10000 ms (479987ms till timeout)
2022-03-31 06:49:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-7dd975ed in namespace namespace-7
2022-03-31 06:49:33 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-7dd975ed
2022-03-31 06:49:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1116101568-1197334626-source-1868787774 not ready, will try again in 10000 ms (169974ms till timeout)
2022-03-31 06:49:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7dd975ed-kafka-clients not ready, will try again in 10000 ms (469973ms till timeout)
2022-03-31 06:49:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-7dd975ed not ready, will try again in 10000 ms (479991ms till timeout)
2022-03-31 06:49:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7dd975ed-my-user-source in namespace namespace-7
2022-03-31 06:49:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7dd975ed-my-user-source
2022-03-31 06:49:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-425825936-2110554116-test-1 in namespace namespace-7
2022-03-31 06:49:43 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-425825936-2110554116-test-1
2022-03-31 06:49:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7dd975ed-my-user-source not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-31 06:49:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7dd975ed-kafka-clients not ready, will try again in 10000 ms (459956ms till timeout)
2022-03-31 06:49:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-425825936-2110554116-test-1 not ready, will try again in 10000 ms (179986ms till timeout)
2022-03-31 06:49:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7dd975ed-target in namespace namespace-7
2022-03-31 06:49:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7dd975ed-target
2022-03-31 06:49:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7dd975ed-target not ready, will try again in 10000 ms (839992ms till timeout)
2022-03-31 06:49:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7dd975ed-kafka-clients not ready, will try again in 10000 ms (449943ms till timeout)
2022-03-31 06:49:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7dd975ed-my-user-target in namespace namespace-7
2022-03-31 06:49:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7dd975ed-my-user-target
2022-03-31 06:49:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7dd975ed-my-user-target not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-31 06:49:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:49:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7dd975ed-source in namespace namespace-7
2022-03-31 06:50:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7dd975ed-source
2022-03-31 06:50:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7dd975ed-source not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-31 06:50:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testMirrorMakerTlsAuthenticated
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479925ms till timeout)
2022-03-31 06:50:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478852ms till timeout)
2022-03-31 06:50:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (477785ms till timeout)
2022-03-31 06:50:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476711ms till timeout)
2022-03-31 06:50:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475637ms till timeout)
2022-03-31 06:50:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (474561ms till timeout)
2022-03-31 06:50:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473488ms till timeout)
2022-03-31 06:50:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472417ms till timeout)
2022-03-31 06:50:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (471339ms till timeout)
2022-03-31 06:50:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (470263ms till timeout)
2022-03-31 06:50:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (469191ms till timeout)
2022-03-31 06:50:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (468112ms till timeout)
2022-03-31 06:50:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (467037ms till timeout)
2022-03-31 06:50:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (465959ms till timeout)
2022-03-31 06:50:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (464881ms till timeout)
2022-03-31 06:50:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (463800ms till timeout)
2022-03-31 06:50:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (462728ms till timeout)
2022-03-31 06:50:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (461658ms till timeout)
2022-03-31 06:50:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (460586ms till timeout)
2022-03-31 06:50:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (459516ms till timeout)
2022-03-31 06:50:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (458439ms till timeout)
2022-03-31 06:50:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (457359ms till timeout)
2022-03-31 06:50:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (456277ms till timeout)
2022-03-31 06:50:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (455206ms till timeout)
2022-03-31 06:50:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-31 06:50:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:50:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (454137ms till timeout)
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[]}
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testMirrorMakerTlsAuthenticated - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated] to and randomly select one to start execution
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMakerIsolatedST] - Removing parallel test: testMirrorMakerTlsAuthenticated
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 0
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-FINISHED
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMakerIsolatedST - After All] - Clean up after test suite
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context MirrorMakerIsolatedST is everything deleted.
2022-03-31 06:50:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,673.478 s - in io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMaker2IsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-31 06:50:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:50:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179986ms till timeout)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:50:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179978ms till timeout)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:50:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:50:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179957ms till timeout)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:50:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179803ms till timeout)
2022-03-31 06:50:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:50:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:50:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:50:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:50:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:50:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:50:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:50:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179919ms till timeout)
2022-03-31 06:50:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179904ms till timeout)
2022-03-31 06:50:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179976ms till timeout)
2022-03-31 06:50:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v142000
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v142000
2022-03-31 06:51:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=142000&allowWatchBookmarks=true&watch=true...
2022-03-31 06:51:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:51:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 142001
2022-03-31 06:51:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:09 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 142018
2022-03-31 06:51:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 142038
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v142018 in namespace default
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@48821180
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@282e50cb
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@282e50cb
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@282e50cb
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:51:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace infra-namespace -o json
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace infra-namespace -o json
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:51:30Z",
        "name": "infra-namespace",
        "resourceVersion": "142039",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "72b00dda-cfce-4a78-808b-f162dfee64cc"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:51:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:51:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-31 06:51:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-31 06:51:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477993ms till timeout)
2022-03-31 06:51:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476989ms till timeout)
2022-03-31 06:51:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475986ms till timeout)
2022-03-31 06:51:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474982ms till timeout)
2022-03-31 06:51:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473979ms till timeout)
2022-03-31 06:51:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472976ms till timeout)
2022-03-31 06:51:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471973ms till timeout)
2022-03-31 06:51:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470970ms till timeout)
2022-03-31 06:51:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469967ms till timeout)
2022-03-31 06:51:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468963ms till timeout)
2022-03-31 06:51:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467960ms till timeout)
2022-03-31 06:51:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466957ms till timeout)
2022-03-31 06:51:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465953ms till timeout)
2022-03-31 06:51:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464950ms till timeout)
2022-03-31 06:51:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463947ms till timeout)
2022-03-31 06:51:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462944ms till timeout)
2022-03-31 06:51:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461941ms till timeout)
2022-03-31 06:51:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460937ms till timeout)
2022-03-31 06:51:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459934ms till timeout)
2022-03-31 06:51:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458931ms till timeout)
2022-03-31 06:51:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457928ms till timeout)
2022-03-31 06:51:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456925ms till timeout)
2022-03-31 06:51:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455921ms till timeout)
2022-03-31 06:51:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454918ms till timeout)
2022-03-31 06:51:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453915ms till timeout)
2022-03-31 06:51:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452912ms till timeout)
2022-03-31 06:51:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:51:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451909ms till timeout)
2022-03-31 06:52:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450905ms till timeout)
2022-03-31 06:52:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449902ms till timeout)
2022-03-31 06:52:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448899ms till timeout)
2022-03-31 06:52:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447896ms till timeout)
2022-03-31 06:52:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446893ms till timeout)
2022-03-31 06:52:05 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:52:05 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:52:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:52:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 06:52:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-31 06:52:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-31 06:52:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-31 06:52:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-31 06:52:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-31 06:52:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-31 06:52:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-31 06:52:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591966ms till timeout)
2022-03-31 06:52:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590962ms till timeout)
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-lhrcd not ready: strimzi-cluster-operator)
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-lhrcd are ready
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-STARTED
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMaker2IsolatedST - Before Each] - Setup test case environment
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMaker2IsolatedST] - Adding parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 1
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2TlsAndTlsClientAuth test now can proceed its execution
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:52:15Z",
        "name": "namespace-8",
        "resourceVersion": "142145",
        "selfLink": "/api/v1/namespaces/namespace-8",
        "uid": "bed4082c-ebb1-424d-a63e-78e16da1adb4"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[namespace-8]}
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7df66d40-source in namespace namespace-8
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7df66d40-source
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7df66d40-source will have desired state: Ready
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7df66d40-source will have desired state: Ready
2022-03-31 06:52:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:52:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-31 06:52:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-31 06:52:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-31 06:52:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-31 06:52:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-31 06:52:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-31 06:52:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-31 06:52:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-31 06:52:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-31 06:52:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (829966ms till timeout)
2022-03-31 06:52:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (828962ms till timeout)
2022-03-31 06:52:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (827959ms till timeout)
2022-03-31 06:52:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (826955ms till timeout)
2022-03-31 06:52:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-31 06:52:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-31 06:52:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-31 06:52:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-31 06:52:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (821938ms till timeout)
2022-03-31 06:52:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (820934ms till timeout)
2022-03-31 06:52:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (819931ms till timeout)
2022-03-31 06:52:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (818928ms till timeout)
2022-03-31 06:52:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (817924ms till timeout)
2022-03-31 06:52:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (816921ms till timeout)
2022-03-31 06:52:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (815918ms till timeout)
2022-03-31 06:52:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (814915ms till timeout)
2022-03-31 06:52:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (813911ms till timeout)
2022-03-31 06:52:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (812908ms till timeout)
2022-03-31 06:52:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (811905ms till timeout)
2022-03-31 06:52:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (810901ms till timeout)
2022-03-31 06:52:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (809898ms till timeout)
2022-03-31 06:52:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (808895ms till timeout)
2022-03-31 06:52:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (807892ms till timeout)
2022-03-31 06:52:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (806889ms till timeout)
2022-03-31 06:52:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (805886ms till timeout)
2022-03-31 06:52:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (804882ms till timeout)
2022-03-31 06:52:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (803879ms till timeout)
2022-03-31 06:52:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (802876ms till timeout)
2022-03-31 06:52:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (801872ms till timeout)
2022-03-31 06:52:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (800869ms till timeout)
2022-03-31 06:52:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (799866ms till timeout)
2022-03-31 06:52:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (798863ms till timeout)
2022-03-31 06:52:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (797859ms till timeout)
2022-03-31 06:52:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (796856ms till timeout)
2022-03-31 06:52:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:52:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (795853ms till timeout)
2022-03-31 06:53:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (794850ms till timeout)
2022-03-31 06:53:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (793846ms till timeout)
2022-03-31 06:53:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (792843ms till timeout)
2022-03-31 06:53:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (791840ms till timeout)
2022-03-31 06:53:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (790836ms till timeout)
2022-03-31 06:53:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (789833ms till timeout)
2022-03-31 06:53:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (788830ms till timeout)
2022-03-31 06:53:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (787826ms till timeout)
2022-03-31 06:53:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (786823ms till timeout)
2022-03-31 06:53:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (785819ms till timeout)
2022-03-31 06:53:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (784816ms till timeout)
2022-03-31 06:53:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (783813ms till timeout)
2022-03-31 06:53:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (782809ms till timeout)
2022-03-31 06:53:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (781806ms till timeout)
2022-03-31 06:53:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (780803ms till timeout)
2022-03-31 06:53:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (779800ms till timeout)
2022-03-31 06:53:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (778797ms till timeout)
2022-03-31 06:53:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (777793ms till timeout)
2022-03-31 06:53:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (776790ms till timeout)
2022-03-31 06:53:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (775787ms till timeout)
2022-03-31 06:53:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (774784ms till timeout)
2022-03-31 06:53:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (773780ms till timeout)
2022-03-31 06:53:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (772777ms till timeout)
2022-03-31 06:53:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (771774ms till timeout)
2022-03-31 06:53:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-source will have desired state: Ready not ready, will try again in 1000 ms (770771ms till timeout)
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7df66d40-source is in desired state: Ready
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7df66d40-target in namespace namespace-8
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7df66d40-target
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7df66d40-target will have desired state: Ready
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7df66d40-target will have desired state: Ready
2022-03-31 06:53:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:53:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-31 06:53:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-31 06:53:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-31 06:53:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-31 06:53:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-31 06:53:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-31 06:53:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-31 06:53:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-31 06:53:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-31 06:53:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-31 06:53:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (828962ms till timeout)
2022-03-31 06:53:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (827959ms till timeout)
2022-03-31 06:53:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (826956ms till timeout)
2022-03-31 06:53:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-31 06:53:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (824949ms till timeout)
2022-03-31 06:53:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (823946ms till timeout)
2022-03-31 06:53:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (822943ms till timeout)
2022-03-31 06:53:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (821939ms till timeout)
2022-03-31 06:53:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (820936ms till timeout)
2022-03-31 06:53:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (819933ms till timeout)
2022-03-31 06:53:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (818929ms till timeout)
2022-03-31 06:53:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (817926ms till timeout)
2022-03-31 06:53:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (816923ms till timeout)
2022-03-31 06:53:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (815920ms till timeout)
2022-03-31 06:53:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (814916ms till timeout)
2022-03-31 06:53:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (813913ms till timeout)
2022-03-31 06:53:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (812910ms till timeout)
2022-03-31 06:53:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (811907ms till timeout)
2022-03-31 06:53:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (810905ms till timeout)
2022-03-31 06:53:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (809902ms till timeout)
2022-03-31 06:53:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (808899ms till timeout)
2022-03-31 06:53:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (807895ms till timeout)
2022-03-31 06:53:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (806892ms till timeout)
2022-03-31 06:53:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:53:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (805889ms till timeout)
2022-03-31 06:54:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (804885ms till timeout)
2022-03-31 06:54:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (803882ms till timeout)
2022-03-31 06:54:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (802879ms till timeout)
2022-03-31 06:54:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (801876ms till timeout)
2022-03-31 06:54:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (800873ms till timeout)
2022-03-31 06:54:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (799869ms till timeout)
2022-03-31 06:54:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (798866ms till timeout)
2022-03-31 06:54:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (797863ms till timeout)
2022-03-31 06:54:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (796860ms till timeout)
2022-03-31 06:54:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (795857ms till timeout)
2022-03-31 06:54:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (794853ms till timeout)
2022-03-31 06:54:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (793850ms till timeout)
2022-03-31 06:54:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (792847ms till timeout)
2022-03-31 06:54:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (791843ms till timeout)
2022-03-31 06:54:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (790840ms till timeout)
2022-03-31 06:54:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (789836ms till timeout)
2022-03-31 06:54:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (788833ms till timeout)
2022-03-31 06:54:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (787829ms till timeout)
2022-03-31 06:54:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (786826ms till timeout)
2022-03-31 06:54:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (785823ms till timeout)
2022-03-31 06:54:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (784820ms till timeout)
2022-03-31 06:54:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (783816ms till timeout)
2022-03-31 06:54:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (782813ms till timeout)
2022-03-31 06:54:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (781810ms till timeout)
2022-03-31 06:54:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (780806ms till timeout)
2022-03-31 06:54:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (779803ms till timeout)
2022-03-31 06:54:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (778800ms till timeout)
2022-03-31 06:54:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (777796ms till timeout)
2022-03-31 06:54:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (776793ms till timeout)
2022-03-31 06:54:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (775790ms till timeout)
2022-03-31 06:54:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (774786ms till timeout)
2022-03-31 06:54:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (773783ms till timeout)
2022-03-31 06:54:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (772780ms till timeout)
2022-03-31 06:54:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (771776ms till timeout)
2022-03-31 06:54:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (770773ms till timeout)
2022-03-31 06:54:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7df66d40-target will have desired state: Ready not ready, will try again in 1000 ms (769769ms till timeout)
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7df66d40-target is in desired state: Ready
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-1115679021 in namespace namespace-8
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1115679021
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-1115679021 will have desired state: Ready
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: mirrormaker2-topic-example-1115679021 will have desired state: Ready
2022-03-31 06:54:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: mirrormaker2-topic-example-1115679021 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-1115679021 is in desired state: Ready
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7df66d40-my-user-source in namespace namespace-8
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-7df66d40-my-user-source
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7df66d40-my-user-source will have desired state: Ready
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-7df66d40-my-user-source will have desired state: Ready
2022-03-31 06:54:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-7df66d40-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7df66d40-my-user-source is in desired state: Ready
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7df66d40-my-user-target in namespace namespace-8
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-7df66d40-my-user-target
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7df66d40-my-user-target will have desired state: Ready
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-7df66d40-my-user-target will have desired state: Ready
2022-03-31 06:54:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-7df66d40-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:54:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7df66d40-my-user-target is in desired state: Ready
2022-03-31 06:54:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7df66d40-kafka-clients in namespace namespace-8
2022-03-31 06:54:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:54:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-7df66d40-kafka-clients is present.
2022-03-31 06:54:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-7df66d40-kafka-clients is present. not ready, will try again in 10000 ms (299997ms till timeout)
2022-03-31 06:54:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1512831762-342736066-test-1 in namespace namespace-8
2022-03-31 06:54:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:54:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1512831762-342736066-test-1
2022-03-31 06:54:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1512831762-342736066-test-1 will have desired state: Ready
2022-03-31 06:54:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1512831762-342736066-test-1 will have desired state: Ready
2022-03-31 06:54:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1512831762-342736066-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1512831762-342736066-test-1 is in desired state: Ready
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1512831762-342736066-test-2 in namespace namespace-8
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1512831762-342736066-test-2
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1512831762-342736066-test-2 will have desired state: Ready
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1512831762-342736066-test-2 will have desired state: Ready
2022-03-31 06:54:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1512831762-342736066-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1512831762-342736066-test-2 is in desired state: Ready
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:328] Setting topic to my-topic-1512831762-342736066-test-2, cluster to my-cluster-7df66d40-target and changing user to my-cluster-7df66d40-my-user-target
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:337] Sending messages to - topic my-topic-1512831762-342736066-test-2, cluster my-cluster-7df66d40-target and message count of 200
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@385f2b3c, which are set.
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3bbf0252, messages=[], arguments=[--bootstrap-server, my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093, --topic, my-topic-1512831762-342736066-test-2, --max-messages, 200, USER=my_cluster_7df66d40_my_user_target], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8', podNamespace='namespace-8', bootstrapServer='my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1512831762-342736066-test-2', maxMessages=200, kafkaUsername='my-cluster-7df66d40-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@385f2b3c}
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-1512831762-342736066-test-2 from pod my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093 --topic my-topic-1512831762-342736066-test-2 --max-messages 200 USER=my_cluster_7df66d40_my_user_target
2022-03-31 06:54:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093 --topic my-topic-1512831762-342736066-test-2 --max-messages 200 USER=my_cluster_7df66d40_my_user_target
2022-03-31 06:54:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4da76c1b, which are set.
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@ce0a858, messages=[], arguments=[--bootstrap-server, my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093, --group-id, my-consumer-group-227928044, --topic, my-topic-1512831762-342736066-test-2, --max-messages, 200, USER=my_cluster_7df66d40_my_user_target, --group-instance-id, instance710951406], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8', podNamespace='namespace-8', bootstrapServer='my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1512831762-342736066-test-2', maxMessages=200, kafkaUsername='my-cluster-7df66d40-my-user-target', consumerGroupName='my-consumer-group-227928044', consumerInstanceId='instance710951406', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4da76c1b}
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-1512831762-342736066-test-2 from pod my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093 --group-id my-consumer-group-227928044 --topic my-topic-1512831762-342736066-test-2 --max-messages 200 USER=my_cluster_7df66d40_my_user_target --group-instance-id instance710951406
2022-03-31 06:54:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093 --group-id my-consumer-group-227928044 --topic my-topic-1512831762-342736066-test-2 --max-messages 200 USER=my_cluster_7df66d40_my_user_target --group-instance-id instance710951406
2022-03-31 06:54:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-7df66d40 in namespace namespace-8
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2'
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:my-cluster-7df66d40
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready
2022-03-31 06:55:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 06:55:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:55:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-31 06:55:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-31 06:55:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-31 06:55:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-31 06:55:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-31 06:55:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-31 06:55:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-31 06:55:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-31 06:55:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (589958ms till timeout)
2022-03-31 06:55:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (588955ms till timeout)
2022-03-31 06:55:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (587951ms till timeout)
2022-03-31 06:55:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (586947ms till timeout)
2022-03-31 06:55:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (585944ms till timeout)
2022-03-31 06:55:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (584941ms till timeout)
2022-03-31 06:55:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (583937ms till timeout)
2022-03-31 06:55:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (582933ms till timeout)
2022-03-31 06:55:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (581930ms till timeout)
2022-03-31 06:55:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (580927ms till timeout)
2022-03-31 06:55:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (579923ms till timeout)
2022-03-31 06:55:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (578920ms till timeout)
2022-03-31 06:55:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (577916ms till timeout)
2022-03-31 06:55:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (576913ms till timeout)
2022-03-31 06:55:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (575909ms till timeout)
2022-03-31 06:55:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (574906ms till timeout)
2022-03-31 06:55:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (573903ms till timeout)
2022-03-31 06:55:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (572899ms till timeout)
2022-03-31 06:55:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (571896ms till timeout)
2022-03-31 06:55:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (570892ms till timeout)
2022-03-31 06:55:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (569889ms till timeout)
2022-03-31 06:55:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (568885ms till timeout)
2022-03-31 06:55:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (567881ms till timeout)
2022-03-31 06:55:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (566878ms till timeout)
2022-03-31 06:55:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (565874ms till timeout)
2022-03-31 06:55:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (564871ms till timeout)
2022-03-31 06:55:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (563868ms till timeout)
2022-03-31 06:55:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (562865ms till timeout)
2022-03-31 06:55:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (561861ms till timeout)
2022-03-31 06:55:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (560858ms till timeout)
2022-03-31 06:55:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (559854ms till timeout)
2022-03-31 06:55:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (558851ms till timeout)
2022-03-31 06:55:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (557847ms till timeout)
2022-03-31 06:55:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (556844ms till timeout)
2022-03-31 06:55:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (555840ms till timeout)
2022-03-31 06:55:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (554836ms till timeout)
2022-03-31 06:55:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (553832ms till timeout)
2022-03-31 06:55:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (552828ms till timeout)
2022-03-31 06:55:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (551824ms till timeout)
2022-03-31 06:55:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (550821ms till timeout)
2022-03-31 06:55:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (549817ms till timeout)
2022-03-31 06:55:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (548814ms till timeout)
2022-03-31 06:55:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (547810ms till timeout)
2022-03-31 06:55:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (546807ms till timeout)
2022-03-31 06:55:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (545804ms till timeout)
2022-03-31 06:55:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (544800ms till timeout)
2022-03-31 06:55:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (543797ms till timeout)
2022-03-31 06:55:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:55:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (542794ms till timeout)
2022-03-31 06:56:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (541789ms till timeout)
2022-03-31 06:56:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (540786ms till timeout)
2022-03-31 06:56:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (539782ms till timeout)
2022-03-31 06:56:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (538779ms till timeout)
2022-03-31 06:56:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (537775ms till timeout)
2022-03-31 06:56:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (536772ms till timeout)
2022-03-31 06:56:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (535768ms till timeout)
2022-03-31 06:56:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (534765ms till timeout)
2022-03-31 06:56:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (533761ms till timeout)
2022-03-31 06:56:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (532758ms till timeout)
2022-03-31 06:56:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (531755ms till timeout)
2022-03-31 06:56:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-7df66d40 will have desired state: Ready not ready, will try again in 1000 ms (530751ms till timeout)
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-7df66d40 is in desired state: Ready
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:397] Setting topic to mirrormaker2-topic-example-1115679021, cluster to my-cluster-7df66d40-source and changing user to my-cluster-7df66d40-my-user-source
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:407] Sending messages to - topic mirrormaker2-topic-example-1115679021, cluster my-cluster-7df66d40-source and message count of 200
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4c5a3ef7, which are set.
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@721a73f4, messages=[], arguments=[--bootstrap-server, my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093, --topic, mirrormaker2-topic-example-1115679021, --max-messages, 200, USER=my_cluster_7df66d40_my_user_source], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8', podNamespace='namespace-8', bootstrapServer='my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093', topicName='mirrormaker2-topic-example-1115679021', maxMessages=200, kafkaUsername='my-cluster-7df66d40-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4c5a3ef7}
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093:mirrormaker2-topic-example-1115679021 from pod my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093 --topic mirrormaker2-topic-example-1115679021 --max-messages 200 USER=my_cluster_7df66d40_my_user_source
2022-03-31 06:56:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093 --topic mirrormaker2-topic-example-1115679021 --max-messages 200 USER=my_cluster_7df66d40_my_user_source
2022-03-31 06:56:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:411] Receiving messages from - topic mirrormaker2-topic-example-1115679021, cluster my-cluster-7df66d40-source and message count of 200
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@29e8b1d4, which are set.
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@34588f98, messages=[], arguments=[--bootstrap-server, my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093, --group-id, my-consumer-group-227928044, --topic, mirrormaker2-topic-example-1115679021, --max-messages, 200, USER=my_cluster_7df66d40_my_user_source, --group-instance-id, instance860388110], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8', podNamespace='namespace-8', bootstrapServer='my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093', topicName='mirrormaker2-topic-example-1115679021', maxMessages=200, kafkaUsername='my-cluster-7df66d40-my-user-source', consumerGroupName='my-consumer-group-227928044', consumerInstanceId='instance860388110', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@29e8b1d4}
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093:mirrormaker2-topic-example-1115679021 from pod my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093 --group-id my-consumer-group-227928044 --topic mirrormaker2-topic-example-1115679021 --max-messages 200 USER=my_cluster_7df66d40_my_user_source --group-instance-id instance860388110
2022-03-31 06:56:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7df66d40-source-kafka-bootstrap.namespace-8.svc:9093 --group-id my-consumer-group-227928044 --topic mirrormaker2-topic-example-1115679021 --max-messages 200 USER=my_cluster_7df66d40_my_user_source --group-instance-id instance860388110
2022-03-31 06:56:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:418] Now setting topic to my-cluster-7df66d40-source.mirrormaker2-topic-example-1115679021, cluster to my-cluster-7df66d40-target and user to my-cluster-7df66d40-my-user-target - the messages should be mirrored
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:427] Consumer in target cluster and topic should receive 200 messages
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@157ccf34, which are set.
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5bca8b57, messages=[], arguments=[--bootstrap-server, my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093, --group-id, my-consumer-group-227928044, --topic, my-cluster-7df66d40-source.mirrormaker2-topic-example-1115679021, --max-messages, 200, USER=my_cluster_7df66d40_my_user_target, --group-instance-id, instance1176748402], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8', podNamespace='namespace-8', bootstrapServer='my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-cluster-7df66d40-source.mirrormaker2-topic-example-1115679021', maxMessages=200, kafkaUsername='my-cluster-7df66d40-my-user-target', consumerGroupName='my-consumer-group-227928044', consumerInstanceId='instance1176748402', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@157ccf34}
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093:my-cluster-7df66d40-source.mirrormaker2-topic-example-1115679021 from pod my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093 --group-id my-consumer-group-227928044 --topic my-cluster-7df66d40-source.mirrormaker2-topic-example-1115679021 --max-messages 200 USER=my_cluster_7df66d40_my_user_target --group-instance-id instance1176748402
2022-03-31 06:56:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7df66d40-kafka-clients-79d67cb7b6-7k2t8 -n namespace-8 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-7df66d40-target-kafka-bootstrap.namespace-8.svc:9093 --group-id my-consumer-group-227928044 --topic my-cluster-7df66d40-source.mirrormaker2-topic-example-1115679021 --max-messages 200 USER=my_cluster_7df66d40_my_user_target --group-instance-id instance1176748402
2022-03-31 06:56:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:432] Messages successfully mirrored
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMaker2IsolatedST - After Each] - Clean up after test
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2TlsAndTlsClientAuth
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7df66d40-kafka-clients in namespace namespace-8
2022-03-31 06:56:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-1115679021 in namespace namespace-8
2022-03-31 06:56:30 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1512831762-342736066-test-2 in namespace namespace-8
2022-03-31 06:56:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7df66d40-target in namespace namespace-8
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7df66d40-kafka-clients
2022-03-31 06:56:30 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1512831762-342736066-test-2
2022-03-31 06:56:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7df66d40-target
2022-03-31 06:56:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1115679021
2022-03-31 06:56:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7df66d40-target not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-31 06:56:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7df66d40-kafka-clients not ready, will try again in 10000 ms (479985ms till timeout)
2022-03-31 06:56:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1115679021 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-31 06:56:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1512831762-342736066-test-2 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-31 06:56:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7df66d40-source in namespace namespace-8
2022-03-31 06:56:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7df66d40-source
2022-03-31 06:56:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-7df66d40 in namespace namespace-8
2022-03-31 06:56:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7df66d40-source not ready, will try again in 10000 ms (839990ms till timeout)
2022-03-31 06:56:40 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-7df66d40
2022-03-31 06:56:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1115679021 not ready, will try again in 10000 ms (169964ms till timeout)
2022-03-31 06:56:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7df66d40-kafka-clients not ready, will try again in 10000 ms (469959ms till timeout)
2022-03-31 06:56:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-7df66d40 not ready, will try again in 10000 ms (599989ms till timeout)
2022-03-31 06:56:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7df66d40-my-user-source in namespace namespace-8
2022-03-31 06:56:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7df66d40-my-user-source
2022-03-31 06:56:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7df66d40-my-user-target in namespace namespace-8
2022-03-31 06:56:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7df66d40-my-user-source not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 06:56:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1512831762-342736066-test-1 in namespace namespace-8
2022-03-31 06:56:50 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1512831762-342736066-test-1
2022-03-31 06:56:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7df66d40-kafka-clients not ready, will try again in 10000 ms (459904ms till timeout)
2022-03-31 06:56:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1512831762-342736066-test-1 not ready, will try again in 10000 ms (179985ms till timeout)
2022-03-31 06:56:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7df66d40-my-user-target
2022-03-31 06:56:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7df66d40-my-user-target not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-31 06:56:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:56:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:57:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7df66d40-kafka-clients not ready, will try again in 10000 ms (449895ms till timeout)
2022-03-31 06:57:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:57:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:57:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479923ms till timeout)
2022-03-31 06:57:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:57:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478851ms till timeout)
2022-03-31 06:57:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:57:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (477781ms till timeout)
2022-03-31 06:57:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:57:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476706ms till timeout)
2022-03-31 06:57:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-31 06:57:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:57:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475629ms till timeout)
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[]}
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2TlsAndTlsClientAuth - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth] to and randomly select one to start execution
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMaker2IsolatedST] - Removing parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 0
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-FINISHED
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMaker2IsolatedST - After All] - Clean up after test suite
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-03-31 06:57:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,329.589 s - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:136] Suite specific.SpecificIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-31 06:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:57:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:57:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:57:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:57:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:57:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:57:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:57:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179986ms till timeout)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:57:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:57:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:57:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:57:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:57:19 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:57:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:57:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179978ms till timeout)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179942ms till timeout)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:57:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179936ms till timeout)
2022-03-31 06:57:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:28 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:57:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:57:28 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:57:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:57:28 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:57:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:57:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:57:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179976ms till timeout)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179963ms till timeout)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:57:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179936ms till timeout)
2022-03-31 06:57:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179819ms till timeout)
2022-03-31 06:57:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v143127
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v143127
2022-03-31 06:57:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=143127&allowWatchBookmarks=true&watch=true...
2022-03-31 06:57:39 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:57:39 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 143128
2022-03-31 06:57:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:44 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 143144
2022-03-31 06:57:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:57:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 143166
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v143144 in namespace default
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@65b5d5c4
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3a082203
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3a082203
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3a082203
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:58:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace infra-namespace -o json
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace infra-namespace -o json
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:58:05Z",
        "name": "infra-namespace",
        "resourceVersion": "143167",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "38fcef5e-2a79-4520-844d-2a49078cf7ae"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:58:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:58:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-31 06:58:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-31 06:58:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-31 06:58:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-31 06:58:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-31 06:58:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-31 06:58:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-31 06:58:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472973ms till timeout)
2022-03-31 06:58:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-31 06:58:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-31 06:58:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469964ms till timeout)
2022-03-31 06:58:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468961ms till timeout)
2022-03-31 06:58:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467958ms till timeout)
2022-03-31 06:58:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466955ms till timeout)
2022-03-31 06:58:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465951ms till timeout)
2022-03-31 06:58:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464948ms till timeout)
2022-03-31 06:58:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463945ms till timeout)
2022-03-31 06:58:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462942ms till timeout)
2022-03-31 06:58:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461939ms till timeout)
2022-03-31 06:58:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460936ms till timeout)
2022-03-31 06:58:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459932ms till timeout)
2022-03-31 06:58:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458929ms till timeout)
2022-03-31 06:58:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457926ms till timeout)
2022-03-31 06:58:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:29 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:58:29 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:58:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:58:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:58:30 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:30 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 06:58:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-31 06:58:32 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:32 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-31 06:58:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-31 06:58:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-31 06:58:35 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:35 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-31 06:58:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-31 06:58:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-31 06:58:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-31 06:58:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-8mgfv not ready: strimzi-cluster-operator)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-8mgfv are ready
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [SpecificIsolatedST:503] 0.21.4
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-STARTED
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:659] [specific.SpecificIsolatedST - Before Each] - Setup test case environment
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:58:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:58:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179980ms till timeout)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179977ms till timeout)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479984ms till timeout)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179951ms till timeout)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:58:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179844ms till timeout)
2022-03-31 06:58:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:58:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:58:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:58:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179946ms till timeout)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179925ms till timeout)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:58:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179979ms till timeout)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:58:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v143348
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v143348
2022-03-31 06:58:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=143348&allowWatchBookmarks=true&watch=true...
2022-03-31 06:58:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 06:58:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 143349
2022-03-31 06:59:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 143362
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 143363
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v143362 in namespace default
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@11bf6efc
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5aef82fa
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5aef82fa
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5aef82fa
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 06:59:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T06:59:04Z",
        "name": "infra-namespace",
        "resourceVersion": "143364",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "c8ee4106-6c71-4336-8b9c-61e637451576"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:59:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479982ms till timeout)
2022-03-31 06:59:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478979ms till timeout)
2022-03-31 06:59:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477976ms till timeout)
2022-03-31 06:59:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476973ms till timeout)
2022-03-31 06:59:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475970ms till timeout)
2022-03-31 06:59:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474967ms till timeout)
2022-03-31 06:59:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473964ms till timeout)
2022-03-31 06:59:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472960ms till timeout)
2022-03-31 06:59:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471957ms till timeout)
2022-03-31 06:59:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470954ms till timeout)
2022-03-31 06:59:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469951ms till timeout)
2022-03-31 06:59:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468948ms till timeout)
2022-03-31 06:59:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467946ms till timeout)
2022-03-31 06:59:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466942ms till timeout)
2022-03-31 06:59:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465940ms till timeout)
2022-03-31 06:59:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464936ms till timeout)
2022-03-31 06:59:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463933ms till timeout)
2022-03-31 06:59:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462930ms till timeout)
2022-03-31 06:59:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461927ms till timeout)
2022-03-31 06:59:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460925ms till timeout)
2022-03-31 06:59:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459922ms till timeout)
2022-03-31 06:59:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458918ms till timeout)
2022-03-31 06:59:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457915ms till timeout)
2022-03-31 06:59:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456912ms till timeout)
2022-03-31 06:59:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455909ms till timeout)
2022-03-31 06:59:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454906ms till timeout)
2022-03-31 06:59:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453902ms till timeout)
2022-03-31 06:59:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452899ms till timeout)
2022-03-31 06:59:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451896ms till timeout)
2022-03-31 06:59:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:34 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:59:34 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 06:59:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:59:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 06:59:35 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:35 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-31 06:59:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-31 06:59:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-31 06:59:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-31 06:59:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-31 06:59:40 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:40 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-31 06:59:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-31 06:59:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-31 06:59:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-31 06:59:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-78689684d4-8mgfv=4ebd7123-a4a4-4420-8b21-d12137686571}
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-77554ffdfb-k5kbr=6250ed49-6e26-4ee0-be66-7536150390c5}
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 06:59:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 06:59:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-31 06:59:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-31 06:59:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-31 06:59:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-31 06:59:50 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:50 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-31 06:59:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-31 06:59:52 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:52 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-31 06:59:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-31 06:59:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-k5kbr not ready: strimzi-cluster-operator)
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-k5kbr are ready
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1ce8c22c in namespace infra-namespace
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-1ce8c22c
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1ce8c22c will have desired state: Ready
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-1ce8c22c will have desired state: Ready
2022-03-31 06:59:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-31 06:59:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (838996ms till timeout)
2022-03-31 06:59:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-31 06:59:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-31 06:59:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-31 06:59:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 06:59:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-31 07:00:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-31 07:00:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (832973ms till timeout)
2022-03-31 07:00:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (831970ms till timeout)
2022-03-31 07:00:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (830966ms till timeout)
2022-03-31 07:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (829961ms till timeout)
2022-03-31 07:00:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (828958ms till timeout)
2022-03-31 07:00:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (827954ms till timeout)
2022-03-31 07:00:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (826951ms till timeout)
2022-03-31 07:00:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (825948ms till timeout)
2022-03-31 07:00:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (824945ms till timeout)
2022-03-31 07:00:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (823942ms till timeout)
2022-03-31 07:00:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (822938ms till timeout)
2022-03-31 07:00:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (821935ms till timeout)
2022-03-31 07:00:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (820931ms till timeout)
2022-03-31 07:00:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (819928ms till timeout)
2022-03-31 07:00:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (818925ms till timeout)
2022-03-31 07:00:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (817922ms till timeout)
2022-03-31 07:00:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (816919ms till timeout)
2022-03-31 07:00:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (815916ms till timeout)
2022-03-31 07:00:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (814913ms till timeout)
2022-03-31 07:00:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (813909ms till timeout)
2022-03-31 07:00:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (812906ms till timeout)
2022-03-31 07:00:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (811902ms till timeout)
2022-03-31 07:00:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (810899ms till timeout)
2022-03-31 07:00:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (809896ms till timeout)
2022-03-31 07:00:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (808893ms till timeout)
2022-03-31 07:00:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (807890ms till timeout)
2022-03-31 07:00:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (806887ms till timeout)
2022-03-31 07:00:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (805883ms till timeout)
2022-03-31 07:00:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (804879ms till timeout)
2022-03-31 07:00:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (803876ms till timeout)
2022-03-31 07:00:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (802873ms till timeout)
2022-03-31 07:00:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (801870ms till timeout)
2022-03-31 07:00:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (800866ms till timeout)
2022-03-31 07:00:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (799862ms till timeout)
2022-03-31 07:00:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (798859ms till timeout)
2022-03-31 07:00:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (797855ms till timeout)
2022-03-31 07:00:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (796850ms till timeout)
2022-03-31 07:00:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (795847ms till timeout)
2022-03-31 07:00:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (794844ms till timeout)
2022-03-31 07:00:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (793842ms till timeout)
2022-03-31 07:00:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (792839ms till timeout)
2022-03-31 07:00:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (791836ms till timeout)
2022-03-31 07:00:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (790833ms till timeout)
2022-03-31 07:00:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (789829ms till timeout)
2022-03-31 07:00:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (788826ms till timeout)
2022-03-31 07:00:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (787823ms till timeout)
2022-03-31 07:00:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (786820ms till timeout)
2022-03-31 07:00:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (785816ms till timeout)
2022-03-31 07:00:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (784813ms till timeout)
2022-03-31 07:00:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (783810ms till timeout)
2022-03-31 07:00:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (782807ms till timeout)
2022-03-31 07:00:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (781803ms till timeout)
2022-03-31 07:00:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (780800ms till timeout)
2022-03-31 07:00:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (779797ms till timeout)
2022-03-31 07:00:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (778794ms till timeout)
2022-03-31 07:00:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (777791ms till timeout)
2022-03-31 07:00:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (776787ms till timeout)
2022-03-31 07:00:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (775784ms till timeout)
2022-03-31 07:00:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:00:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (774781ms till timeout)
2022-03-31 07:01:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (773777ms till timeout)
2022-03-31 07:01:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (772774ms till timeout)
2022-03-31 07:01:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (771771ms till timeout)
2022-03-31 07:01:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (770768ms till timeout)
2022-03-31 07:01:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (769765ms till timeout)
2022-03-31 07:01:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (768761ms till timeout)
2022-03-31 07:01:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (767758ms till timeout)
2022-03-31 07:01:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (766755ms till timeout)
2022-03-31 07:01:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (765752ms till timeout)
2022-03-31 07:01:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (764749ms till timeout)
2022-03-31 07:01:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (763746ms till timeout)
2022-03-31 07:01:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1ce8c22c is in desired state: Ready
2022-03-31 07:01:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic rw-my-topic-895596137-787671 in namespace infra-namespace
2022-03-31 07:01:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:rw-my-topic-895596137-787671
2022-03-31 07:01:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: rw-my-topic-895596137-787671 will have desired state: Ready
2022-03-31 07:01:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: rw-my-topic-895596137-787671 will have desired state: Ready
2022-03-31 07:01:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaTopic: rw-my-topic-895596137-787671 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:01:12 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] KafkaTopic: rw-my-topic-895596137-787671 is in desired state: Ready
2022-03-31 07:01:12 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1ce8c22c-kafka-clients in namespace infra-namespace
2022-03-31 07:01:12 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-1ce8c22c-kafka-clients
2022-03-31 07:01:12 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1ce8c22c-kafka-clients will be ready
2022-03-31 07:01:12 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-1ce8c22c-kafka-clients will be ready
2022-03-31 07:01:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-1ce8c22c-kafka-clients will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-31 07:01:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-1ce8c22c-kafka-clients will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-31 07:01:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1ce8c22c-kafka-clients is ready
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [32mINFO [m [SpecificIsolatedST:308] Deploy KafkaConnect with correct rack-aware topology key: rack-key
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1ce8c22c-scraper in namespace infra-namespace
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-1ce8c22c-scraper
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1ce8c22c-scraper will be ready
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-1ce8c22c-scraper will be ready
2022-03-31 07:01:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-1ce8c22c-scraper will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-31 07:01:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-1ce8c22c-scraper will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-31 07:01:16 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1ce8c22c-scraper is ready
2022-03-31 07:01:16 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-1ce8c22c-scraper to be ready
2022-03-31 07:01:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-31 07:01:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 07:01:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-31 07:01:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-31 07:01:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:19 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:19 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-31 07:01:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-31 07:01:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-31 07:01:22 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:22 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593971ms till timeout)
2022-03-31 07:01:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592967ms till timeout)
2022-03-31 07:01:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:24 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:24 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591961ms till timeout)
2022-03-31 07:01:25 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:25 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-1ce8c22c-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590957ms till timeout)
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-1ce8c22c-scraper-774f6d845-xq94p not ready: my-cluster-1ce8c22c-scraper)
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-1ce8c22c-scraper-774f6d845-xq94p are ready
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-1ce8c22c-scraper is ready
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1ce8c22c-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-1ce8c22c-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-1ce8c22c, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-1ce8c22c-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-1ce8c22c-allow in namespace infra-namespace
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-1ce8c22c-allow
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-1ce8c22c in namespace infra-namespace
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-1ce8c22c
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready
2022-03-31 07:01:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 07:01:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 07:01:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-31 07:01:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (596989ms till timeout)
2022-03-31 07:01:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-31 07:01:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-31 07:01:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (593979ms till timeout)
2022-03-31 07:01:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (592976ms till timeout)
2022-03-31 07:01:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (591973ms till timeout)
2022-03-31 07:01:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (590970ms till timeout)
2022-03-31 07:01:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (589966ms till timeout)
2022-03-31 07:01:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (588963ms till timeout)
2022-03-31 07:01:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (587960ms till timeout)
2022-03-31 07:01:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (586957ms till timeout)
2022-03-31 07:01:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (585953ms till timeout)
2022-03-31 07:01:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (584950ms till timeout)
2022-03-31 07:01:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (583947ms till timeout)
2022-03-31 07:01:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (582944ms till timeout)
2022-03-31 07:01:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (581940ms till timeout)
2022-03-31 07:01:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (580937ms till timeout)
2022-03-31 07:01:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (579934ms till timeout)
2022-03-31 07:01:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (578930ms till timeout)
2022-03-31 07:01:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (577928ms till timeout)
2022-03-31 07:01:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (576924ms till timeout)
2022-03-31 07:01:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (575921ms till timeout)
2022-03-31 07:01:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (574918ms till timeout)
2022-03-31 07:01:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (573915ms till timeout)
2022-03-31 07:01:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (572912ms till timeout)
2022-03-31 07:01:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (571908ms till timeout)
2022-03-31 07:01:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (570905ms till timeout)
2022-03-31 07:01:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (569902ms till timeout)
2022-03-31 07:01:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (568898ms till timeout)
2022-03-31 07:01:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (567895ms till timeout)
2022-03-31 07:01:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:01:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (566892ms till timeout)
2022-03-31 07:02:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (565889ms till timeout)
2022-03-31 07:02:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (564886ms till timeout)
2022-03-31 07:02:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (563882ms till timeout)
2022-03-31 07:02:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (562879ms till timeout)
2022-03-31 07:02:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (561876ms till timeout)
2022-03-31 07:02:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (560873ms till timeout)
2022-03-31 07:02:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (559869ms till timeout)
2022-03-31 07:02:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (558866ms till timeout)
2022-03-31 07:02:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (557863ms till timeout)
2022-03-31 07:02:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (556860ms till timeout)
2022-03-31 07:02:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (555857ms till timeout)
2022-03-31 07:02:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (554854ms till timeout)
2022-03-31 07:02:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (553850ms till timeout)
2022-03-31 07:02:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (552847ms till timeout)
2022-03-31 07:02:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (551843ms till timeout)
2022-03-31 07:02:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (550840ms till timeout)
2022-03-31 07:02:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (549837ms till timeout)
2022-03-31 07:02:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (548834ms till timeout)
2022-03-31 07:02:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (547831ms till timeout)
2022-03-31 07:02:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (546828ms till timeout)
2022-03-31 07:02:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (545824ms till timeout)
2022-03-31 07:02:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (544822ms till timeout)
2022-03-31 07:02:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (543818ms till timeout)
2022-03-31 07:02:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (542815ms till timeout)
2022-03-31 07:02:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (541812ms till timeout)
2022-03-31 07:02:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (540809ms till timeout)
2022-03-31 07:02:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (539806ms till timeout)
2022-03-31 07:02:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (538802ms till timeout)
2022-03-31 07:02:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (537796ms till timeout)
2022-03-31 07:02:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (536793ms till timeout)
2022-03-31 07:02:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (535789ms till timeout)
2022-03-31 07:02:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (534786ms till timeout)
2022-03-31 07:02:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (533783ms till timeout)
2022-03-31 07:02:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (532779ms till timeout)
2022-03-31 07:02:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (531776ms till timeout)
2022-03-31 07:02:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (530773ms till timeout)
2022-03-31 07:02:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (529770ms till timeout)
2022-03-31 07:02:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (528767ms till timeout)
2022-03-31 07:02:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (527764ms till timeout)
2022-03-31 07:02:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (526761ms till timeout)
2022-03-31 07:02:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-1ce8c22c will have desired state: Ready not ready, will try again in 1000 ms (525758ms till timeout)
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-1ce8c22c is in desired state: Ready
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1ce8c22c-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-1ce8c22c-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-1ce8c22c, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-1ce8c22c-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-1ce8c22c-allow in namespace infra-namespace
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-1ce8c22c-allow
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [SpecificIsolatedST:333] PodName: my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq
NodeAffinity: NodeAffinity(preferredDuringSchedulingIgnoredDuringExecution=[], requiredDuringSchedulingIgnoredDuringExecution=NodeSelector(nodeSelectorTerms=[NodeSelectorTerm(matchExpressions=[NodeSelectorRequirement(key=rack-key, operator=Exists, values=[], additionalProperties={})], matchFields=[], additionalProperties={})], additionalProperties={}), additionalProperties={})
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:154] Send and receive messages through KafkaConnect
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Waiting until KafkaConnect API is available
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Return code: 0
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2 -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-895596137-787671", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-1ce8c22c-connect-api.infra-namespace.svc:8083/connectors
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2 -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-895596137-787671", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-1ce8c22c-connect-api.infra-namespace.svc:8083/connectors
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Return code: 0
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3252804d, which are set.
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6634df16, messages=[], arguments=[--bootstrap-server, my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092, --topic, rw-my-topic-895596137-787671, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2', podNamespace='infra-namespace', bootstrapServer='my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-895596137-787671', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3252804d}
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092:rw-my-topic-895596137-787671 from pod my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2 -n infra-namespace -- /opt/kafka/producer.sh --bootstrap-server my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092 --topic rw-my-topic-895596137-787671 --max-messages 100
2022-03-31 07:02:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2 -n infra-namespace -- /opt/kafka/producer.sh --bootstrap-server my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092 --topic rw-my-topic-895596137-787671 --max-messages 100
2022-03-31 07:02:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5b6efbb7, which are set.
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2dfec976, messages=[], arguments=[--bootstrap-server, my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092, --group-id, my-consumer-group-1969666193, --topic, rw-my-topic-895596137-787671, --max-messages, 100, --group-instance-id, instance1567964543], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2', podNamespace='infra-namespace', bootstrapServer='my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-895596137-787671', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1969666193', consumerInstanceId='instance1567964543', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5b6efbb7}
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092#rw-my-topic-895596137-787671 from pod my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2 -n infra-namespace -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092 --group-id my-consumer-group-1969666193 --topic rw-my-topic-895596137-787671 --max-messages 100 --group-instance-id instance1567964543
2022-03-31 07:02:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-1ce8c22c-kafka-clients-74f8945b6d-94jx2 -n infra-namespace -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-1ce8c22c-kafka-bootstrap.infra-namespace.svc:9092 --group-id my-consumer-group-1969666193 --topic rw-my-topic-895596137-787671 --max-messages 100 --group-instance-id instance1567964543
2022-03-31 07:02:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:421] Command: kubectl --namespace infra-namespace exec my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:421] Return code: 0
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-1ce8c22c-connect-7ffccc6db8-5xprq
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 07:02:50 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 07:02:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 07:02:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 07:02:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:02:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:02:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 07:02:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:02:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 07:02:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179985ms till timeout)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:02:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179985ms till timeout)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:02:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479979ms till timeout)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179938ms till timeout)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 07:02:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179982ms till timeout)
2022-03-31 07:02:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:02:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:03:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 07:03:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179941ms till timeout)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179951ms till timeout)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 07:03:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (469931ms till timeout)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:01 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 07:03:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179772ms till timeout)
2022-03-31 07:03:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v144115
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v144115
2022-03-31 07:03:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=144115&allowWatchBookmarks=true&watch=true...
2022-03-31 07:03:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 07:03:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 144116
2022-03-31 07:03:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 144244
2022-03-31 07:03:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 144287
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v144244 in namespace default
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@412e0e63
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4511929e
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4511929e
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4511929e
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 07:03:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T07:03:54Z",
        "name": "infra-namespace",
        "resourceVersion": "144288",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "9dc15ef5-e0a5-45d9-a02c-4ea01ccb70d7"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 07:03:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479995ms till timeout)
2022-03-31 07:03:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478992ms till timeout)
2022-03-31 07:03:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477989ms till timeout)
2022-03-31 07:03:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-31 07:03:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-31 07:03:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:03:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-31 07:04:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-31 07:04:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-31 07:04:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-31 07:04:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470968ms till timeout)
2022-03-31 07:04:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:04:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469965ms till timeout)
2022-03-31 07:04:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468961ms till timeout)
2022-03-31 07:04:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467958ms till timeout)
2022-03-31 07:04:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466955ms till timeout)
2022-03-31 07:04:08 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 07:04:08 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 07:04:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 07:04:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 07:04:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:04:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 07:04:10 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:10 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-31 07:04:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-31 07:04:12 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:12 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-31 07:04:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-31 07:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:04:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-31 07:04:15 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:15 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-31 07:04:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-31 07:04:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-77554ffdfb-k5kbr=6250ed49-6e26-4ee0-be66-7536150390c5}
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-78689684d4-lr8hq=77c5eedf-9cf2-4dc1-9346-cc56d116bab5}
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 07:04:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:04:19 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:19 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 07:04:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-31 07:04:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-31 07:04:22 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:22 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-31 07:04:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:04:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-31 07:04:24 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:24 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-31 07:04:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-31 07:04:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591970ms till timeout)
2022-03-31 07:04:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-31 07:04:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lr8hq not ready: strimzi-cluster-operator)
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lr8hq are ready
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:675] [specific.SpecificIsolatedST - After Each] - Clean up after test
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAwareConnectCorrectDeployment
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-1ce8c22c-allow in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic rw-my-topic-895596137-787671 in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1ce8c22c in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-1ce8c22c in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-1ce8c22c-allow in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:rw-my-topic-895596137-787671
2022-03-31 07:04:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-1ce8c22c
2022-03-31 07:04:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-1ce8c22c
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-1ce8c22c-allow
2022-03-31 07:04:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-1ce8c22c-allow
2022-03-31 07:04:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1ce8c22c-kafka-clients in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1ce8c22c-scraper in namespace infra-namespace
2022-03-31 07:04:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-1ce8c22c-kafka-clients
2022-03-31 07:04:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-1ce8c22c-scraper
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-FINISHED
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:690] [specific.SpecificIsolatedST - After All] - Clean up after test suite
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:346] In context SpecificIsolatedST is everything deleted.
2022-03-31 07:04:29 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,213.044 s - in io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite metrics.MetricsIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:33 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:04:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:04:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:04:33 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 07:04:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-31 07:04:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:04:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 07:04:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 07:04:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:04:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179933ms till timeout)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179933ms till timeout)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 07:04:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479983ms till timeout)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 07:04:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179951ms till timeout)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 07:04:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179971ms till timeout)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 07:04:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 07:04:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 07:04:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 07:04:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 07:04:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179936ms till timeout)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179981ms till timeout)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179982ms till timeout)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 07:04:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179903ms till timeout)
2022-03-31 07:04:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179823ms till timeout)
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v144476
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v144476
2022-03-31 07:04:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=144476&allowWatchBookmarks=true&watch=true...
2022-03-31 07:04:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 07:04:54 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 144477
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 144493
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 144494
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v144493 in namespace default
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@458cc367
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@41b34f0e
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@41b34f0e
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-metrics-cluster-test
bindingsNamespaces=[infra-namespace, second-metrics-cluster-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@41b34f0e
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@30e56259, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4a773f2d, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace,second-metrics-cluster-test', bindingsNamespaces=[infra-namespace, second-metrics-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 07:04:59 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T07:04:59Z",
        "name": "infra-namespace",
        "resourceVersion": "144495",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "85850254-62a2-409b-a702-b9d19014f3f6"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-metrics-cluster-test
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-metrics-cluster-test
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-31T07:04:59Z",
        "name": "second-metrics-cluster-test",
        "resourceVersion": "144499",
        "selfLink": "/api/v1/namespaces/second-metrics-cluster-test",
        "uid": "b3650c17-1b22-48b0-98ed-4379c1261836"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-metrics-cluster-test]}
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-metrics-cluster-test
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-31 07:05:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479999ms till timeout)
2022-03-31 07:05:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478996ms till timeout)
2022-03-31 07:05:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477993ms till timeout)
2022-03-31 07:05:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476990ms till timeout)
2022-03-31 07:05:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475987ms till timeout)
2022-03-31 07:05:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474984ms till timeout)
2022-03-31 07:05:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473981ms till timeout)
2022-03-31 07:05:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472978ms till timeout)
2022-03-31 07:05:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471975ms till timeout)
2022-03-31 07:05:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470971ms till timeout)
2022-03-31 07:05:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469968ms till timeout)
2022-03-31 07:05:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468965ms till timeout)
2022-03-31 07:05:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467962ms till timeout)
2022-03-31 07:05:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466959ms till timeout)
2022-03-31 07:05:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465956ms till timeout)
2022-03-31 07:05:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464953ms till timeout)
2022-03-31 07:05:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463949ms till timeout)
2022-03-31 07:05:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462946ms till timeout)
2022-03-31 07:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-31 07:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-31 07:05:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-31 07:05:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 07:05:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 07:05:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-31 07:05:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-31 07:05:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-31 07:05:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-31 07:05:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-31 07:05:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-31 07:05:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-31 07:05:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-wjdmb not ready: strimzi-cluster-operator)
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-wjdmb are ready
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka metrics-cluster-name in namespace infra-namespace
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:metrics-cluster-name
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-31 07:05:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1799994ms till timeout)
2022-03-31 07:05:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-31 07:05:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1797987ms till timeout)
2022-03-31 07:05:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1796984ms till timeout)
2022-03-31 07:05:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1795980ms till timeout)
2022-03-31 07:05:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1794977ms till timeout)
2022-03-31 07:05:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1793973ms till timeout)
2022-03-31 07:05:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1792970ms till timeout)
2022-03-31 07:05:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1791965ms till timeout)
2022-03-31 07:05:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1790962ms till timeout)
2022-03-31 07:05:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1789958ms till timeout)
2022-03-31 07:05:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1788955ms till timeout)
2022-03-31 07:05:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1787949ms till timeout)
2022-03-31 07:05:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1786945ms till timeout)
2022-03-31 07:05:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1785940ms till timeout)
2022-03-31 07:05:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1784937ms till timeout)
2022-03-31 07:05:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1783933ms till timeout)
2022-03-31 07:05:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1782929ms till timeout)
2022-03-31 07:05:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1781926ms till timeout)
2022-03-31 07:05:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1780923ms till timeout)
2022-03-31 07:05:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1779918ms till timeout)
2022-03-31 07:05:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1778915ms till timeout)
2022-03-31 07:05:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1777911ms till timeout)
2022-03-31 07:05:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1776907ms till timeout)
2022-03-31 07:05:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1775904ms till timeout)
2022-03-31 07:05:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1774901ms till timeout)
2022-03-31 07:05:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1773898ms till timeout)
2022-03-31 07:05:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1772895ms till timeout)
2022-03-31 07:05:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1771891ms till timeout)
2022-03-31 07:05:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1770888ms till timeout)
2022-03-31 07:05:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1769885ms till timeout)
2022-03-31 07:05:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1768881ms till timeout)
2022-03-31 07:06:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1767878ms till timeout)
2022-03-31 07:06:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1766875ms till timeout)
2022-03-31 07:06:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1765872ms till timeout)
2022-03-31 07:06:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1764868ms till timeout)
2022-03-31 07:06:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1763865ms till timeout)
2022-03-31 07:06:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1762862ms till timeout)
2022-03-31 07:06:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1761859ms till timeout)
2022-03-31 07:06:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1760855ms till timeout)
2022-03-31 07:06:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1759852ms till timeout)
2022-03-31 07:06:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1758849ms till timeout)
2022-03-31 07:06:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1757846ms till timeout)
2022-03-31 07:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1756842ms till timeout)
2022-03-31 07:06:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1755839ms till timeout)
2022-03-31 07:06:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1754836ms till timeout)
2022-03-31 07:06:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1753833ms till timeout)
2022-03-31 07:06:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1752830ms till timeout)
2022-03-31 07:06:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1751827ms till timeout)
2022-03-31 07:06:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1750824ms till timeout)
2022-03-31 07:06:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1749821ms till timeout)
2022-03-31 07:06:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1748818ms till timeout)
2022-03-31 07:06:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1747815ms till timeout)
2022-03-31 07:06:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1746812ms till timeout)
2022-03-31 07:06:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1745809ms till timeout)
2022-03-31 07:06:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1744806ms till timeout)
2022-03-31 07:06:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1743802ms till timeout)
2022-03-31 07:06:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1742799ms till timeout)
2022-03-31 07:06:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1741796ms till timeout)
2022-03-31 07:06:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1740793ms till timeout)
2022-03-31 07:06:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1739789ms till timeout)
2022-03-31 07:06:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1738786ms till timeout)
2022-03-31 07:06:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1737782ms till timeout)
2022-03-31 07:06:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1736779ms till timeout)
2022-03-31 07:06:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1735775ms till timeout)
2022-03-31 07:06:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1734772ms till timeout)
2022-03-31 07:06:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1733769ms till timeout)
2022-03-31 07:06:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1732765ms till timeout)
2022-03-31 07:06:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1731762ms till timeout)
2022-03-31 07:06:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1730759ms till timeout)
2022-03-31 07:06:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1729756ms till timeout)
2022-03-31 07:06:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1728753ms till timeout)
2022-03-31 07:06:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1727749ms till timeout)
2022-03-31 07:06:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1726746ms till timeout)
2022-03-31 07:06:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1725743ms till timeout)
2022-03-31 07:06:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1724740ms till timeout)
2022-03-31 07:06:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1723737ms till timeout)
2022-03-31 07:06:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1722733ms till timeout)
2022-03-31 07:06:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1721730ms till timeout)
2022-03-31 07:06:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1720725ms till timeout)
2022-03-31 07:06:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1719722ms till timeout)
2022-03-31 07:06:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1718719ms till timeout)
2022-03-31 07:06:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1717715ms till timeout)
2022-03-31 07:06:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1716712ms till timeout)
2022-03-31 07:06:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1715709ms till timeout)
2022-03-31 07:06:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1714706ms till timeout)
2022-03-31 07:06:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1713703ms till timeout)
2022-03-31 07:06:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1712700ms till timeout)
2022-03-31 07:06:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1711696ms till timeout)
2022-03-31 07:06:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1710692ms till timeout)
2022-03-31 07:06:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1709689ms till timeout)
2022-03-31 07:06:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1708686ms till timeout)
2022-03-31 07:07:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1707682ms till timeout)
2022-03-31 07:07:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1706679ms till timeout)
2022-03-31 07:07:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1705675ms till timeout)
2022-03-31 07:07:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1704672ms till timeout)
2022-03-31 07:07:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1703668ms till timeout)
2022-03-31 07:07:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1702665ms till timeout)
2022-03-31 07:07:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1701662ms till timeout)
2022-03-31 07:07:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1700659ms till timeout)
2022-03-31 07:07:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1699656ms till timeout)
2022-03-31 07:07:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1698652ms till timeout)
2022-03-31 07:07:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1697648ms till timeout)
2022-03-31 07:07:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1696645ms till timeout)
2022-03-31 07:07:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1695641ms till timeout)
2022-03-31 07:07:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1694636ms till timeout)
2022-03-31 07:07:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1693632ms till timeout)
2022-03-31 07:07:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1692629ms till timeout)
2022-03-31 07:07:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1691626ms till timeout)
2022-03-31 07:07:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1690622ms till timeout)
2022-03-31 07:07:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1689617ms till timeout)
2022-03-31 07:07:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1688613ms till timeout)
2022-03-31 07:07:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1687607ms till timeout)
2022-03-31 07:07:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1686602ms till timeout)
2022-03-31 07:07:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1685599ms till timeout)
2022-03-31 07:07:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1684596ms till timeout)
2022-03-31 07:07:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1683593ms till timeout)
2022-03-31 07:07:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1682590ms till timeout)
2022-03-31 07:07:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1681586ms till timeout)
2022-03-31 07:07:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1680583ms till timeout)
2022-03-31 07:07:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1679579ms till timeout)
2022-03-31 07:07:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1678576ms till timeout)
2022-03-31 07:07:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1677573ms till timeout)
2022-03-31 07:07:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1676569ms till timeout)
2022-03-31 07:07:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1675566ms till timeout)
2022-03-31 07:07:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1674562ms till timeout)
2022-03-31 07:07:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1673559ms till timeout)
2022-03-31 07:07:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1672556ms till timeout)
2022-03-31 07:07:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1671552ms till timeout)
2022-03-31 07:07:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1670548ms till timeout)
2022-03-31 07:07:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1669544ms till timeout)
2022-03-31 07:07:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1668541ms till timeout)
2022-03-31 07:07:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1667538ms till timeout)
2022-03-31 07:07:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1666535ms till timeout)
2022-03-31 07:07:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1665531ms till timeout)
2022-03-31 07:07:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1664527ms till timeout)
2022-03-31 07:07:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1663524ms till timeout)
2022-03-31 07:07:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1662519ms till timeout)
2022-03-31 07:07:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1661516ms till timeout)
2022-03-31 07:07:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1660512ms till timeout)
2022-03-31 07:07:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1659509ms till timeout)
2022-03-31 07:07:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1658504ms till timeout)
2022-03-31 07:07:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1657501ms till timeout)
2022-03-31 07:07:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1656497ms till timeout)
2022-03-31 07:07:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1655494ms till timeout)
2022-03-31 07:07:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1654490ms till timeout)
2022-03-31 07:07:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1653487ms till timeout)
2022-03-31 07:07:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1652483ms till timeout)
2022-03-31 07:07:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1651480ms till timeout)
2022-03-31 07:07:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1650477ms till timeout)
2022-03-31 07:07:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1649474ms till timeout)
2022-03-31 07:08:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1648470ms till timeout)
2022-03-31 07:08:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1647467ms till timeout)
2022-03-31 07:08:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1646463ms till timeout)
2022-03-31 07:08:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1645460ms till timeout)
2022-03-31 07:08:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1644456ms till timeout)
2022-03-31 07:08:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1643453ms till timeout)
2022-03-31 07:08:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1642450ms till timeout)
2022-03-31 07:08:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1641446ms till timeout)
2022-03-31 07:08:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1640443ms till timeout)
2022-03-31 07:08:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1639439ms till timeout)
2022-03-31 07:08:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1638428ms till timeout)
2022-03-31 07:08:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1637424ms till timeout)
2022-03-31 07:08:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1636421ms till timeout)
2022-03-31 07:08:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1635417ms till timeout)
2022-03-31 07:08:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1634414ms till timeout)
2022-03-31 07:08:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1633411ms till timeout)
2022-03-31 07:08:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1632407ms till timeout)
2022-03-31 07:08:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1631404ms till timeout)
2022-03-31 07:08:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1630401ms till timeout)
2022-03-31 07:08:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1629397ms till timeout)
2022-03-31 07:08:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1628394ms till timeout)
2022-03-31 07:08:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1627391ms till timeout)
2022-03-31 07:08:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1626387ms till timeout)
2022-03-31 07:08:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1625384ms till timeout)
2022-03-31 07:08:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1624380ms till timeout)
2022-03-31 07:08:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1623377ms till timeout)
2022-03-31 07:08:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1622373ms till timeout)
2022-03-31 07:08:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1621370ms till timeout)
2022-03-31 07:08:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1620367ms till timeout)
2022-03-31 07:08:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1619363ms till timeout)
2022-03-31 07:08:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1618360ms till timeout)
2022-03-31 07:08:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1617356ms till timeout)
2022-03-31 07:08:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1616353ms till timeout)
2022-03-31 07:08:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1615350ms till timeout)
2022-03-31 07:08:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1614346ms till timeout)
2022-03-31 07:08:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1613343ms till timeout)
2022-03-31 07:08:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1612339ms till timeout)
2022-03-31 07:08:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1611336ms till timeout)
2022-03-31 07:08:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1610333ms till timeout)
2022-03-31 07:08:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1609330ms till timeout)
2022-03-31 07:08:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1608326ms till timeout)
2022-03-31 07:08:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1607323ms till timeout)
2022-03-31 07:08:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1606319ms till timeout)
2022-03-31 07:08:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1605316ms till timeout)
2022-03-31 07:08:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1604313ms till timeout)
2022-03-31 07:08:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1603309ms till timeout)
2022-03-31 07:08:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1602306ms till timeout)
2022-03-31 07:08:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1601302ms till timeout)
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: metrics-cluster-name is in desired state: Ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:second-kafka-cluster
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: second-kafka-cluster is in desired state: Ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-kafka-clients is ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-metrics-cluster-test-kafka-clients is ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-bridge in namespace infra-namespace
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:my-bridge
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-bridge will have desired state: Ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: my-bridge will have desired state: Ready
2022-03-31 07:08:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-31 07:08:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-31 07:08:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (477992ms till timeout)
2022-03-31 07:08:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (476989ms till timeout)
2022-03-31 07:08:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (475986ms till timeout)
2022-03-31 07:08:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (474983ms till timeout)
2022-03-31 07:08:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (473980ms till timeout)
2022-03-31 07:08:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (472977ms till timeout)
2022-03-31 07:08:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (471974ms till timeout)
2022-03-31 07:08:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (470971ms till timeout)
2022-03-31 07:08:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (469968ms till timeout)
2022-03-31 07:08:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (468965ms till timeout)
2022-03-31 07:09:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (467962ms till timeout)
2022-03-31 07:09:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (466959ms till timeout)
2022-03-31 07:09:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (465956ms till timeout)
2022-03-31 07:09:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (464953ms till timeout)
2022-03-31 07:09:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (463950ms till timeout)
2022-03-31 07:09:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (462947ms till timeout)
2022-03-31 07:09:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (461944ms till timeout)
2022-03-31 07:09:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (460941ms till timeout)
2022-03-31 07:09:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (459938ms till timeout)
2022-03-31 07:09:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (458936ms till timeout)
2022-03-31 07:09:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (457932ms till timeout)
2022-03-31 07:09:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-bridge is in desired state: Ready
2022-03-31 07:09:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-31 07:09:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-31 07:09:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-31 07:09:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-31 07:09:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-31 07:09:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-31 07:09:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-31 07:09:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-31 07:09:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-31 07:09:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-31 07:09:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-31 07:09:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-31 07:09:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-31 07:09:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-31 07:09:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (589962ms till timeout)
2022-03-31 07:09:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (588959ms till timeout)
2022-03-31 07:09:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (587955ms till timeout)
2022-03-31 07:09:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (586952ms till timeout)
2022-03-31 07:09:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (585949ms till timeout)
2022-03-31 07:09:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (584945ms till timeout)
2022-03-31 07:09:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (583942ms till timeout)
2022-03-31 07:09:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (582939ms till timeout)
2022-03-31 07:09:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (581936ms till timeout)
2022-03-31 07:09:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (580932ms till timeout)
2022-03-31 07:09:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (579929ms till timeout)
2022-03-31 07:09:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (578926ms till timeout)
2022-03-31 07:09:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (577923ms till timeout)
2022-03-31 07:09:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (576919ms till timeout)
2022-03-31 07:09:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (575917ms till timeout)
2022-03-31 07:09:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (574913ms till timeout)
2022-03-31 07:09:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (573910ms till timeout)
2022-03-31 07:09:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (572907ms till timeout)
2022-03-31 07:09:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (571904ms till timeout)
2022-03-31 07:09:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (570901ms till timeout)
2022-03-31 07:09:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (569897ms till timeout)
2022-03-31 07:09:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (568894ms till timeout)
2022-03-31 07:09:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (567891ms till timeout)
2022-03-31 07:09:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (566888ms till timeout)
2022-03-31 07:09:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (565885ms till timeout)
2022-03-31 07:09:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (564882ms till timeout)
2022-03-31 07:09:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (563878ms till timeout)
2022-03-31 07:09:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (562875ms till timeout)
2022-03-31 07:09:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (561872ms till timeout)
2022-03-31 07:09:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (560869ms till timeout)
2022-03-31 07:09:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (559866ms till timeout)
2022-03-31 07:09:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (558862ms till timeout)
2022-03-31 07:09:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (557859ms till timeout)
2022-03-31 07:09:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (556856ms till timeout)
2022-03-31 07:09:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (555853ms till timeout)
2022-03-31 07:09:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (554849ms till timeout)
2022-03-31 07:09:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (553846ms till timeout)
2022-03-31 07:09:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (552843ms till timeout)
2022-03-31 07:09:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (551840ms till timeout)
2022-03-31 07:10:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (550836ms till timeout)
2022-03-31 07:10:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (549833ms till timeout)
2022-03-31 07:10:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (548829ms till timeout)
2022-03-31 07:10:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (547826ms till timeout)
2022-03-31 07:10:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (546822ms till timeout)
2022-03-31 07:10:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (545819ms till timeout)
2022-03-31 07:10:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (544816ms till timeout)
2022-03-31 07:10:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (543812ms till timeout)
2022-03-31 07:10:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (542810ms till timeout)
2022-03-31 07:10:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (541807ms till timeout)
2022-03-31 07:10:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (540803ms till timeout)
2022-03-31 07:10:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (539800ms till timeout)
2022-03-31 07:10:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (538797ms till timeout)
2022-03-31 07:10:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (537793ms till timeout)
2022-03-31 07:10:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (536790ms till timeout)
2022-03-31 07:10:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (535787ms till timeout)
2022-03-31 07:10:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (534783ms till timeout)
2022-03-31 07:10:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (533780ms till timeout)
2022-03-31 07:10:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (532777ms till timeout)
2022-03-31 07:10:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (531773ms till timeout)
2022-03-31 07:10:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (530770ms till timeout)
2022-03-31 07:10:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (529767ms till timeout)
2022-03-31 07:10:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (528764ms till timeout)
2022-03-31 07:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: mm2-cluster is in desired state: Ready
2022-03-31 07:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-95744331-1999718717 in namespace infra-namespace
2022-03-31 07:10:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-95744331-1999718717
2022-03-31 07:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-95744331-1999718717 will have desired state: Ready
2022-03-31 07:10:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-95744331-1999718717 will have desired state: Ready
2022-03-31 07:10:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-95744331-1999718717 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:10:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-95744331-1999718717 is in desired state: Ready
2022-03-31 07:10:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1151442496-657678134 in namespace infra-namespace
2022-03-31 07:10:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1151442496-657678134
2022-03-31 07:10:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1151442496-657678134 will have desired state: Ready
2022-03-31 07:10:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1151442496-657678134 will have desired state: Ready
2022-03-31 07:10:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1151442496-657678134 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:10:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1151442496-657678134 is in desired state: Ready
2022-03-31 07:10:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1859001861-99403819 in namespace infra-namespace
2022-03-31 07:10:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1859001861-99403819
2022-03-31 07:10:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1859001861-99403819 will have desired state: Ready
2022-03-31 07:10:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1859001861-99403819 will have desired state: Ready
2022-03-31 07:10:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1859001861-99403819 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1859001861-99403819 is in desired state: Ready
2022-03-31 07:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-672769025-1798374723 in namespace infra-namespace
2022-03-31 07:10:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-672769025-1798374723
2022-03-31 07:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-672769025-1798374723 will have desired state: Ready
2022-03-31 07:10:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-672769025-1798374723 will have desired state: Ready
2022-03-31 07:10:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-672769025-1798374723 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-672769025-1798374723 is in desired state: Ready
2022-03-31 07:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-780650372-593659274 in namespace infra-namespace
2022-03-31 07:10:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-780650372-593659274
2022-03-31 07:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-780650372-593659274 will have desired state: Ready
2022-03-31 07:10:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-780650372-593659274 will have desired state: Ready
2022-03-31 07:10:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-780650372-593659274 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:10:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-780650372-593659274 is in desired state: Ready
2022-03-31 07:10:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-31 07:10:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-31 07:10:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-31 07:10:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-31 07:10:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 07:10:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-31 07:10:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-31 07:10:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-31 07:10:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-31 07:10:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (594981ms till timeout)
2022-03-31 07:10:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-31 07:10:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-31 07:10:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-31 07:10:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-31 07:10:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (589962ms till timeout)
2022-03-31 07:10:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (588959ms till timeout)
2022-03-31 07:10:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (587956ms till timeout)
2022-03-31 07:10:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (586953ms till timeout)
2022-03-31 07:10:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (585950ms till timeout)
2022-03-31 07:10:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (584946ms till timeout)
2022-03-31 07:10:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (583943ms till timeout)
2022-03-31 07:10:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (582940ms till timeout)
2022-03-31 07:10:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (581937ms till timeout)
2022-03-31 07:10:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (580934ms till timeout)
2022-03-31 07:10:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (579930ms till timeout)
2022-03-31 07:10:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (578927ms till timeout)
2022-03-31 07:10:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (577924ms till timeout)
2022-03-31 07:10:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (576921ms till timeout)
2022-03-31 07:10:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (575917ms till timeout)
2022-03-31 07:10:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (574914ms till timeout)
2022-03-31 07:10:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (573911ms till timeout)
2022-03-31 07:10:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (572908ms till timeout)
2022-03-31 07:10:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (571905ms till timeout)
2022-03-31 07:10:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (570902ms till timeout)
2022-03-31 07:10:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (569898ms till timeout)
2022-03-31 07:10:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (568895ms till timeout)
2022-03-31 07:11:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (567892ms till timeout)
2022-03-31 07:11:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (566889ms till timeout)
2022-03-31 07:11:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (565886ms till timeout)
2022-03-31 07:11:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (564883ms till timeout)
2022-03-31 07:11:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (563880ms till timeout)
2022-03-31 07:11:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (562877ms till timeout)
2022-03-31 07:11:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (561874ms till timeout)
2022-03-31 07:11:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (560871ms till timeout)
2022-03-31 07:11:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (559868ms till timeout)
2022-03-31 07:11:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (558864ms till timeout)
2022-03-31 07:11:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (557861ms till timeout)
2022-03-31 07:11:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (556858ms till timeout)
2022-03-31 07:11:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (555855ms till timeout)
2022-03-31 07:11:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (554852ms till timeout)
2022-03-31 07:11:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (553849ms till timeout)
2022-03-31 07:11:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (552846ms till timeout)
2022-03-31 07:11:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (551843ms till timeout)
2022-03-31 07:11:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (550839ms till timeout)
2022-03-31 07:11:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (549836ms till timeout)
2022-03-31 07:11:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (548833ms till timeout)
2022-03-31 07:11:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (547830ms till timeout)
2022-03-31 07:11:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (546827ms till timeout)
2022-03-31 07:11:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (545824ms till timeout)
2022-03-31 07:11:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (544821ms till timeout)
2022-03-31 07:11:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (543817ms till timeout)
2022-03-31 07:11:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (542814ms till timeout)
2022-03-31 07:11:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (541811ms till timeout)
2022-03-31 07:11:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (540808ms till timeout)
2022-03-31 07:11:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (539805ms till timeout)
2022-03-31 07:11:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (538802ms till timeout)
2022-03-31 07:11:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (537798ms till timeout)
2022-03-31 07:11:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (536795ms till timeout)
2022-03-31 07:11:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (535792ms till timeout)
2022-03-31 07:11:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (534789ms till timeout)
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: metrics-cluster-name is in desired state: Ready
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:72] Apply NetworkPolicy access to cluster-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:88] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=cluster-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/kind=cluster-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:90] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to metrics-cluster-name-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-entity-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to second-kafka-cluster-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-entity-operator-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to metrics-cluster-name-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-kafka-exporter-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to second-kafka-cluster-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-kafka-exporter-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-31 07:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-31 07:12:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:12:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.16:9404
2022-03-31 07:12:56 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.16 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:12:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.17:9404
2022-03-31 07:12:58 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.17 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:12:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.15:9404
2022-03-31 07:13:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.15 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.10:9404
2022-03-31 07:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.12:9404
2022-03-31 07:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.11:9404
2022-03-31 07:13:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-31 07:13:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.21:9404/metrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaActiveControllers
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testKafkaActiveControllers test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testUserOperatorMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testUserOperatorMetrics test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperWatchersCount
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testZookeeperWatchersCount test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBrokersCount
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicUnderReplicatedPartitions test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:230] testKafkaBrokersCount test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectIoNetwork
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectIoNetwork test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testUserOperatorMetrics=my-cluster-a57f67ca, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testUserOperatorMetrics=my-user-1997374367-848658272, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testUserOperatorMetrics=my-topic-634069187-162335770, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testUserOperatorMetrics=my-cluster-a57f67ca, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testUserOperatorMetrics=my-user-1997374367-848658272, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testUserOperatorMetrics=my-topic-634069187-162335770, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testUserOperatorMetrics=my-cluster-a57f67ca, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testUserOperatorMetrics=my-user-1997374367-848658272, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testUserOperatorMetrics=my-topic-634069187-162335770, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testZookeeperWatchersCount is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testZookeeperWatchersCount - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperWatchersCount
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperQuorumSize
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testZookeeperQuorumSize test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testUserOperatorMetrics=my-cluster-a57f67ca, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testUserOperatorMetrics=my-user-1997374367-848658272, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testUserOperatorMetrics=my-topic-634069187-162335770, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaTopicUnderReplicatedPartitions is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicUnderReplicatedPartitions - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperAliveConnections
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testZookeeperAliveConnections test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testUserOperatorMetrics=my-cluster-a57f67ca, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testUserOperatorMetrics=my-user-1997374367-848658272, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testUserOperatorMetrics=my-topic-634069187-162335770, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testZookeeperAliveConnections is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testZookeeperAliveConnections - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperAliveConnections
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testZookeeperQuorumSize is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testZookeeperQuorumSize - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperQuorumSize
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicPartitions
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicPartitions test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testMirrorMaker2Metrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testUpdateUser=my-cluster-a375259c, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testKafkaActiveControllers=my-cluster-f628c7f6, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testUserOperatorMetrics=my-cluster-a57f67ca, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2Metrics test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testUpdateUser=my-user-134946258-722731532, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testKafkaActiveControllers=my-user-1094946718-672314291, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testUserOperatorMetrics=my-user-1997374367-848658272, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testUpdateUser=my-topic-686947015-279679095, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testKafkaActiveControllers=my-topic-1006913488-1203956843, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testUserOperatorMetrics=my-topic-634069187-162335770, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testKafkaActiveControllers=my-cluster-f628c7f6, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testUserOperatorMetrics=my-cluster-a57f67ca, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testKafkaActiveControllers=my-user-1094946718-672314291, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testUserOperatorMetrics=my-user-1997374367-848658272, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testKafkaActiveControllers=my-topic-1006913488-1203956843, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testUserOperatorMetrics=my-topic-634069187-162335770, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testKafkaActiveControllers is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testKafkaActiveControllers - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaActiveControllers
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBridgeMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.19:8081/metrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testKafkaBridgeMetrics test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testKafkaActiveControllers=my-cluster-f628c7f6, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testUserOperatorMetrics=my-cluster-a57f67ca, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testKafkaActiveControllers=my-user-1094946718-672314291, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testUserOperatorMetrics=my-user-1997374367-848658272, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testKafkaActiveControllers=my-topic-1006913488-1203956843, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testUserOperatorMetrics=my-topic-634069187-162335770, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaTopicPartitions is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicPartitions - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicPartitions
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.23:9404
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaMetricsSettings
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaMetricsSettings test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testKafkaActiveControllers=my-cluster-f628c7f6, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testKafkaActiveControllers=my-user-1094946718-672314291, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testKafkaActiveControllers=my-topic-1006913488-1203956843, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace infra-namespace
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-producer
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:346] In context testKafkaBrokersCount is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:267] testKafkaBrokersCount - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBrokersCount
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testClusterOperatorMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:230] testClusterOperatorMetrics test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179985ms till timeout)
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testKafkaActiveControllers=my-cluster-f628c7f6, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testKafkaActiveControllers=my-user-1094946718-672314291, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testKafkaActiveControllers=my-topic-1006913488-1203956843, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.5:8080/metrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299942ms till timeout)
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testUserOperatorMetrics is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testUserOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testUserOperatorMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testCruiseControlMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlMetrics test now can proceed its execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testKafkaActiveControllers=my-cluster-f628c7f6, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testKafkaActiveControllers=my-user-1094946718-672314291, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testKafkaActiveControllers=my-topic-1006913488-1203956843, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-75bd668f5c-q5db2 -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.5 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testTopicOperatorMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 7
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:205] [testTopicOperatorMetrics] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-31 07:13:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:210] testTopicOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:346] In context testClusterOperatorMetrics is everything deleted.
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:267] testClusterOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics] to and randomly select one to start execution
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testClusterOperatorMetrics
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-FINISHED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-STARTED
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectRequests
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 7
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:205] [testKafkaConnectRequests] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-31 07:13:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [SuiteThreadController:210] testKafkaConnectRequests is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:346] In context testKafkaConnectIoNetwork is everything deleted.
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectIoNetwork - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests] to and randomly select one to start execution
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectIoNetwork
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-FINISHED
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-STARTED
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectResponse
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:205] [testKafkaConnectResponse] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:210] testKafkaConnectResponse is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-75bd668f5c-q5db2 -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsIsolatedST:450] Verifying that we have more than 0 groups
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-STARTED
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaExporterDifferentSetting
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:205] [testKafkaExporterDifferentSetting] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testKafkaExporterDifferentSetting is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_info{runtime="OpenJDK Runtime Environment",vendor="Red Hat, Inc.",version="11.0.14.1+1-LTS",} -> 1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'profiled nmethods'",} -> 1.3177984E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Old Gen",} -> 2.301884E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Eden Space",} -> 5.07510784E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-profiled nmethods'",} -> 2879616.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Survivor Space",} -> 1.1534336E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Compressed Class Space",} -> 5398472.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Metaspace",} -> 4.725876E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-nmethods'",} -> 1467008.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_current -> 54.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_daemon -> 36.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_peak -> 54.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_started_total -> 69.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked_monitor -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="RUNNABLE",} -> 14.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TIMED_WAITING",} -> 24.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="WAITING",} -> 16.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="BLOCKED",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TERMINATED",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="NEW",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_objects_pending_finalization -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="heap",} -> 5.458688E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="nonheap",} -> 7.14052E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="heap",} -> 8.51443712E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="nonheap",} -> 7.4842112E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="heap",} -> 8.37812224E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="nonheap",} -> -1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="heap",} -> 1.34217728E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="nonheap",} -> 7667712.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-nmethods'",} -> 1467008.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Metaspace",} -> 4.7641008E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'profiled nmethods'",} -> 1.3857408E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Compressed Class Space",} -> 5421408.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Eden Space",} -> 3.0408704E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Old Gen",} -> 2.1032448E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Survivor Space",} -> 3145728.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-profiled nmethods'",} -> 3018368.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Metaspace",} -> 4.9283072E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'profiled nmethods'",} -> 1.3893632E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Compressed Class Space",} -> 6029312.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Eden Space",} -> 4.0894464E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Old Gen",} -> 8.0740352E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Survivor Space",} -> 3145728.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-profiled nmethods'",} -> 3080192.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-nmethods'",} -> 5828608.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Metaspace",} -> -1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'profiled nmethods'",} -> 1.22912768E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Compressed Class Space",} -> 1.073741824E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Eden Space",} -> -1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Survivor Space",} -> -1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-profiled nmethods'",} -> 1.22916864E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Metaspace",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'profiled nmethods'",} -> 2555904.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Compressed Class Space",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Eden Space",} -> 7340032.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Survivor Space",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-profiled nmethods'",} -> 2555904.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Eden Space",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Survivor Space",} -> 3145728.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Eden Space",} -> 4.0894464E7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Survivor Space",} -> 3145728.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Eden Space",} -> -1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Survivor Space",} -> -1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Eden Space",} -> 7340032.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Survivor Space",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_exporter_build_info{version="0.16.1",name="jmx_prometheus_javaagent",} -> 1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="mapped",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="direct",} -> 348153.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="mapped",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="direct",} -> 348153.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="mapped",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="direct",} -> 25.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] process_cpu_seconds_total -> 17.99
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] process_start_time_seconds -> 1.648710481444E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] process_open_fds -> 170.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] process_max_fds -> 1048576.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] process_virtual_memory_bytes -> 1.4651133952E10
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] process_resident_memory_bytes -> 3.4557952E8
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Young Generation",} -> 25.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Young Generation",} -> 0.25
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Old Generation",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Old Generation",} -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_total -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_total -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_75thpercentile -> 61.482456
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_count -> 1.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_min -> 30.236458
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_number -> 12.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_value -> 5.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_value -> 2.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_95thpercentile -> 61.482456
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fiveminuterate -> 0.008643479026060353
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_value -> 1000.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_99thpercentile -> 232.048408
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_number -> 1512.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_999thpercentile -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_oneminuterate -> 0.0015506900910289933
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_number -> 2.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_meanrate -> 0.19513639878648764
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_stddev -> 38.32093777379811
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_mean -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_count -> 4.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_value -> 0.7551020383834839
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_98thpercentile -> 232.048408
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_number -> 17.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_number -> 0.7551020383834839
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_98thpercentile -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fiveminuterate -> 0.0020729741807790705
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fifteenminuterate -> 0.19903968381855255
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_value -> 17.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_number -> 2.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_number -> 1000.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_value -> 12.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_min -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_value -> 1512.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_meanrate -> 0.003322134717609839
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fifteenminuterate -> 0.003825367020675967
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_95thpercentile -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_oneminuterate -> 0.19159943008498997
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_number -> 100.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_99thpercentile -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fifteenminuterate -> 9.484070116640398E-4
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_oneminuterate -> 0.01173086126109628
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_number -> 5.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_count -> 57.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_75thpercentile -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_max -> 232.048408
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_50thpercentile -> 61.482456
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_mean -> 58.68508299748671
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_value -> 2.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_999thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_value -> 100.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_50thpercentile -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_number -> 148.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_mean -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_min -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_value -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fiveminuterate -> 0.19768881846023417
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_count -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_value -> 148.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_999thpercentile -> 232.048408
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_oneminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_max -> 1.544352
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_meanrate -> 0.013566833721198403
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_number -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_max -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_meanrate -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_duration_seconds -> 0.313008217
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_error -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_cached_beans -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded -> 8602.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded_total -> 8602.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_unloaded_total -> 0.0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_created -> 1.648710481625E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_created -> 1.648710481624E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'profiled nmethods'",} -> 1.648710481914E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Old Gen",} -> 1.648710481919E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Eden Space",} -> 1.648710481919E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-profiled nmethods'",} -> 1.648710481919E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Survivor Space",} -> 1.648710481919E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Compressed Class Space",} -> 1.648710481919E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Metaspace",} -> 1.648710481919E9
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-nmethods'",} 1.648710481919E9 -> 
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testCruiseControlMetrics is everything deleted.
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testCruiseControlMetrics
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 7
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-FINISHED
2022-03-31 07:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-consumer in namespace infra-namespace
2022-03-31 07:13:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-consumer
2022-03-31 07:13:04 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: bridge-consumer will be in active state
2022-03-31 07:13:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-31 07:13:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298930ms till timeout)
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.23 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testMirrorMaker2Metrics is everything deleted.
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2Metrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testMirrorMaker2Metrics
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 6
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-FINISHED
2022-03-31 07:13:04 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaProducer metrics will be available
2022-03-31 07:13:05 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-31 07:13:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-31 07:13:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297909ms till timeout)
2022-03-31 07:13:05 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaProducer metrics will be available not ready, will try again in 1000 ms (299451ms till timeout)
2022-03-31 07:13:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296899ms till timeout)
2022-03-31 07:13:06 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-31 07:13:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConsumer metrics will be available
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:428] Looking for 'strimzi_bridge_kafka_consumer_connection_count' in bridge metrics
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-31 07:13:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295890ms till timeout)
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeMetrics
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job bridge-consumer in namespace infra-namespace
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-consumer
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace infra-namespace
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-producer
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testKafkaBridgeMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBridgeMetrics
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-FINISHED
2022-03-31 07:13:07 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294839ms till timeout)
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:230] testTopicOperatorMetrics test now can proceed its execution
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testTopicOperatorMetrics=my-cluster-0fcfa107, testKafkaActiveControllers=my-cluster-f628c7f6, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testTopicOperatorMetrics=my-user-240555346-1853285669, testKafkaActiveControllers=my-user-1094946718-672314291, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testTopicOperatorMetrics=my-topic-426674512-923979873, testKafkaActiveControllers=my-topic-1006913488-1203956843, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testTopicOperatorMetrics=my-cluster-0fcfa107-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectRequests test now can proceed its execution
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testTopicOperatorMetrics=my-cluster-0fcfa107, testKafkaActiveControllers=my-cluster-f628c7f6, testKafkaConnectRequests=my-cluster-f840e4a2, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testTopicOperatorMetrics=my-user-240555346-1853285669, testKafkaActiveControllers=my-user-1094946718-672314291, testKafkaConnectRequests=my-user-942907161-876299063, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testTopicOperatorMetrics=my-topic-426674512-923979873, testKafkaActiveControllers=my-topic-1006913488-1203956843, testKafkaConnectRequests=my-topic-879154910-1049670227, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testTopicOperatorMetrics=my-cluster-0fcfa107-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testKafkaConnectRequests=my-cluster-f840e4a2-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.19:8080/metrics
2022-03-31 07:13:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectResponse test now can proceed its execution
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1096bfa0, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testTopicOperatorMetrics=my-cluster-0fcfa107, testKafkaActiveControllers=my-cluster-f628c7f6, testKafkaConnectRequests=my-cluster-f840e4a2, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-655699378-262097995, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testTopicOperatorMetrics=my-user-240555346-1853285669, testKafkaActiveControllers=my-user-1094946718-672314291, testKafkaConnectRequests=my-user-942907161-876299063, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-207889547-1952557653, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testTopicOperatorMetrics=my-topic-426674512-923979873, testKafkaActiveControllers=my-topic-1006913488-1203956843, testKafkaConnectRequests=my-topic-879154910-1049670227, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1096bfa0-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testTopicOperatorMetrics=my-cluster-0fcfa107-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testKafkaConnectRequests=my-cluster-f840e4a2-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaExporterDifferentSetting test now can proceed its execution
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1096bfa0, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testTopicOperatorMetrics=my-cluster-0fcfa107, testKafkaActiveControllers=my-cluster-f628c7f6, testKafkaExporterDifferentSetting=my-cluster-c88d7247, testKafkaConnectRequests=my-cluster-f840e4a2, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-655699378-262097995, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testTopicOperatorMetrics=my-user-240555346-1853285669, testKafkaActiveControllers=my-user-1094946718-672314291, testKafkaExporterDifferentSetting=my-user-822198449-998083329, testKafkaConnectRequests=my-user-942907161-876299063, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-207889547-1952557653, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testTopicOperatorMetrics=my-topic-426674512-923979873, testKafkaActiveControllers=my-topic-1006913488-1203956843, testKafkaExporterDifferentSetting=my-topic-280586102-146625382, testKafkaConnectRequests=my-topic-879154910-1049670227, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1096bfa0-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testTopicOperatorMetrics=my-cluster-0fcfa107-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-c88d7247-kafka-clients, testKafkaConnectRequests=my-cluster-f840e4a2-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-8454677f49-gxklz -n infra-namespace -- cat /tmp/run.sh
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: heartbeats
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-config
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-offsets
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-status
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-configs
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-offsets
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-status
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1151442496-657678134
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1859001861-99403819
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-95744331-1999718717
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: second-kafka-cluster.checkpoints.internal
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.metrics
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:346] In context testTopicOperatorMetrics is everything deleted.
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:267] testTopicOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testTopicOperatorMetrics
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-FINISHED
2022-03-31 07:13:09 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-8454677f49-gxklz return code - 0
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:346] In context testKafkaConnectRequests is everything deleted.
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectRequests - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectRequests
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-FINISHED
2022-03-31 07:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (599954ms till timeout)
2022-03-31 07:13:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293796ms till timeout)
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:346] In context testKafkaConnectResponse is everything deleted.
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectResponse - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectResponse
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-FINISHED
2022-03-31 07:13:09 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:13:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:13:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:13:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:13:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:13:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292787ms till timeout)
2022-03-31 07:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:13:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291774ms till timeout)
2022-03-31 07:13:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:13:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:13:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:13:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:13:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290765ms till timeout)
2022-03-31 07:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:13:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289756ms till timeout)
2022-03-31 07:13:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (594947ms till timeout)
2022-03-31 07:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:13:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288747ms till timeout)
2022-03-31 07:13:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:13:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:13:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:13:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:13:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287738ms till timeout)
2022-03-31 07:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:13:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286729ms till timeout)
2022-03-31 07:13:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:13:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:13:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:13:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:13:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285721ms till timeout)
2022-03-31 07:13:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:13:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:13:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:13:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:13:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284711ms till timeout)
2022-03-31 07:13:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (589898ms till timeout)
2022-03-31 07:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:13:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283702ms till timeout)
2022-03-31 07:13:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:13:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:13:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:13:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:13:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282693ms till timeout)
2022-03-31 07:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:13:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281684ms till timeout)
2022-03-31 07:13:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:13:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:13:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:13:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:13:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280675ms till timeout)
2022-03-31 07:13:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:13:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:13:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:13:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:13:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279666ms till timeout)
2022-03-31 07:13:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (584891ms till timeout)
2022-03-31 07:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:13:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278657ms till timeout)
2022-03-31 07:13:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:13:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:13:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:13:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:13:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277649ms till timeout)
2022-03-31 07:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:13:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276640ms till timeout)
2022-03-31 07:13:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:13:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:13:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:13:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:13:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275631ms till timeout)
2022-03-31 07:13:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:13:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:13:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:13:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:13:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274622ms till timeout)
2022-03-31 07:13:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (579884ms till timeout)
2022-03-31 07:13:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:13:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:13:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:13:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:13:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273613ms till timeout)
2022-03-31 07:13:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:13:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:13:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:13:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:13:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272604ms till timeout)
2022-03-31 07:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:13:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271595ms till timeout)
2022-03-31 07:13:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:13:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:13:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:13:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:13:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270587ms till timeout)
2022-03-31 07:13:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:13:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:13:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:13:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:13:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269578ms till timeout)
2022-03-31 07:13:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (574877ms till timeout)
2022-03-31 07:13:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:13:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:13:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:13:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:13:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268568ms till timeout)
2022-03-31 07:13:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:13:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:13:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:13:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:13:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267560ms till timeout)
2022-03-31 07:13:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:13:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:13:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:13:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:13:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266550ms till timeout)
2022-03-31 07:13:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:13:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:13:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:13:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:13:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265541ms till timeout)
2022-03-31 07:13:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:13:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:13:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:13:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:13:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264531ms till timeout)
2022-03-31 07:13:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (569870ms till timeout)
2022-03-31 07:13:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:13:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:13:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:13:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:13:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263522ms till timeout)
2022-03-31 07:13:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:13:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:13:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:13:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:13:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262513ms till timeout)
2022-03-31 07:13:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:13:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:13:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:13:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:13:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261503ms till timeout)
2022-03-31 07:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:13:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260494ms till timeout)
2022-03-31 07:13:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:13:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:13:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:13:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:13:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259486ms till timeout)
2022-03-31 07:13:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (564863ms till timeout)
2022-03-31 07:13:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:13:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:13:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:13:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:13:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258477ms till timeout)
2022-03-31 07:13:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:13:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:13:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:13:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:13:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257468ms till timeout)
2022-03-31 07:13:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:13:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:13:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:13:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:13:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256460ms till timeout)
2022-03-31 07:13:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:13:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:13:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:13:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:13:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255450ms till timeout)
2022-03-31 07:13:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:13:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:13:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:13:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:13:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254441ms till timeout)
2022-03-31 07:13:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (559856ms till timeout)
2022-03-31 07:13:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:13:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:13:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:13:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:13:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253432ms till timeout)
2022-03-31 07:13:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:13:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:13:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:13:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:13:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252423ms till timeout)
2022-03-31 07:13:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:13:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:13:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:13:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:13:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251413ms till timeout)
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x ,second-kafka-cluster-zookeeper-0
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-31 07:13:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299992ms till timeout)
2022-03-31 07:13:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-31 07:13:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298983ms till timeout)
2022-03-31 07:13:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (554850ms till timeout)
2022-03-31 07:13:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-31 07:13:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297974ms till timeout)
2022-03-31 07:13:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-31 07:13:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296964ms till timeout)
2022-03-31 07:13:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-31 07:13:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295956ms till timeout)
2022-03-31 07:13:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-31 07:13:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294946ms till timeout)
2022-03-31 07:13:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-31 07:13:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293896ms till timeout)
2022-03-31 07:13:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e, metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:13:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (549843ms till timeout)
2022-03-31 07:14:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:14:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:14:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:14:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-31 07:14:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292886ms till timeout)
2022-03-31 07:14:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:14:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:14:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:14:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-31 07:14:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291877ms till timeout)
2022-03-31 07:14:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:14:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:14:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:14:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-31 07:14:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290868ms till timeout)
2022-03-31 07:14:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:14:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:14:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:14:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-31 07:14:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289858ms till timeout)
2022-03-31 07:14:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:14:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:14:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:14:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-31 07:14:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288849ms till timeout)
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-gxklz=ecbcfe62-bbcc-47cc-bd33-2b1dc00cfafa}
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv=f059805d-c080-4ca2-94e4-eaf835768b7e}
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: metrics-cluster-name-kafka-exporter is ready
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-31 07:14:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:14:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:14:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:14:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-31 07:14:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287840ms till timeout)
2022-03-31 07:14:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-31 07:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-31 07:14:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286830ms till timeout)
2022-03-31 07:14:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-31 07:14:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:14:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:14:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:14:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-31 07:14:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285821ms till timeout)
2022-03-31 07:14:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-31 07:14:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:14:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:14:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:14:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-31 07:14:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284810ms till timeout)
2022-03-31 07:14:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-31 07:14:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:14:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:14:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:14:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-31 07:14:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283801ms till timeout)
2022-03-31 07:14:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-31 07:14:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:14:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:14:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:14:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-31 07:14:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282792ms till timeout)
2022-03-31 07:14:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593971ms till timeout)
2022-03-31 07:14:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:14:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:14:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:14:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-31 07:14:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281783ms till timeout)
2022-03-31 07:14:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592967ms till timeout)
2022-03-31 07:14:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:14:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:14:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:14:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-31 07:14:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280773ms till timeout)
2022-03-31 07:14:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591963ms till timeout)
2022-03-31 07:14:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:14:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:14:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:14:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-31 07:14:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279764ms till timeout)
2022-03-31 07:14:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590958ms till timeout)
2022-03-31 07:14:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:14:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:14:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:14:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-31 07:14:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278755ms till timeout)
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv not ready: metrics-cluster-name-kafka-exporter)
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv are ready
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment metrics-cluster-name-kafka-exporter rolling update finished
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv -n infra-namespace -- cat /tmp/run.sh
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-56595b8d96-gwnzv return code - 0
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDifferentSetting is everything deleted.
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaExporterDifferentSetting - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaExporterDifferentSetting
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-FINISHED
2022-03-31 07:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:14:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:14:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:14:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:14:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-31 07:14:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277745ms till timeout)
2022-03-31 07:14:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:14:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:14:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:14:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-31 07:14:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276736ms till timeout)
2022-03-31 07:14:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:14:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:14:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:14:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-31 07:14:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275727ms till timeout)
2022-03-31 07:14:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:14:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:14:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:14:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-31 07:14:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274717ms till timeout)
2022-03-31 07:14:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:14:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:14:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:14:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-31 07:14:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273708ms till timeout)
2022-03-31 07:14:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:14:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:14:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:14:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-31 07:14:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272699ms till timeout)
2022-03-31 07:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-31 07:14:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271689ms till timeout)
2022-03-31 07:14:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:14:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:14:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:14:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-31 07:14:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270680ms till timeout)
2022-03-31 07:14:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:14:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:14:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:14:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-31 07:14:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269671ms till timeout)
2022-03-31 07:14:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:14:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:14:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:14:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-31 07:14:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268662ms till timeout)
2022-03-31 07:14:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:14:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:14:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:14:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-31 07:14:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267652ms till timeout)
2022-03-31 07:14:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:14:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:14:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:14:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-31 07:14:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266640ms till timeout)
2022-03-31 07:14:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:14:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:14:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:14:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-31 07:14:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265630ms till timeout)
2022-03-31 07:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-31 07:14:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264621ms till timeout)
2022-03-31 07:14:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:14:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:14:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:14:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-31 07:14:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263612ms till timeout)
2022-03-31 07:14:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:14:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:14:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:14:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-31 07:14:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262603ms till timeout)
2022-03-31 07:14:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:14:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:14:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:14:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-31 07:14:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261594ms till timeout)
2022-03-31 07:14:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:14:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:14:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:14:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-31 07:14:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260585ms till timeout)
2022-03-31 07:14:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:14:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:14:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:14:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-31 07:14:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259576ms till timeout)
2022-03-31 07:14:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:14:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:14:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:14:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-31 07:14:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258566ms till timeout)
2022-03-31 07:14:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:14:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:14:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:14:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-31 07:14:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257557ms till timeout)
2022-03-31 07:14:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:14:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:14:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:14:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-31 07:14:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256547ms till timeout)
2022-03-31 07:14:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:14:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:14:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:14:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-31 07:14:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255538ms till timeout)
2022-03-31 07:14:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:14:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:14:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:14:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-31 07:14:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254488ms till timeout)
2022-03-31 07:14:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:14:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:14:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:14:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-31 07:14:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253480ms till timeout)
2022-03-31 07:14:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:14:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:14:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:14:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-31 07:14:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252470ms till timeout)
2022-03-31 07:14:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:14:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:14:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:14:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-31 07:14:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251462ms till timeout)
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-tjh4v ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-7vg6x ,second-kafka-cluster-zookeeper-0
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaMetricsSettings is everything deleted.
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaMetricsSettings - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testUpdateUser, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testCruiseControlBasicAPIRequests, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testKafkaActiveControllers, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaTopicUnderReplicatedPartitions, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaMetricsSettings
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-FINISHED
2022-03-31 07:14:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-STARTED
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1096bfa0, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testReconcileStateMetricInTopicOperator=my-cluster-66878206, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testTopicOperatorMetrics=my-cluster-0fcfa107, testKafkaActiveControllers=my-cluster-f628c7f6, testKafkaExporterDifferentSetting=my-cluster-c88d7247, testKafkaConnectRequests=my-cluster-f840e4a2, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-655699378-262097995, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testReconcileStateMetricInTopicOperator=my-user-455657673-1131260116, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testTopicOperatorMetrics=my-user-240555346-1853285669, testKafkaActiveControllers=my-user-1094946718-672314291, testKafkaExporterDifferentSetting=my-user-822198449-998083329, testKafkaConnectRequests=my-user-942907161-876299063, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-207889547-1952557653, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testReconcileStateMetricInTopicOperator=my-topic-2140517486-1767444103, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testTopicOperatorMetrics=my-topic-426674512-923979873, testKafkaActiveControllers=my-topic-1006913488-1203956843, testKafkaExporterDifferentSetting=my-topic-280586102-146625382, testKafkaConnectRequests=my-topic-879154910-1049670227, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1096bfa0-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-66878206-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testTopicOperatorMetrics=my-cluster-0fcfa107-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-c88d7247-kafka-clients, testKafkaConnectRequests=my-cluster-f840e4a2-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-metrics-cluster-test
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2140517486-1767444103 in namespace second-metrics-cluster-test
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2140517486-1767444103
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2140517486-1767444103 will have desired state: Ready
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2140517486-1767444103 will have desired state: Ready
2022-03-31 07:14:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2140517486-1767444103 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2140517486-1767444103 is in desired state: Ready
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h finished with return code: 0
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:553] Checking if resource state metric reason message is "none" and KafkaTopic is ready
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:556] Changing topic name in spec.topicName
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2140517486-1767444103 will have desired state: NotReady
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2140517486-1767444103 will have desired state: NotReady
2022-03-31 07:14:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2140517486-1767444103 will have desired state: NotReady not ready, will try again in 1000 ms (179992ms till timeout)
2022-03-31 07:14:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2140517486-1767444103 is in desired state: NotReady
2022-03-31 07:14:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:14:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h finished with return code: 0
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:564] Changing back to it's original name and scaling replicas to be higher number
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-2140517486-1767444103
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-2140517486-1767444103
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h finished with return code: 0
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:576] Scaling replicas to be higher than before
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-2140517486-1767444103
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-2140517486-1767444103
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h finished with return code: 0
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:584] Changing KafkaTopic's spec to correct state
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2140517486-1767444103 will have desired state: Ready
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2140517486-1767444103 will have desired state: Ready
2022-03-31 07:14:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2140517486-1767444103 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2140517486-1767444103 is in desired state: Ready
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-8zc7h finished with return code: 0
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReconcileStateMetricInTopicOperator
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2140517486-1767444103 in namespace second-metrics-cluster-test
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2140517486-1767444103
2022-03-31 07:14:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2140517486-1767444103 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-FINISHED
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-STARTED
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1096bfa0, testSendSimpleMessageTls=my-cluster-82f3beb1, testKafkaConnectIoNetwork=my-cluster-19c3bb80, testReconcileStateMetricInTopicOperator=my-cluster-66878206, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233, testCruiseControlBasicAPIRequests=my-cluster-57cad830, testZookeeperWatchersCount=my-cluster-52c992fa, testKafkaTopicPartitions=my-cluster-50e8ce5e, testMirrorMaker2Metrics=my-cluster-d8e683c6, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720, testZookeeperAliveConnections=my-cluster-2b59457e, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b, testSendMessagesTlsScramSha=my-cluster-7009fdd7, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c, testZookeeperQuorumSize=my-cluster-5d25f8be, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870, testKafkaBridgeMetrics=my-cluster-3935bc24, testKafkaExporterDataAfterExchange=my-cluster-6d38bf67, testUpdateUser=my-cluster-a375259c, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801, testTopicOperatorMetrics=my-cluster-0fcfa107, testKafkaActiveControllers=my-cluster-f628c7f6, testKafkaExporterDifferentSetting=my-cluster-c88d7247, testKafkaConnectRequests=my-cluster-f840e4a2, testCruiseControlMetrics=my-cluster-cb6880a3, testReceiveSimpleMessageTls=my-cluster-490c646e, testKafkaBrokersCount=my-cluster-c34372e4, testClusterOperatorMetrics=my-cluster-60b1a1bf, testUserOperatorMetrics=my-cluster-a57f67ca, testKafkaMetricsSettings=my-cluster-776bb40d, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed}
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-655699378-262097995, testSendSimpleMessageTls=my-user-237170398-1980436763, testKafkaConnectIoNetwork=my-user-443922231-58098478, testReconcileStateMetricInTopicOperator=my-user-455657673-1131260116, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1524199389-925217429, testSendMessagesCustomListenerTlsScramSha=my-user-1898862452-1098575469, testCruiseControlBasicAPIRequests=my-user-248800159-1595848461, testZookeeperWatchersCount=my-user-1866846041-125439783, testKafkaTopicPartitions=my-user-540878000-750406683, testMirrorMaker2Metrics=my-user-670160365-962723176, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1423892763-2804982, testZookeeperAliveConnections=my-user-1323016306-1558362571, testAutoRenewAllCaCertsTriggeredByAnno=my-user-332320981-1578568188, testKafkaTopicUnderReplicatedPartitions=my-user-1729008923-540052073, testSendMessagesTlsScramSha=my-user-2082882415-1062946918, testMirrorMaker2TlsAndTlsClientAuth=my-user-1577263207-1820585929, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-855136009-332089227, testRackAwareConnectCorrectDeployment=my-user-1449887307-921495971, testZookeeperQuorumSize=my-user-1213976917-588863161, testKafkaAndZookeeperScaleUpScaleDown=my-user-1993501230-801238233, testKafkaBridgeMetrics=my-user-976608913-1648295728, testKafkaExporterDataAfterExchange=my-user-1764467308-79567076, testUpdateUser=my-user-134946258-722731532, testKafkaInDifferentNsThanClusterOperator=my-user-59727414-191681013, testTopicOperatorMetrics=my-user-240555346-1853285669, testKafkaActiveControllers=my-user-1094946718-672314291, testKafkaExporterDifferentSetting=my-user-822198449-998083329, testKafkaConnectRequests=my-user-942907161-876299063, testCruiseControlMetrics=my-user-189343056-1851761029, testReceiveSimpleMessageTls=my-user-1864695920-280026871, testKafkaBrokersCount=my-user-1574384-551080029, testClusterOperatorMetrics=my-user-689236115-2040447897, testUserOperatorMetrics=my-user-1997374367-848658272, testKafkaMetricsSettings=my-user-1412286468-561959476, testMirrorMakerTlsAuthenticated=my-user-489244562-902012880}
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-207889547-1952557653, testSendSimpleMessageTls=my-topic-431455278-78371477, testKafkaConnectIoNetwork=my-topic-1127538917-683051662, testReconcileStateMetricInTopicOperator=my-topic-2140517486-1767444103, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1369039929-704566855, testSendMessagesCustomListenerTlsScramSha=my-topic-260837071-1349464842, testCruiseControlBasicAPIRequests=my-topic-2141704916-1621845577, testZookeeperWatchersCount=my-topic-1760777749-1179376703, testKafkaTopicPartitions=my-topic-1823026723-787384066, testMirrorMaker2Metrics=my-topic-502560855-1852163125, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1638710342-783058949, testZookeeperAliveConnections=my-topic-1002586369-906221541, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-606982444-1208823186, testKafkaTopicUnderReplicatedPartitions=my-topic-1825026216-1529896941, testSendMessagesTlsScramSha=my-topic-1777395573-775924041, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1512831762-342736066, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-619640975-1365947180, testRackAwareConnectCorrectDeployment=my-topic-1587930759-1293003827, testZookeeperQuorumSize=my-topic-1762439397-1530944464, testKafkaAndZookeeperScaleUpScaleDown=my-topic-606370994-844170518, testKafkaBridgeMetrics=my-topic-1206311307-335361836, testKafkaExporterDataAfterExchange=my-topic-1903987740-113528439, testUpdateUser=my-topic-686947015-279679095, testKafkaInDifferentNsThanClusterOperator=my-topic-2135152674-942633642, testTopicOperatorMetrics=my-topic-426674512-923979873, testKafkaActiveControllers=my-topic-1006913488-1203956843, testKafkaExporterDifferentSetting=my-topic-280586102-146625382, testKafkaConnectRequests=my-topic-879154910-1049670227, testCruiseControlMetrics=my-topic-896697433-1290788947, testReceiveSimpleMessageTls=my-topic-775532915-933074552, testKafkaBrokersCount=my-topic-992246248-1171170546, testClusterOperatorMetrics=my-topic-1945414514-577436556, testUserOperatorMetrics=my-topic-634069187-162335770, testKafkaMetricsSettings=my-topic-270000976-1532764306, testMirrorMakerTlsAuthenticated=my-topic-425825936-2110554116}
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1096bfa0-kafka-clients, testSendSimpleMessageTls=my-cluster-82f3beb1-kafka-clients, testKafkaConnectIoNetwork=my-cluster-19c3bb80-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-66878206-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-92a05c6e-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-0a4e5233-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-57cad830-kafka-clients, testZookeeperWatchersCount=my-cluster-52c992fa-kafka-clients, testKafkaTopicPartitions=my-cluster-50e8ce5e-kafka-clients, testMirrorMaker2Metrics=my-cluster-d8e683c6-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-93b50720-kafka-clients, testZookeeperAliveConnections=my-cluster-2b59457e-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-2def1ad9-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-7301cd4b-kafka-clients, testSendMessagesTlsScramSha=my-cluster-7009fdd7-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-7df66d40-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-d58e9437-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-1ce8c22c-kafka-clients, testZookeeperQuorumSize=my-cluster-5d25f8be-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-012ef870-kafka-clients, testKafkaBridgeMetrics=my-cluster-3935bc24-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-6d38bf67-kafka-clients, testUpdateUser=my-cluster-a375259c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-1c5e0801-kafka-clients, testTopicOperatorMetrics=my-cluster-0fcfa107-kafka-clients, testKafkaActiveControllers=my-cluster-f628c7f6-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-c88d7247-kafka-clients, testKafkaConnectRequests=my-cluster-f840e4a2-kafka-clients, testCruiseControlMetrics=my-cluster-cb6880a3-kafka-clients, testReceiveSimpleMessageTls=my-cluster-490c646e-kafka-clients, testKafkaBrokersCount=my-cluster-c34372e4-kafka-clients, testClusterOperatorMetrics=my-cluster-60b1a1bf-kafka-clients, testUserOperatorMetrics=my-cluster-a57f67ca-kafka-clients, testKafkaMetricsSettings=my-cluster-776bb40d-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7dd975ed-kafka-clients}
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1cd93cc0, which are set.
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2f0c7cb8, messages=[], arguments=[--bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-1151442496-657678134, --max-messages, 5000], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='infra-namespace-kafka-clients-748578f786-lg9fz', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1151442496-657678134', maxMessages=5000, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1cd93cc0}
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 5000 messages to metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092:my-topic-1151442496-657678134 from pod infra-namespace-kafka-clients-748578f786-lg9fz
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- /opt/kafka/producer.sh --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-1151442496-657678134 --max-messages 5000
2022-03-31 07:14:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- /opt/kafka/producer.sh --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-1151442496-657678134 --max-messages 5000
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 5000 messages
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11fbef32, which are set.
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@55e42fac, messages=[], arguments=[--bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --group-id, my-consumer-group-1449205686, --topic, my-topic-1151442496-657678134, --max-messages, 5000, --group-instance-id, instance959474177], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='infra-namespace-kafka-clients-748578f786-lg9fz', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1151442496-657678134', maxMessages=5000, kafkaUsername='null', consumerGroupName='my-consumer-group-1449205686', consumerInstanceId='instance959474177', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11fbef32}
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 5000 messages from metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092#my-topic-1151442496-657678134 from pod infra-namespace-kafka-clients-748578f786-lg9fz
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- /opt/kafka/consumer.sh --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --group-id my-consumer-group-1449205686 --topic my-topic-1151442496-657678134 --max-messages 5000 --group-instance-id instance959474177
2022-03-31 07:14:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- /opt/kafka/consumer.sh --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --group-id my-consumer-group-1449205686 --topic my-topic-1151442496-657678134 --max-messages 5000 --group-instance-id instance959474177
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 5000 messages
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-lg9fz -n infra-namespace -- curl 172.17.0.25:9404/metrics
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.25 from Pod infra-namespace-kafka-clients-748578f786-lg9fz finished with return code: 0
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDataAfterExchange is everything deleted.
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-FINISHED
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [metrics.MetricsIsolatedST - After All] - Clean up after test suite
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MetricsIsolatedST
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-780650372-593659274 in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-31 07:15:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-31 07:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka metrics-cluster-name in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace infra-namespace, for cruise control Kafka cluster metrics-cluster-name
2022-03-31 07:15:05 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-780650372-593659274
2022-03-31 07:15:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-31 07:15:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-31 07:15:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-31 07:15:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-780650372-593659274 not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-31 07:15:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster not ready, will try again in 10000 ms (599987ms till timeout)
2022-03-31 07:15:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (479981ms till timeout)
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-672769025-1798374723 in namespace infra-namespace
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-672769025-1798374723
2022-03-31 07:15:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-672769025-1798374723 not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-31 07:15:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name
2022-03-31 07:15:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-31 07:15:15 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-31 07:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-31 07:15:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-bridge in namespace infra-namespace
2022-03-31 07:15:15 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-31 07:15:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-31 07:15:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge
2022-03-31 07:15:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (469950ms till timeout)
2022-03-31 07:15:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name not ready, will try again in 10000 ms (599979ms till timeout)
2022-03-31 07:15:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge not ready, will try again in 10000 ms (479977ms till timeout)
2022-03-31 07:15:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (479963ms till timeout)
2022-03-31 07:15:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1859001861-99403819 in namespace infra-namespace
2022-03-31 07:15:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1859001861-99403819
2022-03-31 07:15:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1151442496-657678134 in namespace infra-namespace
2022-03-31 07:15:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1151442496-657678134
2022-03-31 07:15:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-95744331-1999718717 in namespace infra-namespace
2022-03-31 07:15:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-95744331-1999718717
2022-03-31 07:15:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name not ready, will try again in 10000 ms (829987ms till timeout)
2022-03-31 07:15:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (459937ms till timeout)
2022-03-31 07:15:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (469952ms till timeout)
2022-03-31 07:15:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (449929ms till timeout)
2022-03-31 07:15:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (459946ms till timeout)
2022-03-31 07:15:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (439920ms till timeout)
2022-03-31 07:15:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (449899ms till timeout)
2022-03-31 07:15:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (439892ms till timeout)
2022-03-31 07:16:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (429885ms till timeout)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,953.894 s - in io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-31 07:16:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:16:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-31 07:16:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179984ms till timeout)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179980ms till timeout)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-31 07:16:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179898ms till timeout)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:16:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479972ms till timeout)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-31 07:16:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179907ms till timeout)
2022-03-31 07:16:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179789ms till timeout)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-31 07:16:25 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-31 07:16:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179941ms till timeout)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-31 07:16:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-31 07:16:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-31 07:16:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-31 07:16:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-31 07:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-31 07:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 07:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 07:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v146482
2022-03-31 07:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v146482
2022-03-31 07:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=146482&allowWatchBookmarks=true&watch=true...
2022-03-31 07:16:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 07:16:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 146485
2022-03-31 07:16:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 146694
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 146700
2022-03-31 07:16:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-metrics-cluster-test
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v146694 in namespace default
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@3eaceaf6
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3a9379fb
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3a9379fb
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3a9379fb
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 07:16:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-31 07:16:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-31 07:16:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v146701
2022-03-31 07:16:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v146701
2022-03-31 07:16:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-metrics-cluster-test&resourceVersion=146701&allowWatchBookmarks=true&watch=true...
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-31 07:16:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 146702
2022-03-31 07:16:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 146775
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 146781
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v146775 in namespace default
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@17064afe
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4e24f9b2
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4e24f9b2
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4e24f9b2
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-31 07:16:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-31 07:16:56 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-03-31 07:16:56 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-03-31 07:16:56 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-03-31 07:16:56 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-03-31 07:16:56 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 35, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift . [1;32mSUCCESS[m [  2.615 s]
[[1;34mINFO[m] test ............................................... [1;32mSUCCESS[m [  0.979 s]
[[1;34mINFO[m] crd-annotations .................................... [1;32mSUCCESS[m [  1.035 s]
[[1;34mINFO[m] crd-generator ...................................... [1;32mSUCCESS[m [  2.544 s]
[[1;34mINFO[m] api ................................................ [1;32mSUCCESS[m [  6.780 s]
[[1;34mINFO[m] mockkube ........................................... [1;32mSUCCESS[m [  0.899 s]
[[1;34mINFO[m] config-model ....................................... [1;32mSUCCESS[m [  0.703 s]
[[1;34mINFO[m] certificate-manager ................................ [1;32mSUCCESS[m [  0.783 s]
[[1;34mINFO[m] operator-common .................................... [1;32mSUCCESS[m [  1.775 s]
[[1;34mINFO[m] systemtest ......................................... [1;32mSUCCESS[m [  01:06 h]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  01:06 h
[[1;34mINFO[m] Finished at: 2022-03-31T07:16:56Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
